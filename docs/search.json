[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Biostatistics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#learning-in-this-era",
    "href": "index.html#learning-in-this-era",
    "title": "Applied Biostatistics",
    "section": "Learning in this era",
    "text": "Learning in this era\nI know you’re dealing with a lot. Every year students are dealing with a lot – from jobs, to supporting family, and all the other challenges of modern college life. Yet, we are all trying to make the most of life in this era. We want to teach, learn, and grow.\nMoreover, I believe this content is increasingly important – statistics is obsessed with the critical evaluation of claims in the face of data, and is therefore particularly useful in uncertain times. Given this focus, and given that you all have different energies, motivations and backgrounds, I am restructuring this course slightly from previous years. The biggest change is a continued de-emphasis on math and programming – that doesn’t mean I’m eliminating these features, but rather that I am streamlining the required math and programming to what I believe are the essentials. For those who want more mathematical and/or computational details (either because you want to push yourself or you need this to make sense of things), I am including a bunch of optional content and support. I am also wrestling with the impact of LLMs in our education (more below).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#i-love-teaching-this-course",
    "href": "index.html#i-love-teaching-this-course",
    "title": "Applied Biostatistics",
    "section": "I love teaching this course",
    "text": "I love teaching this course\nThe content is very important to me. I also care deeply about you. I want to make sure you get all you can / all you need from this course, while recognizing the many challenges we are all facing. One tangible thing I leave you with is this book, which I hope you find useful as you go on in your life. Another thing I leave you with is my concern for your well-being and understanding – please contact me with any suggestions about the pace, content, or structure of this course and/or any life updates which may change how and when you can complete the work.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-philosophy-goals",
    "href": "index.html#course-philosophy-goals",
    "title": "Applied Biostatistics",
    "section": "Course philosophy / goals",
    "text": "Course philosophy / goals\nMy motivating goal for this course is to empower you to produce, present, and critically evaluate statistical evidence — especially as applied to biological topics. You should know that statistical models are only models and that models are imperfect abstractions of reality. You should be able to think about how a biological question could be formulated as a statistical question, present graphs which show how data speak to this question, be aware of any shortcomings of that model, and how statistical analysis of a data set can be brought back into our biological discussion.\n\n“By the end of this course…\n\nStudents should be statistical thinkers. \nStudents will recognize that data are comprised of observations that partially reflect chance sampling, & that a major goal of statistics is to incorporate this idea of chance into our interpretation of observations. Thinking this way can be challenging because it is a fundamentally new way to think about the world. Once this is mastered, much of the material follows naturally. Until then, it’s more confusing.\n\n\n Students should think about probability quantitatively.\nThat chance influences observations is CRITICAL to statistics (see above). Quantitatively translating these probabilities into distributions and associated statistical tests allows for mastery of the topic.\n\n\n Students should recognize how bias can influence our results. \nNot only are results influenced by chance, but factors outside of our focus can also drive results. Identifying subtle biases and non-independence is key to conducting and interpreting statistics.\n\n\n Students should become familiar with standard statistical tools / approaches and when to use them. \nRecognize how bias can influence our results. What is the difference between Bayesian and frequentist thinking? How can data be visualized effectively? What is the difference between statistical and real-world significance? How do we responsibly present/ interpret statistical results? We will grapple with & answer these questions over the term.\n\n\n Students should have familiarity with foundational statistical values and concepts. \nStudents will gain an intuitive feel for the meaning of stats words like variance, standard error, p-value, t-statistic, and F-statistic, and will be able to read and interpret graphs, and how to translate linear models into sentences.\n\n\n Students should be able to conduct the entire process of data analysis in R. \nStudents will be able to utilize the statistical language, R, to summarize, analyze, and combine data to make appropriate visualizations and to conduct appropriate statistical tests.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-rstudio-and-the-tidyverse",
    "href": "index.html#r-rstudio-and-the-tidyverse",
    "title": "Applied Biostatistics",
    "section": "R, RStudio, and the tidyverse",
    "text": "R, RStudio, and the tidyverse\n\n\n\n\n\n\nThis image comes with permissions from Allison Horst, who makes tremendous aRt. If you appreciate her work, she would appreciate your support for Data for Black Lives\n\n\n\nWe will be using R (version 4.4.0 or above.) in this course, in the RStudio environment. My goal is to have you empowered to make figures, run analyses, and be well positioned for future work in R, with as much fun and as little pain as possible. RStudio is an environment and the tidyverse is a set of R packages that makes R’s powers more accessible without the need to learn a bunch of computer programming.\nSome of you might have experience with R and some may not. Some of this experience might be in tidyverse or not. There will be ups and downs — the frustration of not understanding and/or it not working and the joy of small successes. Remember to be patient, forgiving and kind to yourself, your peers, and me. Ask for help from the internet, your favorite LLM, your friends, your TAs, and your professor.\n\nR Installation\nBefore you can use R you must download and install it.\\(^*\\)  So, to get started, download R from CRAN, and follow the associated installation instructions (see below for detailed instructions for your system).\\(^*\\) This is not strictly true. You can use R online via posit cloud. This is a “freemium” service and the free plan is unlikely to meet your needs.\n\nPC install guideMac install guideLinux install guide\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a PC youtube link here.\n“To install R on Windows, click the Download R for Windows link. Then click the base link. Next, click the first link at the top of the new page. This link should say something like Download R 4.4.2 for Windows except the 4.4.2 will be replaced by the most current version of R. The link downloads an installer program, which installs the most up-to-date version of R for Windows. Run this program and step through the installation wizard that appears. The wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a mac].\n“To install R on a Mac, click the Download R for macOS link. Next, click on the [newest package link compatible with your computer]. An installer will download to guide you through the installation process, which is very easy. The installer lets you customize your installation, but the defaults will be suitable for most users. I’ve never found a reason to change them. If your computer requires a password before installing new programs, you’ll need it here.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nR comes pre-installed on many Linux systems, but you’ll want the newest version of R if yours is out of date. The CRAN website provides files to build R from source on [Debian], Redhat, SUSE, and Ubuntu systems under the link “Download R for Linux.” Click the link and then follow the directory trail to the version of Linux you wish to install on. The exact installation procedure will vary depending on the Linux system you use. CRAN guides the process by grouping each set of source files with documentation or README files that explain how to install on your system.\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nAfter installing R download/update RStudio from here.\n\nAlternatively you can simply join the course via RStudioCloud. This could be desirable if you do not want to or have trouble doing this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-and-how-will-we-use-it",
    "href": "index.html#what-is-this-book-and-how-will-we-use-it",
    "title": "Applied Biostatistics",
    "section": "What is this ‘book’ and how will we use it?",
    "text": "What is this ‘book’ and how will we use it?\nA fantastic feature of this book is that it does not stand alone. It is neither the entirety of the course content, nor is it all my idea. In addition from lifting from a few other courses online (with attribution), I also make heavy use of these texts:\n\nThe Analysis of Biological Data Third Edition (Whitlock & Schluter, 2020): I taught with this book for years. It is fantastic and shaped how I think about teaching Biostats. It has many useful resources available online. The writing is great, as are the examples. Most of my material originates here (although I occasionally do things a bit differently). Buy the latest edition.\nCalling Bullshit (Bergstrom & West, 2020): This book is not technical, but points to the big picture concerns of statisticians. It is very practical and well written. I will occasionally assign readings from this book, and/or point you to videos on their website. All readings will be made available for you, but you might want to buy a physical copy.\nFundamentals of Data Visualization (Wilke, 2019): This book is free online, and is very helpful for thinking about graphing data. In my view, graphing is among the most important skills in statistical reasoning, so I reference it regularly.\nR for Data Science (Grolemund & Wickham, 2018): This book is free online, and is very helpful for doing the sorts of things we do in R regularly. This is a great resource.\nThe storytelling with data podcast is a fantastic data viz podcast. Be sure to check out Cole Nussbaumer Knaflic’s books too!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-will-this-term-work-look",
    "href": "index.html#how-will-this-term-work-look",
    "title": "Applied Biostatistics",
    "section": "How will this term work / look?",
    "text": "How will this term work / look?\n\nPrep for ‘class’. This class is flipped with asynchronous content delivery and synchronous meetings.\n\nBe sure to look over the assigned readings and/or videos, and complete the short low-stakes homework BEFORE each course.\n\nDuring class time, I will address questions make announcements, and get you started on in-class work. The TA & I will bounce around your breakout rooms to provide help and check-in. If you cannot make the class, you could do this on your own time without help, but we do not recommend this as a class strategy.\n\nThe help of your classmates and the environment they create is one of the best parts of this class. Help each other.\n\nIn addition to low stakes work before and in class, there will be a few more intense assignments, some collaborative projects, some in class exams, and a summative project as the term ends.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-use-of-large-language-models",
    "href": "index.html#the-use-of-large-language-models",
    "title": "Applied Biostatistics",
    "section": "The Use of Large Language Models",
    "text": "The Use of Large Language Models\nWe are in the early days of a truly disruptive technology. Large Language Models (LLMs) like ChatGPT and Claude are transforming how we work and learn. While the impact of these tools on future employment, expertise, and citizenry is yet to be settled, it seems clear that no one will hire you to copy and paste AI-generated output. At the same time, no one will hire you to ignore this technology. Success lies in learning how to critically evaluate and work with LLMs—to validate their output, improve your own understanding, and create high-quality results. Subject-level expertise, in conjunction with strong skills in working with AI, will be essential for the foreseeable future.\n\nYou can use LLMs to learn things or avoid learning things. Choose wisely.\n\nLearning from AI and having it help you solve problems will allow you all to do better and learn more than people have been able to do previously. Using AI to avoid learning – e.g. having it write or code for you without you thinking/learning will always come back to bite you in the ass.\nWhile you are ultimately in charge of your learning, I will provide plenty of opportunities for in-class, computer-free efforts to show your mastery of the subject. I will also provide guidance on individual assignments about the appropriate use of AI to help maximize the impact of the assignment on your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-path-through-the-term",
    "href": "index.html#the-path-through-the-term",
    "title": "Applied Biostatistics",
    "section": "The path through the term",
    "text": "The path through the term\nI start by assuming you know nothing about R or statistics to start (I know this assumption is wrong – many of you all know a lot!). From this humble beginning I aim to leave you with the skills to conduct standard statistical analyses, and the understanding of statistics and the ability to go beyond what we have learned. We take the following path in Figure 1, below:\n\n\n\n\n\n\n\n\nFigure 1: Our journey through biostatistics begins with (1) gaining comfort in R, then moves on to (2) describing data and (3) considering sampling and uncertainty. Next, we (4) introduce null hypothesis significance testing, (5) build models, and (6) address more advanced topics (aka “the big lake of statistics”, aka Lake Isabella).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Applied Biostatistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nStudents\nFirst and foremost, I would like to thank the more than 500 students who have taken my Applied Biostatistics course. Students provide the most important feedback on whether a particular pedagogical approach is effective. While not every experiment succeeds, I am incredibly grateful to each student who has helped me learn what works and what doesn’t as they engaged with the material.\n\n\nTeaching Assistants (TAs)\nI have been fortunate to work with outstanding graduate teaching assistants over the past ten years:\n\nDerek Nedveck: Derek played a key role in helping me establish the course during its early years.\nGerman Vargas Gutierrez: A highly skilled statistician, German’s assistance was invaluable in refining the course a few years into its development.\nChaochih Liu: A brilliant programmer, Chaochih contributed greatly to the course’s organization and structure.\nHusain Agha: Husain has remarkable insights into statistics, genetics, and teaching. My work has greatly benefited from bouncing ideas off him.\nBrooke Kern: Brooke was not only an exceptional TA but also a valuable collaborator. Much of the data in this book is drawn from her dissertation research.\n\n\n\n\n\n\n\n\n\n\nFigure 2: My incredible TAs who have all helped shape this material.\n\n\n\n\n\n\n\nCollaborators\nBrooke Kern, Dave Moeller and Shelley Sianta have generated much of the data in this book and have been patient with my delays in turning around our research during teaching times. Dave also provided nearly every picture in this book.\n\n\nTeaching Colleagues\nI have learned a lot about statistics and how to teach it from John Fieberg. His book, Statistics for Ecologists is fantastic! I am also deeply indebted to Fumi Katagiri who began this course and worked through a lot of it before I arrived at UMN, and who thinks deeply about stats and how to teach it.\n\n\nPeople who provided comments\nJohn Rotenberry, and Ruth Shaw have provided helpful comments!\n\n\nUnknowing contributors\nThe online community of statistics and R teaching is an amazing place. I have borrowed heavily from the many amazing free resources. Here are the most critical:\n\nAllison Horst has fantastic illustrations for statistics that she makes freely available.\nPeter D.R. Higgins has created a truly marvelous book – Reproducible Medical Research With R (Higgins (2024)). I have learned a lot and stolen some teaching tricks from this work.\nJenny Bryan has helped me think about getting students able to do things in R well and quickly. Her book, STAT 545: Data wrangling, exploration, and analysis with R (Bryan (2020)), is a classic.\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and analysis with r. Bookdown. https://stat545.com\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own functions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r. Bookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RILs between sympatric and allopatric parviflora\nWouldn’t it be cool if, at this stage, the populations could evolve a mechanism to preferentially mate with their own kind? The adaptive evolution of avoiding mating with a closely related species—a process known as reinforcement—does just that. However, the evolution of reinforcement is complex and has only been conclusively documented in a handful of cases.\nDave Moeller and colleagues (including me) have been investigating one potential case of reinforcement. Clarkia xantiana subspecies parviflora (hereafter parviflora) is an annual flowering plant native to California. Unlike its outcrossing sister subspecies, Clarkia xantiana subspecies xantiana (hereafter xantiana), parviflora predominantly reproduces through self-pollination.\nNot all populations of parviflora self-fertilize at the same frequency. Dave has observed that populations sympatric with (i.e., occurring in the same area as) xantiana appear more likely to self-fertilize than allopatric populations (Figure 1). Over the past few years, we have conducted numerous studies to evaluate the hypothesis that this increased rate of self-fertilization has evolved via reinforcement as a mechanism to avoid hybridizing with xantiana.\nThroughout this book, I will use data related to the topic of divergence, speciation, and reinforcement between Clarkia subspecies as a path through biostatistics. I hope that this approach allows you to engage with the statistics while not having to keep pace with a bunch of different biological examples. Below, I introduce the major datasets that we will explore.\nFigure 2: Making a RIL population: A cross between individuals from two populations is followed by multiple generations of self-fertilization. As a result, each “line” becomes a mosaic of ancestry blocks inherited from either initial parent of the RIL. The figure above (from Behrouzi & Wit (2017)) illustrates this process, with the original parental chromosome segments depicted in green and red.\nTo investigate which traits, if any, help parviflora populations sympatric with xantiana avoid hybridization, Dave generated Recombinant Inbred Lines (RILs). To do so, he crossed a parviflora plant from “Sawmill Road”—a population sympatric with xantiana—with a parviflora plant from “Long Valley,” far from any xantiana populations. After this initial cross, lines were self-fertilized for eight generations. This process breaks up and shuffles genetic variation from the two parental populations while ensuring each line is genetically stable.\nBy setting these RILs out in the field and observing how many pollinators visited each line, we hope to identify which traits influence pollinator visitation and ultimately hybridization. Because parviflora plants often self-pollinate and because pollinators effectively transfer pollen from the plentiful xantiana plants to parviflora, we assume that greater pollinator visitation corresponds to higher hybrid seed set. However, we will test this assumption!!!",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RIL Data\nBelow is the RIL dataset. You can learn about the columns (in the Data dictionary tab) and browse the data (in the Data set tab). The full data are available at\nthis link. Aside from pollinator visitation and hybrid seed set, all phenotypes measured come not from the plants in the field, but means from replicates of the genotype grown in the greenhouse.\n\nRIL variationData DictionaryData set\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: An illustration o the variabiltiy in the recombinant inbred lines. Pictures by Taz Mueller and arranged by Brooke Kern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable_Name\nData_Type\nDescription\n\n\n\n\nril\nCategorical (Factor/String)\nIdentifier for Recombinant Inbred Line (RIL). This is the 'genotype'.\n\n\nlocation\nCategorical (Factor/String)\nField site where the plant was grown.\n\n\nprop_hybrid\nNumeric (discrete)\nProportion of genotyped seeds that were hybrids (see num_hybrid and offspring_genotyped for more information).\n\n\nmean_visits\nNumeric\nAverage number of pollinator visits per plant over a 15-minute observation.\n\n\ngrowth_rate\nNumeric\nGrowth rate of the plant.\n\n\npetal_color\nCategorical (Binary)\nPetal color phenotype (in this case 'pink' or 'white').\n\n\npetal_area_mm\nNumeric\nDate when the first flower opened (in Julian days, i.e., days since New Year's).\n\n\ndate_first_flw\nDate\nNode position of the first flower on the stem.\n\n\nnode_first_flw\nNumeric\nPetal area measured in square millimeters (mm²).\n\n\npetal_perim_mm\nNumeric\nPetal perimeter measured in millimeters (mm).\n\n\nasd_mm\nNumeric\nThe Anther-Stigma Distance (ASD) is the linear distance between the closest anther (the floral part that releases pollen) and the stigma (the floral part that accepts pollen) in a flower, measured in millimeters (mm). The smaller this distance, the more opportunity for self-fertilization.\n\n\nprotandry\nNumeric\nDegree of protandry (e.g., time difference between male and female phase) measured in days. More protandry means more outcrossing.\n\n\nstem_dia_mm\nNumeric\nStem diameter measured in millimeters (mm).\n\n\nlwc\nNumeric\nLeaf water content (LWC).\n\n\ncrossDir\nCategorical (Binary)\nCross direction\n\n\nnum_hybrid\nNumeric (discrete)\nThe number ofseeds that where hybrid.\n\n\noffspring_genotyped\nNumeric (discrete)\nThe number of seeds genotyped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRIL Hybridization Data\nBelow is the hybridization dataset. For each plant in the field we genotyped eight seeds at species-specific markers to identify if they were the product of hybridization with xantiana. The phenotypes belong to the genotype of the maternal plant (i.e. they are the same as those in the pollinator visitation data set). I include data at both the level of the seed and a summary at the level of the maternal plant.\n\n\nRIL Combined Data\n\n\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with partially observed genotype data using copula graphical models. Journal of the Royal Statistical Society: Series C (Applied Statistics), 68. https://doi.org/10.1111/rssc.12287",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html",
    "href": "book_sections/example_mini/varType.html",
    "title": "Types of Variables",
    "section": "",
    "text": "Explanatory and Response Variables\nAlthough these taxa hybridize in nature, they remain quite distinct, even in areas of sympatry where they exchange genes.\nDave Moeller and his colleagues (including me) have been studying this species for decades. Their research addresses fundamental questions in evolution, such as:\nTo answer these big and exciting questions, Dave and his collaborators must break them down into smaller, direct scientific studies that can be addressed through a combination of experiments and observations. To conduct such studies, we must map these complex ideas onto measurable variables.\nFor example, rather than directly comparing the flower images in Figure 1, we simplify these flowers into variables that summarize them. For instance, we could represent a flower using a set of variables such as flower color, petal length, the distance between stigma and style, etc.\nDave and his team have conducted numerous studies to tackle these big questions. In most cases, they examine how the value of a response variable — the outcome we aim to understand — changes with different values of one or more explanatory variables (also called predictor or independent variables), which are thought to influence or be associated with the biological process of interest.\nUnderstanding the distinction between explanatory and response variables is crucial for framing hypotheses, designing experiments, and interpreting statistical results. However, this distinction often depends on how the research question is framed and can even vary within a single study. For example, in a recent study on predictors of pollinator visitation in parviflora",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "href": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "title": "Types of Variables",
    "section": "",
    "text": "We first aimed to identify which loci in the genome predicted flower color and petal length in parviflora. Here, genotype was the explanatory variable, while the floral attributes (petal color and petal length) were the response variables.\nWe then asked whether certain floral attributes (petal color and petal length) predicted pollinator visitation to parviflora plants. In this case, the floral attributes became the explanatory variables, and pollinator visitation was the response variable.",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#types-of-variables",
    "href": "book_sections/example_mini/varType.html#types-of-variables",
    "title": "Types of Variables",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\n\n\n\n\n\n\nFigure 2: Which type of variable is color? Some variables (like color) can be treated as either categorical or continuous depending on the question. Because most parviflora plants are either pink or white, we treat color as a binary categorical variable. But, as shown above, color can be measured and analyzed quantiatively as well.\n\n\n\n\n\n# INSERT POLINATOR OBSERVATION VIDEO \n\nVariables come in different flavors, and knowing the flavor of a variable is key to choosing appropriate summary statistics, data visualizations, and statistical models. Our parviflora pollinator visitation example above included both major types of variables (Figure 1):\n\nNumeric variables are quantitative and have magnitude. For instance, we measured pollinator visits as the number of times a pollinator visited a flower during a [5-minute observation period], and petal length in centimeters.\n\nCategorical variables are qualitative. In our example, flower color and genotype were treated as categorical variables.\n\nLike much of stats – the line between these types of variables is blurry. For example, we often treat color as a category, but color can be measured quantitatively (Figure 2). So depending on our question we may want to treat color as either a numeric or categorical variable.\n\nNot All Numbers Are Numeric. For example, a gene ID may be represented as a number, but it is an arbitrary label rather than a measurement. Similarly, in our Clarkia studies, some sites were identified by numbers (e.g., Site 22 or Site 100). However, Site 22 is not “less than” Site 100 — these are categorical variables, despite being represented numerically.\n\nWithin these two categories are further sub-flavors which allow us to further refine our statistical approach:\n\nTypes of Numeric Variables\n\n\n\n\n\n\n\n\n\nFigure 3: Types of numeric variables.  A discrete variable: Xantiana flowers with four, five or six petals (photo courtesy of Dave Moeller).   A continuous variable: Parviflora petal whose length is being measured. Image from The University and Jepson Herbaria University of California, Berkeley. Copyright from ©2020 Chris Winchell. (image link)\n\n\n\n\nNumeric variables can be categorized into two main types:\n\nDiscrete variables come in chunks. For instance, flowers receive zero, one, two, three, and so on, pollinators. Pollinators do not come in fractions, making this variable inherently discrete.\nContinuous variables can take any value within a range. Classic examples include height, weight, and temperature. In our example, petal length is a continuous variable because it can be measured to any level of precision within its range.\n\n\nSubtle Distinctions and Blurry Boundaries. The two cases above represent pretty clear distinctions between discrete and continuous variables, but sometimes such distinctions are more subtle.\nConsider the number of pollinators—these cannot be fractional, but the number of pollinators per minute can be fractional. There are other similarly blurry cases. For example, time to first flower is inherently continuous, but we often check on flowers only once per day, so it is measured as discrete. Similarly, in human studies, age is usually reported in whole months or years (discrete), rather than on the more continuous scale of fractional seconds. In such cases, the appropriate question is not “is my data discrete or continuous?” but rather “what process generated my data? and what statistical distribution do the data follow?\n\n\n\nCategorical variables\n\n\n\n\n\n\n\n\n\nFigure 4: Species is a categorical variable: The Clarkia specialist – Clarkia Evening Bee, (Hesperapis regularis) on a Clarkia flower. Shared by © Gene H under CC BY-NC 4.0 copyright on iNaturalist. In our Clarkia research, pollinator is usually nominal (that is which bee species), but is sometimes binary (Clarkia specialist, or non-specialist), and sometimes ordinal (e.g. frequent pollinator, rare pollinator, never pollinator).\n\n\n\n\nCategorical variables are qualitative, and include, nominal, binary, and ordinal variables.\n\nNominal variables cannot be ordered and have names – like sample ID, species, study site etc…\n\nBinary variables are a type of nominal variable with only two options (or for which we only consider two options. Alive/dead, pass/fail, on/off are classic binary variables). In our example of pollinator visitation in parviflora, we considered only two flower colors (pink/white) so flower color in this case is binary.\n\nOrdinal variables can be ordered, but do not correspond to a magnitude. For example, bronze, silver and gold medals in the Olympics are ranked from best to worst, but first is not some reliable distance away from second or third etc… In our pollinator example, we often may wish to distinguish between frequent pollinators (e.g. specialist bees, Figure 4), common but less frequent pollinators (e.g. non-specialist bees), and rare/incidental pollinators (e.g. flies).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#closing-resources",
    "href": "book_sections/example_mini/varType.html#closing-resources",
    "title": "Types of Variables",
    "section": "Closing Resources",
    "text": "Closing Resources\n\nSummary\nUnderstanding the types of variables in our data can help us translate complex biological questions into measurable data that can be evaluated with the statistical tools we develop in this book. Variables can be categorized as numeric or categorical, and further subdivided into types like discrete, continuous, nominal, binary, or ordinal. These classifications influence how we summarize, visualize, analyze, and modelize our data.\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you’ve got what you needed from it.\n\n\n\nPractice Questions\nTry the questions below! Likert scales look like this:- How do you feel about Clarkia? (1) Love it  (2) Like it  (3) Don’t care  (4) Do not like (5) Hate it\n\nQ1) In a species with pink or white flowers, flower color is a special kind of categorical variable known as a ___ variable NominalBinaryOrdinalBimodal\n\n\nClick here for explanation\n\nThis was a little tricky! The correct answer is Binary, because there are only two possible values. If you picked either “nominal” or “bimodal,” you’re pretty close and definitely thinking along the right path, but not quite there! So let’s walk through these “not quite right” answers:\n\nNominal: Petal color is nominal in the sense that “pink” isn’t greater or lesser than “white”—the categories have no natural order. But because there are only two options here, the more specific (and better) description is Binary.\nBimodal: A bimodal distribution refers to a numeric variable that has two distinct peaks or clusters. If we had measured flower color quantitatively—say, by recording percent reflectance at 550 nm—and the data clustered around two values (say, “mostly pink” vs. “mostly white”), then the distribution would be bimodal. (But even then, we’d probably simplify it to binary for analysis.)\n\nIf you answered “ordinal,” you should probably take another look at the chapter—ordinal variables have a meaningful order, like “small,” “medium,” and “large.”\n\n.\nQ2) Which of these variables is best described as continuous? Flower color (pink, white)Petal lengthNumber of flowers on a plant\nQ3) The number of offspring produced by a single animal in one breeding season is: BinaryContinuousDiscrete\nQ4) TRUE or FALSE: Populations of Clarkia that we named 100 and 22 are numeric TRUEFALSE.\nQ5) TRUE or FALSE: A variable on a “Likert scale” (see margin for details) is clearly numeric TRUEFALSE.\n\n\nClick here for explanation\n\nThe word “clearly” is the key clue here. A Likert scale (like rating agreement from “Strongly disagree” to “Strongly agree”) is based on numbers (e.g., 1, 2, 3, 4, 5), but those numbers represent ordered categories, not truly continuous or clearly numeric values.\nIn other words, while you can treat Likert scale data like numbers sometimes (e.g., calculating averages), the numbers themselves are standing in for categories with an order—not for measured quantities along a true number line. So while you might have seen Likert data analyzed using means, t-tests, or even regressions—treating them like numeric variables—this is a common (and sometimes reasonable) modeling shortcut. Conceptually, Likert data are still clearly ordinal: they are ordered categories, not continuous measurements.\nIf you missed this, don’t worry — Likert scales can be a little tricky because they look numeric. But always pay close attention to what the numbers mean. If they’re just labeling ordered choices (rather than measuring something truly continuous, like height or weight), the variable is ordinal categorical, not clearly numeric.\n\n.\nQ6) The variable, kingdom (corresponding to one of the six kingdoms of life), is a ___ variable NominalBinaryOrdinalDiscrete\nQ7) TRUE of FALSE: A continuous variable can never be modeled as discrete and vice versa TRUEFALSE\n\n\n\n\nGlossary of Terms\n\n\nVariable: A characteristic or attribute that can take on different values or categories in a dataset.\n\nExplanatory Variable: Also known as a predictor or independent variable, this is a variable that is thought to influence or explain the variation in another variable.\nResponse Variable: Also known as a dependent variable, this is the outcome or effect being studied, which changes in response to the explanatory variable.\n\n\n\n\nNumeric Variable: A variable that represents measurable quantities and has magnitude, either as counts (discrete) or as continuous values.\n\nDiscrete Variable: A numeric variable that represents distinct, separate values or counts (e.g., number of pollinators).\nContinuous Variable: A numeric variable that can take any value within a range and is measured with precision (e.g., petal length).\n\n\n\n\nCategorical Variable: A variable that represents categories or groups and is qualitative in nature.\n\nNominal Variable: A categorical variable without an inherent order (e.g., flower species or study site).\nBinary Variable: A nominal variable with only two possible categories (e.g., alive/dead, pink/white).\nOrdinal Variable: A categorical variable with a meaningful order, but without measurable distances between levels (e.g., gold, silver, bronze).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html",
    "href": "book_sections/intro2r_index.html",
    "title": "SECTION I: Intro to R",
    "section": "",
    "text": "A “tidyverse” Approach\nFigure 2: A collection of Tidyverse hex stickers representing key R packages for data science, including dplyr, ggplot2, tidyr, readr, and more—each with a unique thematic design.\nAs R has evolved over time and its capabilities can be extended with packages (we will discuss this soon), different “dialects” of R have emerged. While many of you have likely seen Base R – built on the standard R program you download, here we will use Tidyverse – a specific and highly standardized set of packages designed for data science workflows (Figure 2). As a broad overgeneralization, Base R allows for much more control of what you are doing but requires more programming skill, while tidyverse allows you to do a lot with less programming skill.\nI focus on tidyverse programming, not because it is better than base R, but because learning tidyverse is a powerful way to do a lot without learning a lot of formal programming. This means that you will be well prepared for a lot of complex data analysis. If you continue to pursue advanced programming in R (or other languages) you will have some programming concepts to catch up on.\nIf you already know how to accomplish certain tasks with base R tools, I encourage you to invest the time in learning the equivalent approaches in tidyverse. While it may feel redundant at first, this foundation knowledge will make you a more versatile and effective R programmer in the long term, and will allow you to make sense of what we do throughout the term.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "href": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "title": "SECTION I: Intro to R",
    "section": "Important hints for R coding",
    "text": "Important hints for R coding\nYears of learning and teaching have taught me the following key points about learning R. These amount to the simple observation that a student’s mindset and attitude towards learning and using R is the most important key to their success. I summarize these tips in the video and bullet points, below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Replace your automatic negative thoughts with balanced alternatives.\n\n\n\n\n\nBe patient with yourself. Every expert R programmer started exactly where you are now. Your understanding will grow naturally as you tackle real problems and challenges. Do not beat yourself up, you are learning. Replace automatic negative thoughts with balanced thoughts (Figure 3).\nR is literally a language. Languages take a while to learn – at first, looking at an unfamiliar alphabet or hearing people speak a foreign language makes no sense. Aluksi vieraan aakkoston katsominen tai vieraan kielen puhumisen kuuleminen ei tunnu lainkaan järkevältä. With time and effort, you can make sense of a bunch of words and sentences but it takes time. You are not dumb for not understanding the sentence I pasted above (and if you do understand it is because you know Finnish, not because you are smart).\nYou don’t need to memorize anything. You have access to dictionaries, translators, LLMs etc etc. That said, these tools or more useful the more you know.\nDo not compare yourself to others. R will come fast to some, and slower to others. This has absolutely nothing to do with either your intelligence or your long-term potential as a competent R user.\nStart small and don’t be afraid to experiment. There is nothing wrong about typing code that is imperfect and/or does not work out. Start with the simplest way of addressing your problem and see how far you get. Start small, maybe with some basic data analysis or creating a simple plot. Each little victory builds your confidence. You can always try new and more complex approaches as you go.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#whats-ahead",
    "href": "book_sections/intro2r_index.html#whats-ahead",
    "title": "SECTION I: Intro to R",
    "section": "What’s ahead?",
    "text": "What’s ahead?\n\n\n\n\n\n\n\n\nFigure 4: A pretty scene of Clarkia’s home showing the world we get to investigate as we get equipped with R.\n\n\n\n\n\nNow we begin our intro to R. While we will keep practicing what we have learned and learning new R stuff all term, the next four chapters, listed below will get you started:\n\nGetting up and Running. This section introduces RStudio, math in R, vectors, variable assignment, using functions, r packages, loading data (from the internet), and data types. There is a lot here!\nData in R. Here we continue on our introduction to R. We first introduce the concept of tidy data, and introduce the capabilities of the tidyverse package, dplyr.\nIntro to ggplot. The ggplot package allows us to make nice plots quickly. We will get started understanding how ggplot thinks, and introduce the wide variety of figures you can make. Later in this term we will make better figures in ggplot.\nReproducible science. We consider how to collect data, and store it in a folder. We then introduce the concept of R projects and loading data from our computer. Finally, we introduce the idea of saving R scripts.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html",
    "href": "book_sections/getting_started.html",
    "title": "1. Getting started with R",
    "section": "",
    "text": "What is R? What is RStudio?\nR is a computer program built for data analysis. As opposed to GUIs, like Excel, or click-based stats programs, R is focused on writing and sharing scripts. This enables us to be shared and replicate analyses, ensuring that data manipulation occurs in a script. This practice both preserving the integrity of the original data, while providing tremendous flexibility. R has become the computer language of choice for most statistical work because it’s free, allows for reproducible analyses, makes great figures, and has many “packages” that support the integration of novel statistical approaches. In a recent paper, we used R to analyze hundreds of Clarkia genomes and learn about the (Figure 1 from Sianta et al. (2024)). RStudio is an Integrated Development Environment (IDE)—a nice setup to interact with R and make it easier to use.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "href": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "title": "1. Getting started with R",
    "section": "",
    "text": "More precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\n— From Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Ismay & Kim, 2019)",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "href": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "title": "1. Getting started with R",
    "section": "The Shortest Introduction to R",
    "text": "The Shortest Introduction to R\nBefore opening RStudio, let’s get familiar with two key ays we use R – (1) Using R as a calculator, and (2) Storing information by assigning values to variables.\nR can perform simple (or complex) calculations. For example, entering 1 + 1 returns 2, and entering 2^3 (two raised to the power of three) returns 8. Try it yourself by running the code below, and then experiment with other simple calculations.Math in R: See posit's recipe for using R as a calculator for more detail.\nCommenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!Commenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!\nChallengeSolution\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nStoring values in variables allows for efficient (and less error-prone) analyses, while paving the way to more complex calculations. In R, we assign values to variables using the assignment operator, &lt;-. For example, to store the value 1 in a variable named x, type x &lt;- 1. Now, 2 * x will return 2.\n\n\nx &lt;- 1 # Assign 1 to x\n2 *  x # Multiply x by 2\n\n[1] 2\n\nBut R must “know” something before it can “remember” it. The code below aims to set y equal to five, and see what y plus one is (it should be six). However, it returns an error. Run the code to see the error message, then fix it!\n\nChallengeSolution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nR reads and executes each line of code sequentially, from top to bottom. Think about what y + 1 means to R if it hasn’t seen a definition of y yet.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nIn R, variables must be defined before they are used. When you try to use y + 1 before assigning a value to y, R throws an error because it doesn’t know what y is yet. When we switch the order—assigning y &lt;- 5 before using y + 1—R understands the command and evaluates it properly.\n\n\n\n\nNow, try assigning different numbers to x and y, or even using them together in a calculation, such as x + y. Understanding this concept of assigning values is critical to understanding how to use R.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#lets-get-started-with-r",
    "href": "book_sections/getting_started.html#lets-get-started-with-r",
    "title": "1. Getting started with R",
    "section": "Let’s get started with R",
    "text": "Let’s get started with R\nThe following sections introduce the very basics of R including:\n\nFunctions and vectors in R.\n\nLoading packages and data into R.\n\nData types in R.\n\nAn orientation to RStudio.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of introgression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is determined by temporal environmental variation and mating system. Proceedings of the National Academy of Sciences, 121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html",
    "href": "book_sections/getting_started/functions_and_vectors.html",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R Functions\nR comes with tons of built-in functions that do everything from basic math to advanced statistical modeling. So not only can we calculate the mean and variance in Clarkia xantiana petal lengths with the mean() and var() functions, respectively, but we can test the null hypothesis that mean petal size in xantiana is equal to that of parviflora with the t.test() function. Functions are the foundation of how we do things in R – they save time and ensure consistency across your analyses.\nFunctions take arguments, which we put in parentheses. When typing sqrt(25), sqrt() is the function, 25 is the argument, and 5 is the output.\nFunctions can take multiple arguments: If you don’t specify them all, R will either tell you to provide them, or assumes default values. For example, the log function defaults to the natural log (base e), so log(1000) returns 6.908. If you want the logarithm with base 10, you need to specify it explicitly as log(1000, 10), which returns 3. Note that argument order matters:—log(10, 1000) returns 0.3333333, while log(1000, 10) returns 3.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "href": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R functions: See posit's recipe for R functions for more detail.\n\n\n\n\n# Natural log of 1000\nlog(1000)             \n\n[1] 6.907755\n\n\n\n\n# Log base 1000 of 10\nlog(1000, base = 10)  \n\n[1] 3\n\n\nTips for using functions in R\n\nUse named arguments in functions.\nFor example, typing log(1000, base = 10) makes what each value represents obvious (improving code readability), and allows flexibility in argument order (e.g. log(base = 10, 1000) gives the same value as log(1000, base = 10)). Thus, using named arguments makes your code readable and robust.\n\n\nUse = to assign arguments in functions\nWhen specifying arguments inside a function, always use = (e.g., log(1000, base = 10)). Do not use &lt;-, which is for assigning values to variables. Otherwise, R might mistakenly store the argument as a variable, leading to unexpected results.\n\n\nPipe together functions with |&gt;\nThe pipe, |&gt;, provides a clean way to pass the output of one function into another. For example, we can find the square root of the \\(\\text{log}_{10}\\) of 1000, rounded to two decimal places, as follows:\n\nlog(1000, base = 10)   |&gt;  \n    sqrt()             |&gt;  \n    round(digits = 2)\n\n[1] 1.73\n\n\nNotice that we did not explicitly provide an argument to sqrt() — it simply used the output of log(1000, base = 10). Similarly, the round() function then rounded the square root of 3 to two decimal places.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "href": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "title": "• 1. Functions and vectors",
    "section": "Working with vectors",
    "text": "Working with vectors\nIf we observed one Clarkia plant with one flower, a second with two flowers, a third with three flowers, and a fourth with two flowers, we could find the mean number of flowers as (1 + 2 + 3 + 2)/4 = 2, but this would be tedious and error-prone. It would be easier to store these values in an ordered sequence of values (called a vector) and then use the (mean()) function.\nVectors are the primary way that data is stored in R—even more complex data structures are often built from vectors. We create vectors with the combine function, c(), which takes arguments that are the values in the vector.\n\n# A vector of flower numbers\n  # 1st plant has one flower\n  # 2nd plant has two flowers\n  # 3rd plant has three flowers\n  # 4th plant has two flowers\nnum_flowers &lt;- c(1, 2, 3, 2)  # Create a vector for number of flowers per plant\nmean(num_flowers) # finding the mean flower number\n\n[1] 2\n\n\n\n\n# If each flower produces four petals  \nnum_petals &lt;- 4 * num_flowers\nnum_petals\n\n[1]  4  8 12  8\n\n\n# If we wanted the log_2 of petal number \nlog(num_petals, base = 2) |&gt;\n  round(digits = 3)\n\n[1] 2.000 3.000 3.585 3.000\n\n\n\nVariable assignment can be optional: In the code, I assigned observations to the vector, num_flowers, and then found the mean. But we could have skipped variable assignment—variable assignment — mean(c(1, 2, 3, 2)) also returns 2.\nThere are two good reasons not to skip variable assignment:\n\nVariable assignment makes code easier to understand. If I revisited my code in weeks I would know what the mean of this vector meant.\nVariable assignment allows us to easily reuse the information For example, below I can easily find the mean petal number.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html",
    "href": "book_sections/getting_started/loading_packages_and_data.html",
    "title": "• 1. Load packages and data",
    "section": "",
    "text": "R packages\nWhile R has many built-in functions, packages provide even more functions to extend R’s capabilities. Packages can offer alternative (often more efficient and user-friendly) approaches to tasks that can be done with base R functions, or they can enable entirely new functionality that is not included in base R at all. In fact, R packages are a major way that the latest statistical and computational methods in various fields are shared with practitioners.\nBelow I introduce the readr, and dplyr packages. Because these packages are so useful for streamlining data import, manipulation, and cleaning, I use them in nearly every R project. I also introduce the conflicted package, which identifies any functions with shared names across packages, and allows us to tell R which function we mean when more than one function has the same name.\nInstall a package the first time you use it The first time you need a package, install it with the install.packages() function. Here the argument is the package (or vector of packages) you want to install. So, to install the packages above, type:\n# We do this the first time we need a package.\ninstall.packages(c(\"readr\", \"dplyr\", \"conflicted\"))\nLoad installed packages every time you open RStudio You only install a package once, but you must use the library() function, as I demonstrate below, to load installed packages every time you open R.\n# We do this every time we open R and want to use these packages.\nlibrary(conflicted)\nlibrary(readr)\nlibrary(dplyr)",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "href": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "title": "• 1. Load packages and data",
    "section": "Reading data into R",
    "text": "Reading data into R\nRather than typing large datasets into R, we usually want to read in data that is already stored somewhere. For now, we will load data saved as a csv file from the internet with the read_csv(link) structure from the readr package. Later, we will revisit the challenge of importing data from other file types and locations into R.\n\n\nLoading data: See posit's recipe for importing data for more detail. Note also that read.csv() is a base R function similar to read_csv(), but it behaves a bit differently – for example it reads data in as a dataframe, not a tibble.\nBelow, I show an example of reading pollinator visitation data from a link on my GitHub. After loading a dataset, you can see the first ten lines and all the columns that fit by simply typing its name. Alternatively, the View() function opens up the full spreadsheet for you to peruse.\n\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data\n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;\n\n\n\n\n\n\npackage::function() format: I read in the data with the read_csv() function in the readr package by typing: readr::read_csv(), but typing read_csv() gives the same result. The package::function() format comes in handy when two functions in different packages have the same name.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/data_types.html",
    "href": "book_sections/getting_started/data_types.html",
    "title": "• 1. Data types in R",
    "section": "",
    "text": "Motivating scenario: You have loaded data into R and are curious about what “types” of data R thinks it is working with.\nLearning goals: By the end of this sub-chapter you should be able to\n\nList the different types of variables that R can keep in a vector.\n\nIdentify which type of variable is in a given column.\n\nAsk logical questions of the data to make a logical vector.\n\n\n\n\nR handles different types of data in specific ways. Understanding these data types is crucial because what you can do with your data depends on how R interprets it. For example, although you know that 1 + \"two\" equals 3, R cannot add a number and a word. So, to use R effectively, you will need to make sure the type of data R has in memory matches the type it needs to have to do what you want. This will also help you understand R’s error messages and confusing results when things don’t work as expected.\nLooking back at the pollinator visitation dataset we loaded above, we see that (if you read it in with read_csv()) R tells you the class of each column before showing you its first few values. In that dataset, columns one, two, five, six, and fifteen (note R provides a peek of the first few variables — in this case, seven — and then provides the names and data type for the rest) — location, ril, growth_rate, and petal_color — are of class &lt;chr&gt;, while all other columns contain numbers (data of class &lt;dbl&gt;). What does this mean? Well it tells you what type of data R thinks is in that column. Here are the most common options:\n\nNumeric: Numbers, including doubles (&lt;dbl&gt;) and integers (&lt;int&gt;). Integers keep track of whole numbers, while doubles keep track of decimals (but R often stores whole numbers as doubles).\nCharacter: Text, such as letters, words, and phrases (&lt;chr&gt;, e.g., \"pink\" or \"Clarkia xantiana\").\n\nLogical: Boolean values—TRUE or FALSE (&lt;logi&gt;), often used for comparisons and conditional statements.\n\nFactors: Categorical variables that store predefined levels, often used in statistical modeling. While they resemble character data, they behave differently in analyses. We will ignore them in this chapter but revisit them later.\n\nWhen you load data into R, you should always check to ensure that the data are in the expected format. Here we are surprised to see that growth_rate is a character, because it should be a number. A close inspection shows that in row three someone accidentally entered the letter O instead of the number zero (0) in what should be 1.80.\n\n\n\n# A tibble: 4 × 2\n  ril   growth_rate\n  &lt;chr&gt; &lt;chr&gt;      \n1 A1    1.272      \n2 A100  1.448      \n3 A102  1.8O       \n4 A104  0.816      \n\n\nAsking logical questions We often generate logical variables by asking logical questions of the data. Here is how you do that in R.\n\n\n\nQuestion\nR Syntax\n\n\n\n\nDoes a equal b?\na == b\n\n\nDoes a not equal b?\na != b\n\n\nIs a greater than b?\na &gt; b\n\n\nIs a less than b?\na &lt; b\n\n\nIs a greater than or equal to b?\na &gt;= b\n\n\nIs a less than or equal to b?\na &lt;= b",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Data types in R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/rstudio_orientation.html",
    "href": "book_sections/getting_started/rstudio_orientation.html",
    "title": "• 1. Orientation to RStudio",
    "section": "",
    "text": "Motivating scenario: You have just downloaded R and RStudio, and want to understand all the stuff that you see when you open RStudio.\nLearning goals: By the end of this sub-chapter you should be able to\n\nIdentify the source pane and what to do there.\n\nIdentify the terminal pane and what to see and do there.\n\nIdentify the environment / history pane, what to see and do there, and how to navigate tabs in this pane.\n\nIdentify the file / plot / help / viewer pane, what to see and do there, and how to navigate tabs in this pane.\n\n\n\n\n\n\nAbove, you ran R in this web browser, but more often you will work with R in RStudio. When you open RStudio for the first time, you will see three primary panes. The one on the left works identically to the basic R console. Navigating to ‘File &gt; New File &gt; R Script’ opens a new script and reveals a fourth pane.\n\n\nR Scripts are ways to keep a record of your code so that you can pick up where you left off, build on previous work, and share your efforts. We will introduce R Scripts more formally soon!\n\n\n\nLike the R console above (and all computer languages) RStudio does not “know” what you wrote until you enter it into memory. There are a few ways to do this, but our preferred way is to highlight the code you intend to run, and then click the Run button in the top right portion of the R script pane. Alternatively, press Ctrl+Return for Windows/Linux or ⌘+Return on OS X.\n\n\n\n\n\n\n\n\nFigure 1: More panes = less pain. A brief tour of RStudio’s panes.\n\n\n\n\n\nFigure 1 shows what your RStudio session might look like after doing just a little bit of work:\n\nThe source pane Pane 1 is used for writing and editing scripts, R Markdown files etc. This is where you write reproducible code that can be saved and reused.\nThe console pane Pane 2 is basically the R command prompt from vanilla R, it is where you directly interact with R. You can type commands here to execute them immediately. It will display output, messages, and error logs.\nThe environment / history pane Pane 3 shows what R has in working memory and what it has done.\n\nThe Environment Tab shows all objects (e.g., data frames, vectors) currently in memory.\n\nThe History Tab shows all the commands you have run in your session. You can even search through your history, which can be easier than scrolling through the console.\n\nThe files / plots / help / viewer pane. Pane 4 is remarkably useful!\n\nThe Plots Tab shows the plots generated during your session. You can delete an individual plot by clicking the red X button, or delete all plots by clicking the broom button.\nThe Help Tab: allows you to access documentation and help files.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Orientation to RStudio"
    ]
  },
  {
    "objectID": "book_sections/getting_started/getting_started_summary.html",
    "href": "book_sections/getting_started/getting_started_summary.html",
    "title": "• 1. Getting started summary",
    "section": "",
    "text": "Figure 1: Some pretty R from Allison Horst.\n\n\n\n\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. Additional resources.\n\nChapter summary\nR is (much more than just) a simple calculator – it can keep track of variables, and has functions to make plots, summarize data, and build statistical models. R also has many packages that can extend its capabilities. Now that we are familiar with R, RStudio, vectors, functions, data types and packages, we are ready to build our R skills even further to work with data!\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\n\n\n\n\n\n\n\n\n\nFigure 2: Some encouragement from Allison Horst.\n\n\n\n\nThe interactive R environment below allows you to work without switching tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) Entering \"p\"^2 into R produces which error?\n\n What error? It works great? Error: object p not found Error: object of type closure is not subsettable Error in “p”^2 : non-numeric argument to binary operator\n\nQ2) Which logical question provides an unexpected answer?\n\n (2.0 + 1.0) == 3.0 (0.2 + 0.1) == 0.3 2^2 &gt; 8 (1/0) == (10 * 1/0)\n\n\n\nClick here for an explanation\n\nThis is a floating-point precision issue. In R (and most programming languages), some decimal values cannot be represented exactly in the binary code that they use under the hood. To see this, try (0.2 + 0.1) - 0.3:\n\n(0.2 + 0.1) - 0.3\n\n[1] 5.551115e-17\n\n\nIf you are worried about floating point errors, use the all.equal() function instead of ==, or round to 10 decimal places before asking logical questions.\n\nQ3) R has a built-in dataset called iris. You can look at it or give it to functions by typing iris. Which variable type is the Species in the iris dataset?\n\n numeric logical character factor\n\nFor the following questions consider the diabetes dataset available at: https://rb.gy/fan785\nQ4) Which variable in the diabetes dataset is a character but should be a number?:\n\n ratio location age frame none\n\nQ5) True OR False: The numeric variable, bp.1d, is a double, but could be changed to an integer without changing any of our analyses: TRUEFALSE\nQ6) Which categorical variable in the dataset is ordinal?\n\n id location gender frame\n\nQ7) You collected five leaves of the wild grape (Vitis riparia) and measured their length and width. You have a table of lengths and widths of each leaf and a formula for grape leaf area (below).\nThe area of a grape leaf is: \\[\\text{leaf area } = 0.851 \\times \\text{ leaf length } \\times \\text{ leaf width}\\] The data are here, each column is a leaf:\n\n\n\n\n\nlength\n5.0\n6.1\n5.8\n4.9\n6.0\n\n\nwidth\n3.2\n3.0\n4.1\n2.9\n4.5\n\n\n\n\n\nThe mean leaf area is \n\n\nClick here for a hint\n\n\nFirst make vectors for length and width\n\nlength = c(5, 6.1, 5.8, 4.9, 6)\nwidth = c(3.2, 3, 4.1, 2.9, 4.5)\nThen multiply these vectors by each other and 0.851.\nFinally find the mean\n\n\n\n\nClick here for the solution\n\n\n# Create length and width vectors\nlength &lt;- c(5, 6.1, 5.8, 4.9, 6)\nwidth &lt;- c(3.2, 3, 4.1, 2.9, 4.5)\n\n\nleaf_areas &lt;- 0.851 * length * width # find area\nmean(leaf_areas)                     # find mean\n\n[1] 16.89916\n\n# or in one step:\n(0.851 * length * width) |&gt;\n  mean()\n\n[1] 16.89916\n\n\n\n\n\n\n\nGlossary of Terms\n\n\nR: A programming language designed for statistical computing and data analysis.\nRStudio: An Integrated Development Environment (IDE) that makes using R more user-friendly.\nVector: An ordered sequence of values of the same data type in R.\nAssignment Operator (&lt;-): Used to store a value in a variable.\nLogical Operator: A symbol used to compare values and return TRUE or FALSE (e.g., ==, !=, &gt;, &lt;).\nNumeric Variable: A variable that represents numbers, either as whole numbers (integers) or decimals (doubles).\nCharacter Variable: A variable that stores text (e.g., \"Clarkia xantiana\").\nPackage: A collection of R functions and data sets that extend R’s capabilities.\n\n\n\n\n\nNew R functions\n\n\nc(): Combines values into a vector.\ninstall.packages(): Installs an R package.\nlibrary(): Loads an installed R package for use.\nlog(): Computes the logarithm of a number, with an optional base.\nmean(): Calculates the average (mean) of a numeric vector.\nread_csv() (readr): Reads a CSV file into R as a data frame.\nround(): Rounds a number to a specified number of decimal places.\nsqrt(): Finds the square root of a number.\nView(): Opens a data frame in a spreadsheet-style viewer.\n\n\n\n\n\nR Packages Introduced\n\n\nbase: The core R package that provides fundamental functions like c(), log(), sqrt(), and round().\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\nconflicted: Helps resolve function name conflicts when multiple packages have functions with the same name.\n\n\n\n\nAdditional resources\nThese optional resources reinforce or go beyond what we have learned.\n\nR Recipes:\n\nDoing math in R.\n\nUsing R functions.\n\nImporting data from a .csv.\n\nVideos:\n\nCoding your Data Analysis for Success (From Stat454).\n\nWhy use R? (Yaniv Talking).\n\nAccessing R and RStudio (Yaniv Talking).\n\nRStudio orientation (Yaniv Talking).\n\nR functions (Yaniv Talking).\n\nR packages (Yaniv Talking).\n\nLoading data into R (Yaniv Talking).\n\nData types (Yaniv Talking). Uses compression data as an example.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Getting started summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html",
    "href": "book_sections/data_in_R.html",
    "title": "2. Data in R",
    "section": "",
    "text": "Tidy data\nFigure 2: A visual explanation of tidy data. Modified from Wickham (2014).\nData can be structured in different ways: in a tidy format, each variable has its own column, and each row represents an observation. In contrast, messy data might combine multiple variables into a single column or store observations in a less structured format. Figure 2 A shows “long” data with one variable per column. Figure 2 B contains boxes (rather than rows or columns) with petals from a given flower laid out neatly, and information about the flower and plant written beneath it. Both formats have their costs and benefits:\nNote that the tidy data format is not necessarily “prettier” or easier to read – in fact, in visual presentation of data for people, we often choose an untidy format. But when analyzing data on our computer, a tidy format simplifies our work. For this reason we will work with tidy data when possible in this book.\nFigure 3: An example of tidy versus untidy data. A) A table where each row is an observation (a petal), and each column is a variable (e.g. pop, plant, image etc…). B) A nicely arranged (but not tidy) sheet of Clarkia xantiana petals - arranged by flower.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tidy-data",
    "href": "book_sections/data_in_R.html#tidy-data",
    "title": "2. Data in R",
    "section": "",
    "text": "Like families, tidy datasets are all alike but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\nHadley Wickham. Tidy data. Wickham (2014).\n\n\n\nFigure 2 A is “tidy”: Each row is an observation (a petal), and each column is a variable related to that observation. Because this style is so predictable, this format simplifies computational analyses.\n\nFigure 2 B is not “tidy”: There are not simple rows and columns, and variables are combined in a long string. This format is useful in many ways—for example, humans can easily identify patterns, and data can be stored compactly.\n\n\n\n\nBecause all untidy data are different, there is no way to uniformly tidy an untidy dataset. However, the tidyr package has many useful functions. Specifically, the pivot_longer() function allows for converting data from wide format to long format.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tibbles",
    "href": "book_sections/data_in_R.html#tibbles",
    "title": "2. Data in R",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble is the name for the primary structure that holds data in the tidyverse. A tibble—much like a spreadsheet—does not automatically make data tidy, but encourages a structured, consistent format that works well with tidyverse functions.\n\nIn a tibble, each column is a vector. This means that all entries in a column must be of the same class. If you mix numeric and character values in a column, every entry becomes a character.\nIn a tibble, each row unites observations. A row can have any mix of data types.\n\n\n\nTibbles vs. Data Frames For base R users – A tibble is much like a data frame, but some minor features distinguish them. See Chapter 10 of Grolemund & Wickham (2018) for more info.\n\n\n\n\n\n\n\n\nFeature\nTibble\nData Frame\n\n\n\n\nWhat you see on screen\nFirst ten rows & cols that fit\nEntire dataset\n\n\nData Types Displayed\nYes – &lt;dbl&gt;, &lt;chr&gt;, etc\nNo\n\n\nSubsetting to one column returns\nA tibble\nA vector\n\n\n\nThe read_csv() function that we introduced earlier to load data imports data as a tibble. Looking at the data below, you are probably surprised to see that growth rate is a character &lt;chr&gt;, because it should be a number &lt;dbl&gt;. A little digging reveals that the entry in the third row has a growth rate of 1.8O (with the letter, O, at the end) which should be 1.80 (with the number 0 at the end)\n\nlibrary(readr)\nlibrary(dplyr)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data \n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "href": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "title": "2. Data in R",
    "section": "Let’s get ready to deal with data in R",
    "text": "Let’s get ready to deal with data in R\nThe following sections introduce the very basics of R including:\n\nAdding columns with mutate.\n\nSelecting columns.\n\nSummarizing columns.\n\nChoosing rows.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html",
    "href": "book_sections/data_in_R/add_vars.html",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Changing or adding variables with mutate()\nOften we want to change the values in a column, or make a new column. For example in our data we may hope to:\nThe mutate() function in the dplyr package can solve this. You can overwrite data in an existing column or make a new column as follows:\nril_data      |&gt;\n  dplyr::mutate(growth_rate = as.numeric(growth_rate),   # make numeric\n                visited = mean_visits &gt; 0)\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `growth_rate = as.numeric(growth_rate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25       NA     pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;\nAfter confirming this worked, we can assign it to R’s memory: In doing so, I even converted 1.8O into 1.80 so we have an observation in that cell rather than missing data.\nril_data       &lt;- ril_data      |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25        1.8   pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "href": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Convert growth_rate into a number, with the as.numeric() function.\nAdd the logical variable, visited, which is TRUE if a plant had more than zero pollinators visit them, and is FALSE otherwise.\n\n\n\n\nWarning… ! NAs introduced by coercion:\nYou can see that R gave us a warning. Warnings do not mean that something necessarily went wrong, but they do mean we should look and see what happened. In this case, we see that when trying to change the character string, 1.8O, into a number R did not know what to do and converted it to NA. In the next bit of code I convert it into \"1.80\" with the case_when() function.\n\n\n\n\n\n\n\n\n\n\nBe careful combining classes with case_when() Click the arrow to learn more\n\n\n\n\n\nWhen I was trying to change the character “1.8O” into the 1.80, R kept saying: Error in dplyr::mutate()… Caused by error in case_when(): ! Can’t combine ..1 (right)  and ..2 (right) . Unlike warnings, which tell you to watch out, errors tell you R cannot do what you’re asking of it. It turns out that I could not assign the number 1.80 to the vector held in petal_area_mm because I could not blend characters add numbers. So, as you can see, I replaced \"1.8O\" with \"1.80\", and then I used as.numeric() to convert the vector to numeric.",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html",
    "href": "book_sections/data_in_R/select_vars.html",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "select()ing columns of interest\nThe dataset above is not tiny – seventeen columns accompany the 593 rows of data. To simplify our lives, let’s use the dplyr function, select(), to limit our data to a few variables of interest:\nril_data |&gt; \n  dplyr::select(location,   prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm, \n                growth_rate, visited)\n\n# A tibble: 593 × 8\n   location prop_hybrid mean_visits petal_color petal_area_mm asd_mm growth_rate\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 GC             0           0     white                44.0  0.447       1.27 \n 2 GC             0.125       0.188 pink                 55.8  1.07        1.45 \n 3 GC             0.25        0.25  pink                 51.7  0.674       1.8  \n 4 GC             0           0     white                57.3  0.959       0.816\n 5 GC             0           0     white                68.6  1.41        0.728\n 6 GC             0.125       0     pink                 66.3  0.788       1.76 \n 7 GC            NA          NA     &lt;NA&gt;                 51.5  0.6         1.58 \n 8 GC             0           0     white                48.1  0.561       1.48 \n 9 GC             0          NA     white                51.6  1.02        1.14 \n10 GC             0.25        0     white                89.8  0.618       1    \n# ℹ 583 more rows\n# ℹ 1 more variable: visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "href": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "location: The plant’s location. The pollinator visitation experiment was limited to two locations (either SR or GC), while the hybrid seed formation study was replicated at four locations (SR, GC, LB or US). This should be a &lt;chr&gt; (character), and it is!\nprop_hybrid: The proportion of genotyped seeds that were hybrids.\n\nmean_visits: The mean number of pollinator visits recorded (per fifteen minute pollinator observation) for that RIL genotype at that site. This should be a number &lt;dbl&gt; (double), and it is.\npetal_area_mm: The area of the petals (in mm). This should be a number &lt;dbl&gt; (double), and it is!\n\nasd_mm: The distance between anther (the place where pollen comes from) and stigma (the place that pollen goes to) on a flower. The smaller this number, the easier it is for a plant to pollinated itself. This should be a number &lt;dbl&gt; (double), and it is.\ngrowth_rate: The variable we should have just fixed now it should be a number.\n\nvisited: A logical variable indicating if the plant received any visits at all.\n\n\n\n\n\n\n\n\nWarning: R does not remember this change until you assign it.\n\n\n\nSo, now that we see that our code worked as expected, enter:\n\nril_data &lt;- ril_data |&gt; \n  dplyr::select(location,  prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm,  \n                growth_rate, visited)",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html",
    "href": "book_sections/data_in_R/summarize_vars.html",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "summarize()ing data\nWe rarely want to look at entire datasets, we want to summarize() them (e.g. finding the mean, variance, etc..).\nWe previously used the mean function to find the mean of a vector. When we want to summarize a variable in a tibble we use the function inside of summarize().\nComputing summary statistics with summarize(). The top table contains two columns: prop_hyb (proportion of hybrids) and n_hyb (the number of hybrids). The summarize(mean_hyb = mean(n_hyb)) function is applied to calculate the mean of n_hyb, producing a single-row output where mean_hyb represents the average number of hybrids across the dataset. The final result, shown in the bottom table, contains a single value of 1.\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1         NA\nWe notice two things.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "href": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "The answer was NA. This is because there are NAs in the data.\nThe results are a tibble. This is sometimes what we want and sometimes not. If you want a vector you can pull() the value.\n\n\nSummarize data in the face of NAs with the na.rm = TRUE argument.\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\npull() columns from tibbles to extract vectors.\nMany R functions require vectors rather than tibbles. You can pull() them out as follows:\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))|&gt;\n  pull()\n\n[1] 0.693259\n\n\n\n\n\n\n\n\n\n“A visual explanation of the summarize() function from R for the rest of us.\n\n\n\n\n\n\n\n\n\n\nA visual explanation of the summarize() function from R for the rest of us.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "href": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "title": "• 2. Summarizing columns",
    "section": "Combine group_by() andsummarize() to describe groups",
    "text": "Combine group_by() andsummarize() to describe groups\nSay we were curious about differences in pollinator visitation by location. The code below combines group_by() and summarize() to show that site SR had nearly 10 times the mean pollinator visitation per 15 minute observation than did site GC. We also see a much stronger correlation between petal area and visitation in SR than in GC, but a stronger correlation between proportion hybrid and visitation in GC than in SR. Note that the NA values for LB and US arise because we did not conduct pollinator observations at those locations.\n\nril_data      |&gt;\n  group_by(location) |&gt;\n  summarize(grand_mean = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"),\n            cor_visits_prop_hybrid =  cor(mean_visits , prop_hybrid, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n# A tibble: 5 × 4\n  location grand_mean cor_visits_petal_area cor_visits_prop_hybrid\n  &lt;chr&gt;         &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n1 GC            0.116                0.0575                  0.458\n2 LB          NaN                   NA                      NA    \n3 SR            1.27                 0.367                   0.281\n4 US          NaN                   NA                      NA    \n5 &lt;NA&gt;        NaN                   NA                      NA    \n\n\nWe can group by more than one variable. Grouping by location and color reveals not only that white flowers are visited less than pink flowers, but also that petal area has a similar correlation with pollinator visitation for pink and white flowers.\n\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 13 × 4\n# Groups:   location [5]\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA     \n\n\n\n\n\n\n\n\nA visual explanation of group_by() + summarize() from R for the rest of us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nungroup()\nAfter summarizing, the data above are still grouped by location. You can see this under #A tibble: 6 x 4 where it says # Groups:   location [5]. This tells us that the data are still grouped by location (these groups correspond to GC, SR, US, LB and missing location information NA). It’s good practice to ungroup() next, so that R does not do anything unexpected.\n\n\nPeeling of groups: Above we grouped by location and petal_color in that order. When summarize data, by default, R peels off one group, following a “last one in is the first one out” rule. This is what is meant when R says: “summarise() has grouped output by ‘location’…”.\n\n# re-running code above and then ungrouping it.\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 13 × 4\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html",
    "href": "book_sections/data_in_R/choose_rows.html",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "Remove rows with filter()\nThere are reasons to remove rows by their values. For example, we could remove plants from US and LB locations. We can achieve this with the filter() function as follows:\nFigure 1: Using filter() to subset data based on a condition. The top table contains two columns: prop_hyb (proportion of hybrids) and petal_color (flower color), with values including both “white” and “pink” flowers. The function filter(petal_color == \"pink\") is applied to retain only rows where petal_color is “pink.” The resulting dataset, shown in the bottom table, excludes the “white” flower row and keeps only the observations where petal color is “pink.”\nOR, equivalently\n# rerunning the code summarizing visitation by \n# location and petal color and then ungrouping it\n# but filtering out plants from locations US and LB\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  filter(location != \"US\" & location != \"LB\")|&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 4\n  location petal_color avg_visits cor_visits_petal_area\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n1 GC       pink            0.193                 0.0899\n2 GC       white           0.0104                0.0886\n3 GC       &lt;NA&gt;            0.2                   0.443 \n4 SR       pink            1.76                  0.387 \n5 SR       white           0.733                 0.458 \n6 SR       &lt;NA&gt;            1.04                  0.717",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "href": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "ril_data |&gt; filter(location == \"GC\" | location == \"SR\"): To only retain samples from GC or (noted by |) SR. Recall that == asks the logical question, “Does the location equal SR?” So combined, the code reads “Retain only samples with location equal to SR or location equal to GC.”\n\n\n\nril_data |&gt; filter(location != \"US\" & location != \"LB\"): To remove samples from US and (noted by &) LB. Recall that != asks the logical question, “Does the location not equal US?” Combined the code reads “Retain only samples with location not equal to US and with location not equal to LB.”\n\n\n\nWarning! Remove one thing can change another:\nThink hard about removing things (e.g. missing data), and if you decide to remove things, consider where in the pipeline you are doing so. Removing one thing can change another. For example, compare:\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.705\n\n\nand\n\nril_data |&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\nThese answers differ because when we removed plants with no petal color information we also removed their pollinator visitation values.",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html",
    "href": "book_sections/data_in_R/data_in_R_summary.html",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions, R packages introduced, and Additional resources.\nA beautiful Clarkia xantiana flower.\nKeeping data in the tidy format—where each column represents a variable and each row represents an observation—allows you to fully leverage the powerful tools of the tidyverse. In the tidyverse, data are stored in tibbles, a modern update to data frames that enhances readability and maintains consistent data types. The dplyr package offers a suite of intuitive functions for transforming and analyzing data. For example, mutate() lets you create or modify variables, while summarize() computes summary statistics. When paired with group_by(), you can easily generate summaries across groups. Other essential functions include select() for choosing columns, filter() for subsetting rows, rename(), and arrange() for ordering data. Together—and especially when used with the pipe operator (group_by(...) |&gt; summarize(...))—these tools enable clear, reproducible workflows. In the next chapter, you’ll see how tidy data also powers beautiful and flexible plots using ggplot2.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "href": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions! Use the R environment below to work without changing tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) The code above returns the error: \"Error: could not find function \"summarise\"\". How can you solve this?\n\n Change “summarise” to “summarize” load the dplyr library\n\n\nQ2) Revisit the pollinator visitation dataset we explored. Which location has a greater anther stigma distance (asd_mm)? They are the sameSMGCS6\n\nQ3) Consider the table below. The data are tidynot tidy\n\n\n\n\n\nlocation\nGC\nGC\nGC\nGC\nGC\nGC\n\n\nril\nA1\nA100\nA102\nA104\nA106\nA107\n\n\nmean_visits\n0.0000\n0.1875\n0.2500\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere the data are transposed, so the data are not tidy. Remember in tidy data each variable is a column, not a row. This is particularly hard for R because there are numerous types of data in a column.\n\n.\n\nQ4 Consider the table below. The data are tidynot tidy \n\n\n\n\n\nlocation-ril\nmean_visits\n\n\n\n\nGC-A1\n0.0000\n\n\nGC-A100\n0.1875\n\n\nGC-A102\n0.2500\n\n\nGC-A104\n0.0000\n\n\nGC-A106\n0.0000\n\n\nGC-A107\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere location and ril are combined in a single column, so the data are not tidy. Remember in tidy data each variable is its own column. It would be hard to get e.g. means for RILs of locations in this format.\n\n\nQ5 Consider the table below. The data are tidynot tidy\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.0000\n0.6667\n\n\nA100\n0.1875\n0.5833\n\n\nA102\n0.2500\n0.6667\n\n\nA104\n0.0000\n1.7500\n\n\nA106\n0.0000\n0.5000\n\n\nA107\n0.0000\n1.5000\n\n\n\n\n\n\n\nClick here for explanation\n\nThis is known as “wide format” and is not tidy. Here the variable, location, is used as a column heading. This can be a fine way to present data to people, but it’s not how we are analyzing data.\n\n\nQ6 You should always make sure data are tidy when (pick best answer)\n\n collecting data presenting data analyzing data with dplyr all of the above\n\n\nQ7 What is wrong with the code below (pick the most egregious issue).\n\n I overwrote iris and lost the raw data I did not show the output I used summarise() rather than summarize() I did not tell R to remove missing data when calculating the mean.\n\n\niris &lt;- iris |&gt; \n  summarise(mean_sepal_length =  mean(Sepal.Length))\n\n\nQ8 After running the code below, how many rows and columns will the output tibble have? NOTE The original data has 593 rows, 7 columns and 186 unique RILs* 1 row, 1 column1 row, 2 columns2 rows, 2 columns186 rows, 2 columns186 rows, 7 columns593 rows, 2 columns593 rows, 7 columns\n\nril_data   |&gt;\n    group_by(ril) |&gt;\n    summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n\n\n\n\nGlossary of Terms\n\n\nTidy Data A structured format where:\n\nEach row represents an observation.\n\nEach column represents a variable.\n\nEach cell contains a single measurement.\n\nTibbles: A modern form of a data frame in R with:\n\nCleaner printing (only first 10 rows, fits columns to screen).\n\nExplicit display of data types (e.g., , ).\n\nStrict subsetting (prevents automatic type conversion).\n\nCharacter data is not automatically converted to factors.\n\nPiping (|&gt;) functions: A way to chain operations together, making code more readable and modular.\n\nGrouping in Data Analysis: Grouped operations allow calculations within subsets of data (e.g., mean visits per location).\nMissing Data (NA): R uses NA to represent missing values. Operations with NA return NA unless handled explicitly (e.g., na.rm = TRUE to ignore missing values, use = \"pairwise.complete.obs\", etc).\n\nWarnings: Indicate a possible issue but allow code to run (e.g., NAs introduced by coercion).\n\nErrors: Stop execution completely when something is invalid.\n\n\n\n\n\nKey R functions\n\n\nread_csv() (readr): Reads a CSV file into R as a tibble, automatically guessing column types.\nselect() (dplyr): Selects specific columns from a dataset.\nmutate() (dplyr): Creates or modifies columns in a dataset.\ncase_when() (dplyr): Replaces values conditionally within a column.\nas.numeric(): Converts a vector to a numeric data type.\nsummarize() (dplyr): Computes summary statistics on a dataset (e.g., mean, sum).\nmean(): Computes the mean (average) of a numeric vector.\n\nArgument: na.rm = TRUE: An argument used in functions like mean() and sd() to remove missing values (NA) before computation.\n\npull() (dplyr): Extracts a single column from a tibble as a vector.\ngroup_by() (dplyr): Groups data by one or more variables for summary operations.\n|&gt; (Base R Pipe Operator): Passes the result of one function into another, making code more readable.\n\n\n\n\n\nR Packages Introduced\n\n\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nSelecting columns.\n\nAdd a new column (or modify an existing one).\n\nSummarize data.\nSummarize data by group.\n\nOther web resources:\n\nChapter 10: Tidy data from R for data science (Grolemund & Wickham (2018)).\n\nAnimated dplyr functions from R or the rest of us.\n\nVideos:\n\nBasic Data Manipulation (From Stat454).\nCalculations on tibble (From Stat454).\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html",
    "href": "book_sections/intro_to_ggplot.html",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Always visualize your data.\nAnscombe’s quartet famously displays four data sets with identical summary statistics but very different interpretations (Figure 1, Table 1). The key lesson from Anscombe’s quartet is that statistical summaries can miss the story in our data, and therefore must be accompanied by clear visualization of patterns in the data.\ndataset\nsd.x\nsd.y\ncor\nmean.x\nmean.y\n\n\n\n\n1\n3.32\n2.03\n0.82\n9\n7.5\n\n\n2\n3.32\n2.03\n0.82\n9\n7.5\n\n\n3\n3.32\n2.03\n0.82\n9\n7.5\n\n\n4\n3.32\n2.03\n0.82\n9\n7.5\n\n\n\n\n\n\nTable 1: Summary statistics for Anscombe’s quartet.\nFigure 1: Four datasets with identical summary statistics but distinct visual patterns. Each panel represents a dataset with nearly identical means (shown in red), variances, correlations, and regression lines (shown in blue), yet their scatterplots reveal strikingly different structures.\nTherefore before diving into formal analysis, we always generate exploratory plots to uncover key patterns, detect data quality issues, and reveal the underlying structure of the data. One quick exploratory plot is rarely enough though, as you get to know and model your data, you will develop additional visualizations to dig deeper into the story.\nUltimately, after a combination of exploratory plots, summary statistics, and statistical modelling has helped you understand the data, you will generate well-crafted explanatory plots to communicate your insight elegantly to your audience. We will focus on the process of making explanatory plots in a later chapter.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "href": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Detect data quality issues early It is common to split up data collect to a team and maybe someone enters data in centimeters, and someone else in inches (etc). Or maybe for a data point or two a decimal point was lost, a value is way bigger than it should be. Everytime you collect and enter data you should make some plots to help identify any such data issues.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "href": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "title": "3. Introduction to ggplot",
    "section": "Focus on biological questions.",
    "text": "Focus on biological questions.\nBefore starting your R coding, consider the plots you want to make and why. It is all too easy to get stuck in the R vortex—making plots simply because they’re easy to create, visually appealing, fun, or challenging, and before you know it you’ve wasted an hour doing something that did not move your analysis or understanding forward. So before make a (set of) plot(s) always consider\n\nThe thing you want to know, and how it relates to the motivating biology. This includes\n\nIdentifying explanatory and response variables, and\nDistinguishing between key explanatory variables from covariates that you may not care about but need to include.\n\n\nThat the visualization of data reflects your biological motivation.This is particularly tricky for categorical variables which R loves to put in alphabetical order but may likely have a reasonable order to highlight patterns or biology.\n\n\nFor the Clarkia RIL datasets:.\nWe primarily want to know which (if any) phenotypes influence the amount of pollinator visitation and hybrid seed formation.\n\nOur response variables are hybrid seed production and pollinator visitation (and perhaps we would like to know the extent to which pollinator visitation predicts the proportion of hybrid seed).\n\nOur explanatory variables that we care a lot about include: petal area, petal color, herkogamy, protandry etc… We also want to account for differences in the location of the experiment, even though this is not motivating our study.\n\nWe also may want to evaluate the extent to which correlation between traits were broken up as we made the RILs. If trait correlations persists in the RILs it means that we cannot fully dissect the contribution of each trait to the outcome we care about, and that the genetic and/or physiological linkage between traits may have prevented evolution from acting independently on these traits.\n\nIn this case there is not a natural order to our categorical variables, so we do not need to think too hard about that.\n\n\n\n\n\n\n\nVideo\n\n\nFigure 2: Tweet from Shasta E. Webb (@webbshasta) about how she makes a plot. “My approach to figure-making in #ggplot ALWAYS begins with sketching out what I want the final product to look like. It feels a bit analog but helps me determine which #geom or #theme I need, what arrangement will look best, & what illustrations/images will spice it up. #rstats”\n\n\n\n\nLet’s sketch some potential plots to address these questions.\nI recommend sketching out what you want your plot to look like and what alternative results would look like (Figure 2). This helps to ensure that you are making the plot you want, not the one R gives you. In this case, some potentially important questions are:\n\nWhat is the distribution of the number of pollinator visits?\nDo we see different visitation by location?\n\nAre pink flowers more likely to be visited than white flowers?\n\nHow does the number of visits change with petal area, and does this depend on petal color?\n\nDoes pollinator visitation predict hybridization rate?\n\nSee Figure 3 for examples of how these may look.\n\n\n\n\n\n\n\n\nFigure 3: Brainstorming potential figures.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "href": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "title": "3. Introduction to ggplot",
    "section": "The idea of ggplot",
    "text": "The idea of ggplot\n\n\n\n\nAs described in the video above ggplot is based on the grammar of graphics, a framework for constructing plots by mapping data to visual aesthetics.. A major idea here is that plots are made up of data that we map onto aesthetic attributes, and that we build up plots layer by layer.\nLet’s unpack this sentence, because there’s a lot there. Say we wanted to make a very simple plot e.g. observations for categorical data, or a simple histogram for a single continuous variable. Here we are mapping this variable onto a single aesthetic attribute – the x-axis.\n\nWe are using the ggplot2 package to make plots.\nIf you do not have ggplot2 installed, type:\n\ninstall.packages(\"ggplot2\") # Do this the 1st time!\n\nIf you have ggplot2 installed or you just installed it, every time you start a new R session you still need to enter\n\nlibrary(ggplot2)\n\n\n\n\nShow code to load and format the data set, so we are where we left off.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)|&gt;\n  dplyr::select(location,  ril, prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm)|&gt;\n  dplyr::mutate(visited = mean_visits &gt; 0)",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#making-ggplots",
    "href": "book_sections/intro_to_ggplot.html#making-ggplots",
    "title": "3. Introduction to ggplot",
    "section": "Making ggplots:",
    "text": "Making ggplots:\nThe following sections show how to make plots that:\n\nVisualize the distribution of a single continuous variable.\n\nSaving a ggplot.\n\nCompare a continuous variable for different categorical variables.\n\nVisualize associations between two categorical variables.\n\nVisualize associations between two continuous variables.\n\nVisualize multiple explanatory variables.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\nFinally, for the true aficionados, I introduce ggplotly as a great way to get to know your data.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/one_continuous.html",
    "href": "book_sections/intro_to_ggplot/one_continuous.html",
    "title": "• 3. A continuous variable",
    "section": "",
    "text": "Motivating scenario: You want to see the distribution of a single continuous variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nFamiliarize yourself with making plots using the ggplot2 package.\nUse geom_histogram() to make a histogram.\n\nSpecify the width (with binwidth) or number (with bins) of bins in a histogram.\n\n\nUse geom_density() to make a density plot.\nChange the outline color (with color) and the fill color (with fill).\n\n\n\n\nVisualizing Distributions\nLet’s first consider the distribution of a single continuous variable—pollinator visitation. There are some natural questions we would want answered early on in our analysis:\n\nAre most flowers visited frequently, or do visits tend to be rare?\n\nIs the distribution symmetric, or is it skewed, with many flowers receiving few or no visits and a small number receiving many?\n\nAs we will see throughout the term, understanding the shape of our data helps guide our analysis. Let’s start with two common visualizations for distributions:\n\n\n\n\n\n\n\n\n\n\nA histogram bins the x-axis variable (in this case, visitation) into intervals and shows the number of observations in each bin on the y-axis. This allows us to see how frequently different levels of visitation occur.\n\nA density plot fits a smooth function to the histogram, providing a continuous representation of the distribution. This smoothing can sometimes make patterns easier (or harder) to see. Later, we’ll see that density plots can also help compare distributions.\n\nMaking histograms & density plots: A Step-by-Step Guide. To create these visualizations in ggplot2, we need to (minimally) specify three key layers:\n\nThe data layer: This is the dataset we’re plotting—in this case, ril_data. We pass this as an argument inside the ggplot() function, e.g., ggplot(data = ril_data).\n\nThe aesthetic mapping: This tells R how to map each variable onto the plot. In a histogram, we map the variable whose distribution we want to visualize (in this case, mean_visits) onto the x-axis. We define this inside the aes() argument within ggplot(), e.g., ggplot(data = ril_data, aes(x = mean_visits)).\n\nThe geometric object (geom) that displays the data: In this case, we use geom_histogram() for a histogram or geom_density() for a density plot. These are added to the plot using a +, as shown in the examples below.\n\n\nSet upAdding a geomElaborationsDensity plot\n\n\n\nInside the ggplot() function, we tell R that we are working with the ril_data, and we map mean_visits onto x inside aes(). The result is a boring canvas, but it is the canvas we build on.\n\nggplot(data = ril_data, aes(x = mean_visits))\n\n\n\n\n\n\n\n\n\n\n\n\nBut now we want to show the data. To do so we need to add our geom. In this case we show the data as a histogram.\nThe resulting plot shows that most plants get zero visits, but some get up to five visits.\n\nggplot(ril_data,aes(x = mean_visits))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\nTo take a bit more control of my histogram, I like to\n\nSpecify the number or width of bins (with the binwidth and bins arguments, respectively).\n\nAdd white lines between bins to make them easier to see (with color = \"white\").\n\nSpruce up the bins by specifying the color that fills them (with e.g. fill = \"pink\").\n\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can easily make a density plot if you prefer!\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_density(color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating questions, we see that most plants receive no visits, but this distribution is skewed with some plants getting lots of visits.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. A continuous variable"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "href": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "title": "• 3. Saving a ggplot",
    "section": "",
    "text": "Motivating scenario: You have made a plot and want to save it!\nLearning goals: By the end of this sub-chapter you should be able to\n\nSave a ggplot using either a screen grab, RStudio’s point-and-click options, or the ggsave() function.\n\nKnow the costs and benefits of each approach and when to use which.\n\n\n\n\nSaving Your ggplot.\nYou probably want to save your plot once you’ve made one you like. There are several ways to do this, each with its own pros and cons.\n\n\n\n\n\n\n\n\n\nThe quickest approach is to simply take a screenshot – I do this quite often, and it is great because the plot you save looks exactly like the one on your screen. However, these plots are not vectorized and can lose quality in other ways, so I usually use these as a quick first pass for exploratory data analysis, rather than a refined solution.\nThe next fastest way is to use RStudio’s built-in tools – simply click on the plot in the Plots panel, then use the Export button to copy or save the image. This allows you to choose a file format (like PNG or PDF), set the dimensions, and adjust the resolution. This method is convenient and good for quick outputs, but it’s manual, which means it doesn’t lend itself to reproducible workflows — if you make changes and regenerate your plot later, you’ll have to go through the same export process again.\nFor more control and reproducibility, I suggest using the ggsave() function. This function saves the most recently displayed plot by default, or you can specify a plot object manually. You can choose the file type simply by specifying the file extension (e.g., .png, .pdf, .svg) and control the size and resolution of the output. For example, the code below will save a high-resolution pdf file, called ril_visit_hist.pdf:\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\nggsave(\"ril_visit_hist.pdf\", width = 6, height = 4)\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using either ggsave() or RStudio’s built-in tools always check that your saved plot looks as expected, as it is often slightly different than what you saw in your R session.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Saving a ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/cont_cat.html",
    "href": "book_sections/intro_to_ggplot/cont_cat.html",
    "title": "• 3. Continuous y/categorical x",
    "section": "",
    "text": "Motivating scenario: You want to compare a continuous variable across different levels of a categorical explanatory variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUnderstand the challenge of overplotting.\n\nUse geom_point, geom_jitter() and/or geom_boxplot() to visualize the distribution.\n\nUse the size and alpha arguments to adjust the size and transparency of points.\n\n\nCombine geom_jitter() and/or geom_boxplot(), noting that\n\nOrder matters - always plot jittered points on top of (i.e. after) boxplots.\n\nWhen showing boxplots and jittered points, make sure the boxplot does not show outliers, otherwise those points will be shown twice.\n\n\n\n\n\nVisualizing associations between a continuous response and a categorical explanatory variable.\nWe often want to know more than just the distribution of variables, we want to know which, if any, explanatory variables are associated with this variation. So, for example, we may want to know if pollinator visitation differs by location. In this case we map the categorical variable (location) onto the x-axis and the continuous response (visitation) onto the y-axis.\n\n\n\n\n\n\n\n\n\nTo compare visitation by site, we start with this setup:\n\nFilter our data for samples from \"GC\" and \"SR\" (because we did not conduct pollinator observations at the other sites).\n\nPipe this directly into our ggplot() function.\n\nNote: We do not need to specify the data argument when piping data. That is because ggplot inherits data from the pipe.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))\n\nContinuous response to a categorical explanatory variable: A Step-by-Step Guide: In the tabs below I show some options for our geom.\n\npoint()jitter()boxplot() + jitter()\n\n\n\nThe Simplest Plot\n\nUses the geom_point() function to display the data.\n\nWe can add a mean using stat_summary().\n\nNote: I plot the mean in red to make it stand out.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt;  # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits)) +\n  geom_point(size =2, alpha = 0.5) +\n  stat_summary(size = 1.2, color = \"red\")\n\n\n\n\n\n\n\n\nHowever, these figures can be difficult to interpret when many points overlap, making it hard to distinguish individual data points—this issue is known as over-plotting.\nOne way to address over-plotting is to adjust the transparency of points using the alpha argument. There are several other techniques to handle this, which we’ll explore in the next section.\n\n\n\n\nJitter plots spread out data along the x-axis. This works well with categorical predictors but can be misleading when used with continuous predictors. To improve clarity, I make a few adjustments in the geom_jitter() function:\n\nI set height = 0 to keep the y-values unchanged. I always do this.\n\nI set width = 0.3 to prevent points from overlapping too much between categories. You may experiment with this value to find the best fit for your plot.\n\nI make the points larger (size = 3) and partially transparent (alpha = 0.5) to help visualize overlap more effectively.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)+\n  stat_summary(size = 1.2,color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add multiple geom layers to a plot. For example, we can combine a boxplot (geom_boxplot()) with a jitter plot. However, there are a few important things to keep in mind:\n\nOrder matters. geom_boxplot() + geom_jitter() places points over the boxplot, making the data visible, while geom_jitter() + geom_boxplot() places the boxplot on top, potentially obscuring the points.\n\nHandling outliers. geom_boxplot() automatically displays outliers as individual points, which is useful when we’re not showing the raw data. However, if we add jittered points with geom_jitter(), these outliers will appear twice, potentially misleading us. To avoid this, I set outlier.shape = NA in the boxplot.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_boxplot(outlier.shape = NA) + \n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that plants at the SR population receive way more visits by pollinators than do plants at GC.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Continuous y/categorical x"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_categorical.html",
    "href": "book_sections/intro_to_ggplot/two_categorical.html",
    "title": "• 3. Two categorical variables",
    "section": "",
    "text": "Motivating scenario: You want to explore how two categorical variables are associated.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake barplots with geom_bar() and geom_col().\n\nMake stacked and grouped barplots.\n\nKnow when to use geom_bar() and when to use geom_col().\n\n\n\n\nCategorical explanatory and response variables\nAbove, we saw that most plants received no visits, so we might prefer to compare the proportion of plants that did and did not receive a visit from a pollinator by some explanatory variable (e.g. petal color or location). Recall that we have added the logical variable, visited, by typing mutate(visited = mean_visits &gt; 0).\n\n\n\n\n\n\n\n\n\nMaking bar plots: A Step-by-Step guide. There are two main geoms for making bar plots, depending on the structure of our data:\n\nIf we have raw data (i.e. a huge dataset with values for each observation) use geom_bar().\n\nIf we have aggregated data (i.e. a summary of a huge dataset with counts for each combination of variables) use geom_col()\n\nNote: Here we map petal color onto the x-axis, and visited (TRUE / FALSE) onto the fill aesthetic.\n\nStacked barplotGrouped barplotAggregated data\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\nIf you had aggregated data, like that below. We need to plot these data somewhat differently. There are two key differences:\n\nWe map our count (in this case n) onto the y aesthetic.\n\nWe use geom_col() instead of geom_bar().\n\n\n\n\n\n\nlocation\npetal_color\nvisited\nn\n\n\n\n\nGC\npink\nFALSE\n32\n\n\nGC\npink\nTRUE\n23\n\n\nGC\nwhite\nFALSE\n46\n\n\nGC\nwhite\nTRUE\n2\n\n\nSR\npink\nFALSE\n1\n\n\nSR\npink\nTRUE\n56\n\n\nSR\nwhite\nFALSE\n11\n\n\nSR\nwhite\nTRUE\n39\n\n\n\n\n\n\nggplot(data = aggregated_pollinator_obs, \n       aes(x = petal_color, y = n, fill = visited))+\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: We see that a greater proportion of pink-flowered plants receive visits compared to white-flowered plants.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two categorical variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_continuous.html",
    "href": "book_sections/intro_to_ggplot/two_continuous.html",
    "title": "• 3. Two continuous variables",
    "section": "",
    "text": "Motivating scenario: You want to visualize the relationship between two continuous variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake a scatterplot with geom_point().\n\nShow trends with geom_smooth().\n\nShow a linear regression with method = \"lm\"\n\nShow a moving average with method = \"loess\"\nOptionally suppress the grey ribbon by setting se = FALSE.\n\nTransform the scale of x or y axis with e.g. scale_x_continuous(trans = \"log1p\").\n\n\n\n\nVisualizing associations between continuous variables.\nWhen Brooke watched pollinators visit parviflora recombinant inbred lines (RILs), she was hoping that these observations also informed the probability of hybrid seed set. The first step in evaluating this hypothesis is to generate a scatterplot – a visualization that shows the association between continuous variables.\n\n\n\n\n\n\n\n\n\nWe use geom_point() to make such a plot and add trends with geom_smooth(). There are numerous types of trendlines we could add with the method argument:\n\nTo add the best fit linear model type method = lm.\n\nTo add a smoothed moving average type method = loess.\n\nBy default ggplot adds uncertainty about its guess of the line with a grey background. This is sometimes helpful but can get in the way. To remove it type se = FALSE.\nAt times transforming the data makes patterns easier to see. We can transform our presentation of the data with the trans argument in the scale_x_continuous() (or scale_y_continuous()) functions.\n\npoint()transformlinear trendsmoothed average\n\n\n\nWe present the data with geom_point(). Because some data points overlapped\n\nI increased the point size (size = 3) and\n\nI made the points partially transparent (alpha = 0.4).\n\nA small jitter could have been ok, but jittering continuous values gives me the ick, because I want my data presented faithfully.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes nonlinear scales better reveal trends. Transforming the scale on which the data are presented (rather than transforming the data itself) is nice because we retain original values.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\nTake care not to lose data when transforming: The log of any number less than or equal to zero is undefined. To avoid losing these data points, I transformed the data as log(x+1) with the \"log1p\" transformation. If the data were all greater than one, I could have used the log log10 or sqrt transform.\n\n\n\n\n\nThe geom_smooth() allows us to highlight patterns in our data. There are lots of ways to draw trends through data, and we can specify how we want to do so with the method argument.\nHere I present the standard “best-fit” line with method = \"lm\". The grey area around that line represents plausible lines that would also have been statistically acceptable (more on that later).\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes a simple line does not do a great job of highlighting patterns. Specifying method = loess presents a smoothed moving average.\nIn addition to showing that moving average, I show you how to suppress that grey area with se = FALSE. I tend to like to include the uncertainty in our estimated trend so I usually don’t do this, but sometimes showing the uncertainty hides other features of the data, so I wanted to empower you.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"loess\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that the proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two continuous variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/many_explanatory.html",
    "href": "book_sections/intro_to_ggplot/many_explanatory.html",
    "title": "• 3. Many explanatory vars",
    "section": "",
    "text": "Motivating scenario: You want to visualize associations between a response variable and numerous explanatory variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUse the color or fill aesthetics to map categorical variables.\n\nUse “small multiples” with facet_wrap() and facet_grid()\n\n\n\n\nThe challenge of many explanatory variables\nThere is usually more than one thing going on in a scientific study, and we may want to see how different combinations of explanatory variables are associated with a response. This can be a challenge over the term, you will encounter many tips and tricks to help in this task. For now we will look at two useful approaches:\n\nMapping different explanatory variables to different aesthetics.\n\nThe use of “small multiples”.\n\n\n\nMultiple aesthetic mappings\nWe saw that the number of pollinators increased with petal size and was greater for pink than white flowers. However, visualizing a response variable as a function of its multiple explanatory variables together can help us home in on the patterns.\n\nTo do so we can make a scatterplot and map petal color onto the color aesthetic.\n\nWe can even use size and color as extra aesthetics to map onto. I show you how to do this, but use it sparingly, because too many extra aesthetics may provide more distraction than insight.\n\n\n\n\n\n\n\n\n\n\n\nTwo explanatory variablesThree explanatory variables\n\n\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nHere I added anther stigma distance as another explanatory variable by mapping it onto size. Note that:\n\nI removed the size = 3 argument from geom_point() otherwise it would not map asd onto size.\n\nIn this case, it did not reveal much and was probably not worth doing.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(aes(size = asd_mm), alpha = .4)+\n  geom_smooth(method = \"lm\",show.legend = FALSE, se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to the motivating question, we see that visitation increases with both petal area (positive slope) and pink petals. The proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.\n\n\nSmall multiples\n\n\n\n\n\n\n\n\nFigure 1: Using small multiples to show the lunar phase moon over a month. From this link\n\n\n\n\n\n\n\nAt the heart of quantitative reasoning is a single question: Compared to what? Small multiple designs, multivariate and data bountiful, answer directly by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives. For a wide range of problems in data presentation, small multiples are the best design solution. \n\nEdward Tufte (Tufte, 1990)\n\nEdward Tufte, a major figure in the field of data visualization - popularized the concept of “small multiples” – showing data with the same structure across various comparisons. He argued that such visualizations can help our eyes make powerful comparisons (See quote above). For example, the lunar phase can be well visualized by using small multiples (Figure 1).\nFor our analyses, we may find that adding location as a variable helps us better see and understand patterns. Additionally, small multiples are sometimes preferable to mapping different categorical variables onto different aesthetics. I show these examples below:\n\nOne explanatory “facet”Two explanatory “facets”\n\n\n\nWe can add a small multiple with a “facet”. For a single categorical variable, use the facet_wrap() function. Here are some notes and options:\n\nThe first argument is ~ &lt;Thing to facet by&gt;, e.g. ~ location.\n\nYou can specify the number of rows or columns with nrow or ncol arguments.\n\nThe labeller = \"label_both\" shows the variable name in addition to its value.\n\nOccasionally, you may want different facets on different scales. You can use the scales argument with options, free_x, free_y, and free.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  facet_wrap(~ location, nrow = 1, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nFor two categorical variables, use the facet_grid() function. Here are some additional notes and options:\n\nThe first argument is &lt;Row to facet by&gt; ~ &lt;Column to facet by&gt;, e.g. petal_color ~ location.\n\nYou can no longer specify the number of rows or columns.\n\nI have shown the same data as the previous plot in a different way. But you can see that this adds room for mapping another variable.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Many explanatory vars"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "title": "• 3. ggplot summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources + BONUS CONTENT: ggplotly.\nEffective data visualization begins with curiosity and clear biological questions. Before writing a single line of code, it’s worth thinking about what you want to learn from your data and how best to visualize that information. Once we have developed some starting ideas and sketches, we are ready to use ggplot2’s flexible framework to bring these ideas to life. Data visualization with ggplot2 is built around the idea that plots are constructed by layering components: you begin by mapping variables to aesthetic properties like position, color, or size, and then choose how to display those mapped variables using geometric elements like histograms, barplots, or points. With this approach you can iteratively build visualizations that reflect your questions and highlight meaningful patterns.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "title": "• 3. ggplot summary",
    "section": "Bonus content: Interactive plots with ggplotly",
    "text": "Bonus content: Interactive plots with ggplotly\nOften data have strange outliers, or patterns you think you see but aren’t sure about, or simply interesting data points. When I run into these issues during exploratory data analysis I often want to know more about individual data points. To do so, I make interactive graphs with the ggplotly() function in the plotly package.\nThe example below shows how to do this. Note that you can make up random aesthetics that you never use and they show up when you hover over points – this helps with understanding outliers. You can also zoom in!\n\nlibrary(plotly)\nbig_plot &lt;- ril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid,\n             ril = ril,\n             mean_visits =  mean_visits))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\nggplotly(big_plot)\n\n\n\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html",
    "href": "book_sections/reproducible_science.html",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Making science reproducible\nThis section includes background on:\nIn my roles as a biostatistics professor and Data Editor at The American Naturalist, I have found that the greatest beneficiary of reproducible research is often the lead author themselves. In this chapter, we will work through the process of creating reproducible research—from collecting data in the field to writing and sharing R scripts that document your analyses.\nThis chapter walks you through the key steps for making your science reproducible—from field notes to final scripts. You will learn how to:\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html#making-science-reproducible",
    "href": "book_sections/reproducible_science.html#making-science-reproducible",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Appropriately collect and store data, including Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\n\nDevelop reproducible computational strategies, including: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html",
    "href": "book_sections/reproducible_science/collecting_data.html",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Making rules for data collection\nThis section includes background on: Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\nSometimes, data come from running a sample through a machine, resulting in a computer readout of our data (e.g., genotypes, gene expression, mass spectrometer, spectrophotometer, etc.). Other times, we collect data through counting, observing, and similar methods, and we enter our data by hand. Either way, there is likely some key data or metadata for which we are responsible (e.g., information about our sample). Below,we focus on how we collect and store our data, assuming that our study and sampling scheme have already been designed. Issues in study design and sampling will be discussed in later chapters.\nBefore we collect data, we need to decide what data to collect. Even in a well-designed study where we want to observe flower color at a field site, we must consider, “What are my options?”, for example:\nSome of these questions have answers that are more correct, while for others, it’s crucial to agree on a consistent approach and ensure each team member uses the same method.\nSimilarly, it’s important to have consistent and unique names – for example, my collaborators and I study Clarkia plants at numerous sites – include Squirrel Mountain and SawMill – which one should we call SM (there is a similar issue in state abbreviation – we live in MiNnesota (MN), not MInnesota, to differentiate our state from MIchigan (MI)). These questions highlight the need for thoughtful data planning, ensuring that every variable is measured consistently.\nOnce you have figured out what you want to measure, how you will measure and report it, and other useful info (e.g., date, time, collector), you are ready to make a data sheet for field collection, a database for analysis, and a README or data dictionary.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "href": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Should the options be “white” and “pink,” or should we distinguish between darker and lighter shades?\n\nIf we’re measuring tree diameter, at what height on the tree should we measure the diameter?\n\nWhen measuring something quantitative, what units are we reporting?",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "href": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "title": "• 4. Collecting data",
    "section": "Making a spreadsheet for data entry",
    "text": "Making a spreadsheet for data entry\nAfter deciding on consistent rules for data collection, we need a standardized format to enter the data. This is especially important when numerous people and/or teams are collecting data at different sites or years. Spreadsheets for data entry should be structured similarly to field sheets (described below) so that it is easy to enter data without needing too much thought.\nSome information (e.g., year, person, field site) might be the same for an entire field sheet, so it can either be written once at the top or explicitly added to each observation in the spreadsheet (copy-pasting can help here).",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "href": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "title": "• 4. Collecting data",
    "section": "Making Data (field) sheets",
    "text": "Making Data (field) sheets\nWhen collecting data or samples, you need a well-designed data (field) sheet. The field sheet should closely resemble your spreadsheet but with a few extra considerations for readability and usability. Ask yourself: How easy is this to read? How easy is it to fill out? Each column and row should be clearly defined so it’s obvious what goes where. Consider the physical layout too—does the sheet fit on one piece of paper? Should it be printed in landscape or portrait orientation? Print a draft to see if there’s enough space for writing. A “notes” column can be useful but should remain empty for most entries, used only for unusual or exceptional cases that might need attention during analysis.\nIt’s also smart to include metadata at the top—things like the date, location, and who collected the data. Whether this metadata gets its own column or is written at the top depends on practical needs—if one person is responsible for an entire sheet, maybe it belongs at the top, not repeated for every sample (contrary to the example in Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: Example field data sheet designed for recording ecological and biological observations. The form includes sections for site information, environmental conditions, personnel details, and a structured table for recording species, individual IDs, sex, location descriptions, time, GPS coordinates, photos, collector name, and additional notes.\n\n\n\n\n\n\nData collection and data entry checklist:\n\nBe consistent and deliberate: You should refer to a thing in the same thoughtful way throughout a column. Take, for example, gender as a nominal variable. Data validation approaches, above, can help.\n\n\n\nA bad organization would be: male, female, Male, M, Non-binary, Female.\nA good organization would be: male, female, male, male, non-binary, female.\nA just as good organization would be: Male, Female, Male, Male, Non-binary, Female.\n\nUse good names for things: Names should be concise and descriptive. They need not tell the entire story. For example, units are better kept in a data dictionary (Figure 4) than column name. This makes downstream analyses easier. Avoiding spaces and special characters makes column names easier to work with in R.\n\n\n\n\n\n\n\n\n\n\nFigure 3: A comparison of bad and good variable naming conventions in a dataset. The top section (labeled “Bad” in red) displays poor naming choices, including spaces, special characters, unclear abbreviations, and inconsistent formatting. The bottom section (labeled “Good” in blue) demonstrates improved naming conventions with consistent formatting, clear descriptions, and no special characters.\n\n\n\n\n\nSave in a good place. Make a folder for your project. Keep your data sheet (and data dictionary) there. Try to keep all that you need for the project in this folder, but try not to let it get too complex. Some people like to add more order with additional subfolders for larger projects.  \nSave early and often: You are welcome.  \nBackup your data, do not touch the original data file and do not perform calculations on it. In addition to scans of your data (if collected on paper) also save your data on both your computer and a locked, un-editable location on the cloud (e.g. google docs, dropbox, etc.). When you want to do something to your data do it in R, and keep the code that did it. This will ensure that you know every step of data manipulation, QC etc.  \nDo Not Use Font Color or Highlighting as Data. You may be tempted to encode information with bolded text, highlighting, or text color. Don’t do this! Be sure that all information is in a readable column. These extra markings will either be lost or add an extra degree of difficulty to your analysis. Reserve such markings for the presentation of data. \nNo values should be implied Never leave an entry blank as “shorthand for same as above”. Similarly, never denote the first replicate (or treatment, or whatever) by its order in a spreadsheet, but rather make this explicit with a value in a column. Data order could get jumbled and besides it would take quite a bit of effort to go from implied order to a statistical analysis. It is ok to skip this while taking data in the field, but make sure no values are implied when entering data into a computer. \nData should be tidy (aka rectangular): Ideally data should be entered as tidy (Each variable must have its own column. Each observation must have its own row. Each value must have its own cell). However, you must balance two practice considerations – “What is the best way to collect and enter data?” vs “What is the easiest way to analyze data?” So consider the future computational pain of tidying untidy data when ultimately deciding on the best way to format your spreadsheet (above).  \nUse data validation approaches: When making a spreadsheet, think about future-you (or your collaborator) who will analyze the data. Typos and inconsistencies in values (e.g., “Male,” “male,” and “M” for “male”) create unnecessary headaches. Accidentally inputting the wrong measure (e.g., putting height where weight should be, or reporting values in kg rather than lb) sucks.  Both Excel and Google Sheets (and likely most spreadsheet programs) have a simple solution: use the “Data validation” feature in the “Data” drop-down menu to limit the potential categorical values or set a range for continuous values that users can enter into your spreadsheet. This helps ensure the data are correct.\n\n\n\nMaking a README and/or “data dictionary”\nA data dictionary is a separate file (or sheet within a file, if you prefer) that explains all the variables in your dataset. It should include the exact variable names from the data file, a version of the variable name that you might use in visualizations, and a longer explanation of what each variable means. Additionally, it is important to list the measurement units and the expected minimum and maximum values for each variable, so anyone using the data knows what to expect.\nAlongside your data dictionary, you should also create a README file that provides an overview of the project and dataset, explaining the purpose of the study and how the data were collected. The README should include a description of each row and column in the dataset. While there may be some overlap with the data dictionary, this is fine. The data dictionary can serve as a quick reference for plotting and performing quality control, whereas the README provides a higher-level summary, designed for those who may not have been directly involved in the project but are seriously interested in the data.\n\n\n\n\n\n\n\n\nFigure 4: Example data dictionary from (Broman & Woo, 2018). This table provides structured metadata for a dataset, including variable names, descriptions, and group classifications. It ensures clarity in data documentation by defining each column’s purpose and expected values. See the Clarkia RIL data dictionary here for another example.\n\n\n\n\n\n\n\nLong-term public data storage\nA key to the scientific method is reproducibility. This is why scientific papers have a methods section. Nowadays - the internet allows for even more detailed sharing of methods. Additionally it is the expectation in most fields that data is made available after publication on repositories like data DRYAD or DRUM.\n\n\n\n\nBabbage, C. (1864). Passages from the life of a philosopher. Longman; Co.\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html",
    "href": "book_sections/reproducible_science/reproducible_analyses.html",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "R Projects, Storing, and Loading data\nThis section includes background on: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).\nIn addition to the high-minded values of transparency and sharing scientific progress, there is a pragmatic and selfish reason to make you work reproducible.\nThis humorous observation highlights a very real challenge. Research is complex and can stretch over months or even years. Well-documented data and code, allows you to retrace your steps, understand your past analyses, and explain your results. By keeping a good record of your workflows, you make your future self’s life much easier.\nFigure 1: A screenshot of the files pane of RStudio. This displayes the folder containing my RProject, my data, and data dictionary. If you do this, there is no need to use the setwd() function or to use a long, “absolute path” to the data.\nI have previously introduced an easy way to load data from the internet into R, but often data is on our computer, not the internet. Loading data from your computer to R can be a difficult challenge, but if you follow the guide below you will see that it can be very easy.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "Making an R project\nThe first step towards reproducible analyses is making an R project. I do this by clicking “File” &gt; “New Project” and I usually use an existing directory (Navigating to the folder with my data and data dictionary that we just made).\n\nNow every time you use R to work on these data, open R by double clicking on this project (or if R is already open, navigate to “Open Project”).\n\n\n\nLoading data\nNow if your project and data are in the same folder, loading the data is super easy. Again just used the read_csv() function in the readr package with an argument of the filename (in quotes) to read in the data!\n\n\nSome people like a different structure of files in their R project – they like the project in the main folder and then a bunch of sub-folders like data and scripts and figures. If you are using this organization, just add the subfolder in the path (e.g. read_csv(\"data/my_data.csv\") ).\n\nlibrary(readr)\nril_data &lt;- read_csv(\"clarkia_rils.csv\")\n\n\nRead excel files into R with the read_xlsx() from the readxl package). You can even specify the sheet as an argument!",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "R Scripts, Commenting Code, + Cleaning & Tidying data",
    "text": "R Scripts, Commenting Code, + Cleaning & Tidying data\nWe have just run a few lines of code – loading the readr library and assigning the data in clarkia_rils.csv to the variable, ril_data.\nWe will likely want to remember that we have done this, and build off it in the future. To do so let’s open up an R Script – by navigating to File, New File, R Script.\nA new file, likely called Untitled 1 will show up in the R Scripts pane of your R Studio session. After giving it a more descriptive name, be sure save all of the code you are using for these analyses in this file.\nHere I introduce the best practices for writing an R script (Part I). I revisit these in greater depth later in the chapter.\n\nFirst include your name, and the goal of the script as comments on the opening lines.\n\nNext load all packages you will use. Don’t worry, you can go back and add more later.\n\nDo not include install.packages() in your script: You only want to do this once, so it should not be part of a reproducible pipeline. People can see they need to install the package from you loading it.\n\n\nNext load all the data you will use.\n\nThen get started coding.\n\nBe sure to comment your code with the #.\n\n\nCleaning data\nWe previously said not to make column names too long, yet in our dataset, we have the variable petal_area_mm. We can use the rename() function in the dplyr package to give columns better names.\nI do this below in my complete and well-commented R script, which I conclude with a small plot. Notice that anyone could use this script and get exactly what I got.\n\n\n\n\n\n\n\n\n\n\n# Yaniv Brandvain. March 07 2025\n# Code to load RIL data, and make a histogram of petal area facetted by petal color\n\nlibrary(readr)                                      # Load the readr package to load our data\nlibrary(ggplot2)                                    # Load the ggplot2 package to make graphs\n\nril_data &lt;- read_csv(\"clarkia_rils.csv\") |&gt;         # load the RIL data\n    filter(!is.na(petal_color))          |&gt;         # remove missing petal color data\n  rename(petal_area = petal_area_mm)                # rename petal_area_mm as petal_area\n\nggplot(ril_data, aes(x = petal_area))+              # set up a plot with petal_area on x\n  geom_histogram(binwidth = 10, color = \"white\")+   # make it a histogram, with a bins of width 10 and separated by white lines \n  facet_wrap(~petal_color, ncol = 1)                # wrap by petal color, with one column\n\n\nIf your data set has many column names that are difficult to use in R, the clean_names() function in the janitor package fixes a bunch of them at once.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "Tidying data",
    "text": "Tidying data\nWe have previously seen that there are many ways for data to be untidy. One common untidy data format is the “wide format”, below. Here, the columns SR and GC describe the proportion of hybrid seed on each ril (row) observed in each of these two locations. You can use the pivot_longer() function in the tidyr package, to tidy such data:\n\n\nCode to create untidy_ril\nlibrary(tidyr)\nlibrary(dplyr)\nuntidy_ril &lt;- ril_data |&gt; \n  select(ril, location,prop_hybrid)|&gt;\n  filter(location %in% c(\"GC\",\"SR\"))|&gt; \n  pivot_wider(id_cols = \"ril\",\n              names_from = location,\n              values_from = prop_hybrid)\n\nuntidy_ril|&gt;head(n = 5)|&gt; kable()\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.000\n0.00\n\n\nA100\n0.125\n0.25\n\n\nA102\n0.250\n0.00\n\n\nA104\n0.000\n0.00\n\n\nA106\n0.000\n0.00\n\n\n\n\n\n\nlibrary(tidyr)\npivot_longer(untidy_ril, \n             cols = c(\"GC\",\"SR\"),\n             names_to = \"location\", \n             values_to = \"prop_hybrid\")|&gt;\n  head(n = 10)|&gt;\n  kable()\n\n\n\n\nril\nlocation\nprop_hybrid\n\n\n\n\nA1\nGC\n0.000\n\n\nA1\nSR\n0.000\n\n\nA100\nGC\n0.125\n\n\nA100\nSR\n0.250\n\n\nA102\nGC\n0.250\n\n\nA102\nSR\n0.000\n\n\nA104\nGC\n0.000\n\n\nA104\nSR\n0.000\n\n\nA106\nGC\n0.000\n\n\nA106\nSR\n0.000",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "title": "• 4. Reproducible analyses",
    "section": "Reproducible Code: A Checklist From AmNat",
    "text": "Reproducible Code: A Checklist From AmNat\nWhat you do to data and how you analyze it is as much a part of science as how you collect it. As such, it is essential to make sure your code:\n\nReliably works – even on other computers\n\nAnd can be understood.\n\nThe principles from The American Naturalist’s policy about this are pasted in the box below:\n\nREQUIRED:\n\nScripts should start by loading required packages, then importing raw data from files archived in your data repository.\nUse relative paths to files and folders (e.g. avoid setwd() with an absolute path in R), so other users can replicate your data input steps on their own computers.\nMake sure your code works. Shut down your R. (or type rm(list=ls()) into the console and run the code again. You should get the same results. If not, go back and fix your mistakes.\nAnnotate your code with comments indicating what the purpose of each set of commands is (i.e., “why?”). If the functioning of the code (i.e., “how”) is unclear, strongly consider re-writing it to be clearer/simpler. In-line comments can provide specific details about a particular command.\n\nNote that ChatGPT is very good at commenting your code.\n\nAnnotate code to indicate how commands correspond to figure numbers, table numbers, or subheadings of results within the manuscript.\nIf you are adapting other researcher’s published code for your own purposes, acknowledge and cite the sources you are using. Likewise, cite the authors of packages that you use in your published article.\n\nRECOMMENDED:\n\nTest code ideally on a pristine machine without any packages installed, but at least using a new session.\nUse informative names for input files, variables, and functions (and describe them in the README file).\nAny data manipulations (merging, sorting, transforming, filtering) should be done in your script, for fully transparent documentation of any changes to the data.\nOrganize your code by splitting it into logical sections, such as importing and cleaning data, transformations, analysis and graphics and tables. Sections can be separate script files run in order (as explained in your README) or blocks of code within one script that are separated by clear breaks (e.g., comment lines, #————–), or a series of function calls (which can facilitate reuse of code).\nLabel code sections with headers that match the figure number, table number, or text subheading of the paper.\nOmit extraneous code not used for generating the results of your publication, or place any such code in a Coda at the end of your script.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html",
    "href": "book_sections/reproducible_science/reproducible_summary.html",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources.\nReproducibility is a cornerstone of good science, ensuring that research is transparent, reliable, and easy to build upon. This chapter covered best practices for collecting, organizing, and analyzing data in a reproducible manner.\nBefore collecting data, establish clear rules for measurement, naming conventions, and data entry to maintain consistency. Field sheets should be well-structured, and tidy. Additionally, creating a data dictionary and README document ensures that variables and project details are well-documented. Finally, storing data and scripts in public repositories supports transparency and open science.\nIn analysis, using an R Project helps keep files organized, and loading data with relative paths avoids location issues. Writing well-structured R scripts with clear comments makes workflows understandable and repeatable. By prioritizing reproducibility you not only strengthen the integrity of your work, but also make future analyses smoother for yourself and others.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "href": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions!\n\nQ1) What is the biggest mistake in the table below?\n\n ID should be lower case Its perfect, change nothing the column name, weight is not sufficiently descriptive, it should include the units. date_colleted_empty_means_same_as_above is too wordy, replace with date Values for date_collected_empty_means_same_as_above are implied. date is in Year-Month-Day format, while Month-Day-Year format is preffered.\n\n\n\n\n\n\nID\nweight\ndate_collected_empty_means_same_as_above\n\n\n\n\n1-A1\n104\n2024-03-01\n\n\n1-1B\n210\n\n\n\n3-7\n150\n\n\n\n2-B\n176\n2024-03-15\n\n\n1-A5\n110\n\n\n\n\n\n\n\n\nClick here for explanation\n\nWhile some of these (like the long name for date) are clearly shortcomings, spreadsheets should never leave values implied.\n\n.\n\nQ2) What would you expect in a data dictionary accompanying the table above? (select all correct)\n\n The units for weight. A statement that date is in Year-Month-Day format A statement explaining that in the date colleted column, empty means same as above.\n\n\nQ3) How do you read data from a Excel sheet, called raw_data in an Excel filed named bird_data.xlsx located inside the R project you are working in?\n\n You cannot load excel files into R. You must save it as a csv, and read it in with read_csv(). Assuming the readxl package is installed and loaded, type read_xlsx(file = “bird_data.xlsx”, sheet = “raw_data”). While you can read excel into R, you cannot specify the sheet.\n\n\nQ4) What should you do to make code reproducible? (pick the best answer)\n\n Specify the working directory with setwd() Show the packages installed with install.packages() Restart R once your done, and rerun your script to see if it works\n\n\n\n\n\nGlossary of Terms\n\nAbsolute Path – A file location specified from the root directory (e.g., /Users/username/Documents/data.csv), which can cause issues when sharing code across different computers. Using relative paths instead is recommended.\nData Dictionary – A structured document that defines each variable in a dataset, including its name, description, units, and expected values. It helps ensure data clarity and consistency.\nData Validation – A method for reducing errors in data entry by restricting input values (e.g., dropdown lists for categorical variables, ranges for numerical values).\nField Sheet – A structured data collection form used in the field or lab, designed for clarity and ease of data entry.\nMetadata – Additional information describing a dataset, such as when, where, and how data were collected, the units of measurement, and details about the variables.\nR Project – A self-contained environment in RStudio that organizes files, code, and data in a structured way, making analysis more reproducible.\nRaw Data – The original, unmodified data collected from an experiment or survey. It should always be preserved in its original form, with any modifications performed in separate scripts.\nREADME File – A text file that provides an overview of a dataset, including project details, data sources, file descriptions, and instructions for use.\nReproducibility – The ability to re-run an analysis and obtain the same results using the same data and code. This requires careful documentation, structured data storage, and clear coding practices.\nRelative Path – A file path that specifies a location relative to the current working directory (e.g., data/my_file.csv), making it easier to share and reproduce analyses.\nTidy Data – A dataset format where each variable has its own column, each observation has its own row, and each value is in its own cell.\n\n\n\n\nKey R functions\n\n\nclean_names(data) – Standardizes column names (from the janitor package).\ndrop_na(data) – Removes rows with missing values (from the tidyr package)).\nread_csv(\"file.csv\") – Reads a CSV file into R as a tibble (from the readr package).\nread_xlsx(\"file.xlsx\", sheet = \"sheetname\") – Reads an excel sheet into R as a tibble (from the readxl package).\nrename(data, new_name = old_name) – Renames columns in a dataset (from the dplyr package).\npivot_longer(data, cols, names_to, values_to) – Converts wide-format data to long format (from the tidyr package).\nsessionInfo() – Displays session details, including loaded packages (useful for reproducibility).\n\n\n\n\n\nR Packages Introduced\n\n\nreadr – Provides fast and flexible functions for reading tabular data (here we revisited read_csv() for CSV files).\ndplyr – A grammar for data manipulation. Here we introduced the rename(data, new_name = old_name) function to give columns better names.\ntidyr – Helps tidy messy data. Here we introduced pivot_longer() to make wide data long.\njanitor – Cleans and standardizes data, including clean_names()](https://sfirke.github.io/janitor/reference/clean_names.html) for formatting column names.\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nRead a .csv: Learn how to read a csv into R as a tibble.\n\nRead an Excel file: Learn how to read an excel file into R as a tibble.\n\nObey R’s naming rules: You want to give a valid name to an object in R.\n\nRename columns in a table: You want to rename one or more columns in a data frame.\n\nOther web resources:\n\nData Organization in Spreadsheets (Broman & Woo, 2018).\nTidy Data: (Wickham, 2014).\nTen Simple Rules for Reproducible Computational Research: (Sandve, 2013).\nNYT article: For big data scientists hurdle to insights is janitor work.\nStyle guide: Chapter 9 of Data management in large-scale education research by Lewis (2024). Includes sections on general good practices, file naming, and variable naming.\nData Storage and security: Chapter 13 of Data management in large-scale education research by Lewis (2024).\n\nVideos:\n\nData integrity: (By Kate Laskowski who was the victim of data fabrication by her collaborator (and my former roommate) Jonathan Pruitt).\nTidying data with pivor_longer (From Stat454)\n\n\n\n\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989\n\n\nLewis, C. (2024). Data management in large-scale education research. CRC Press.\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules for reproducible computational research. PLOS Computational Biology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, Articles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "# Rest of Book",
    "section": "",
    "text": "Preliminary material\nPreface: Clarkia and its data // Types of variables",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "toc.html#section-i-introduction-to-r",
    "href": "toc.html#section-i-introduction-to-r",
    "title": "# Rest of Book",
    "section": "Section I: Introduction to R",
    "text": "Section I: Introduction to R\n1. Getting Started with R: Functions and vectors // Loading packages and data // Data types // RStudio orientation // Summary of Chapter 1.\n\n2. Working with data in R: Adding variables // Selecting variables // Summarizing variables // Choosing rows // Summary of Working with Data in R.\n\n3. Intro to ggplot: One continuous variable // Saving ggplots // Continuous and categorical // Two categorical variables // Two continuous variables // Many explanatory variables // Summary of Intro to ggplot.\n\n4. Reproducible science: Collecting data // Reproducible analyses // Summary of Reproducible Science.",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "toc.html#second-section-summarizing-data",
    "href": "toc.html#second-section-summarizing-data",
    "title": "# Rest of Book",
    "section": "Second Section: Summarizing Data",
    "text": "Second Section: Summarizing Data\n5. Univariate Summaries: Summarizing shape // Changing shape // Summarizing center // Summarizing variability // Summary of Summaries.\n6. Associations: Categorical vs. continuous // Two categorical variables // Two continuous variables // Summary of Associations.\n7. Linear Models: Mean as a model // Categorical predictors // Regression // Two predictors // Summary of Desciptive Linear Modelling.",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Babbage, C. (1864). Passages from the life of a philosopher.\nLongman; Co.\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with\npartially observed genotype data using copula graphical models.\nJournal of the Royal Statistical Society: Series C (Applied\nStatistics), 68. https://doi.org/10.1111/rssc.12287\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The\nart of skepticism in a data-driven world. Random House.\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in\nspreadsheets. The American Statistician, 72(1), 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and\nanalysis with r. Bookdown. https://stat545.com\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own\nfunctions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r.\nBookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data\nscience: A ModernDive into r and the tidyverse. CRC Press.\n\n\nLewis, C. (2024). Data management in large-scale education\nresearch. CRC Press.\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules\nfor reproducible computational research. PLOS Computational\nBiology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of\nintrogression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is\ndetermined by temporal environmental variation and mating system.\nProceedings of the National Academy of Sciences,\n121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of\nbiological data (Third). Macmillan.\n\n\nWickham, H. (2014b). Tidy data. Journal of Statistical\nSoftware, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWickham, H. (2014a). Tidy data. Journal of Statistical Software,\nArticles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on\nmaking informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "References"
    ]
  }
]