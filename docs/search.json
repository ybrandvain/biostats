[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Biostatistics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#learning-in-this-era",
    "href": "index.html#learning-in-this-era",
    "title": "Applied Biostatistics",
    "section": "Learning in this era",
    "text": "Learning in this era\nI know you’re dealing with a lot. Every year students are dealing with a lot – from jobs, to supporting family, and all the other challenges of modern college life. Yet, we are all trying to make the most of life in this era. We want to teach, learn, and grow.\nMoreover, I believe this content is increasingly important – statistics is obsessed with the critical evaluation of claims in the face of data, and is therefore particularly useful in uncertain times. Given this focus, and given that you all have different energies, motivations and backgrounds, I am restructuring this course slightly from previous years. The biggest change is a continued de-emphasis on math and programming – that doesn’t mean I’m eliminating these features, but rather that I am streamlining the required math and programming to what I believe are the essentials. For those who want more mathematical and/or computational details (either because you want to push yourself or you need this to make sense of things), I am including a bunch of optional content and support. I am also wrestling with the impact of LLMs in our education (more below).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#i-love-teaching-this-course",
    "href": "index.html#i-love-teaching-this-course",
    "title": "Applied Biostatistics",
    "section": "I love teaching this course",
    "text": "I love teaching this course\nThe content is very important to me. I also care deeply about you. I want to make sure you get all you can / all you need from this course, while recognizing the many challenges we are all facing. One tangible thing I leave you with is this book, which I hope you find useful as you go on in your life. Another thing I leave you with is my concern for your well-being and understanding – please contact me with any suggestions about the pace, content, or structure of this course and/or any life updates which may change how and when you can complete the work.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-philosophy-goals",
    "href": "index.html#course-philosophy-goals",
    "title": "Applied Biostatistics",
    "section": "Course philosophy / goals",
    "text": "Course philosophy / goals\nMy motivating goal for this course is to empower you to produce, present, and critically evaluate statistical evidence — especially as applied to biological topics. You should know that statistical models are only models and that models are imperfect abstractions of reality. You should be able to think about how a biological question could be formulated as a statistical question, present graphs which show how data speak to this question, be aware of any shortcomings of that model, and how statistical analysis of a data set can be brought back into our biological discussion.\n\n“By the end of this course…\n\nStudents should be statistical thinkers. \nStudents will recognize that data are comprised of observations that partially reflect chance sampling, & that a major goal of statistics is to incorporate this idea of chance into our interpretation of observations. Thinking this way can be challenging because it is a fundamentally new way to think about the world. Once this is mastered, much of the material follows naturally. Until then, it’s more confusing.\n\n\n Students should think about probability quantitatively.\nThat chance influences observations is CRITICAL to statistics (see above). Quantitatively translating these probabilities into distributions and associated statistical tests allows for mastery of the topic.\n\n\n Students should recognize how bias can influence our results. \nNot only are results influenced by chance, but factors outside of our focus can also drive results. Identifying subtle biases and non-independence is key to conducting and interpreting statistics.\n\n\n Students should become familiar with standard statistical tools / approaches and when to use them. \nRecognize how bias can influence our results. What is the difference between Bayesian and frequentist thinking? How can data be visualized effectively? What is the difference between statistical and real-world significance? How do we responsibly present/ interpret statistical results? We will grapple with & answer these questions over the term.\n\n\n Students should have familiarity with foundational statistical values and concepts. \nStudents will gain an intuitive feel for the meaning of stats words like variance, standard error, p-value, t-statistic, and F-statistic, and will be able to read and interpret graphs, and how to translate linear models into sentences.\n\n\n Students should be able to conduct the entire process of data analysis in R. \nStudents will be able to utilize the statistical language, R, to summarize, analyze, and combine data to make appropriate visualizations and to conduct appropriate statistical tests.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-rstudio-and-the-tidyverse",
    "href": "index.html#r-rstudio-and-the-tidyverse",
    "title": "Applied Biostatistics",
    "section": "R, RStudio, and the tidyverse",
    "text": "R, RStudio, and the tidyverse\n\n\n\n\n\n\nThis image comes with permissions from Allison Horst, who makes tremendous aRt. If you appreciate her work, she would appreciate your support for Data for Black Lives\n\n\n\nWe will be using R (version 4.4.0 or above.) in this course, in the RStudio environment. My goal is to have you empowered to make figures, run analyses, and be well positioned for future work in R, with as much fun and as little pain as possible. RStudio is an environment and the tidyverse is a set of R packages that makes R’s powers more accessible without the need to learn a bunch of computer programming.\nSome of you might have experience with R and some may not. Some of this experience might be in tidyverse or not. There will be ups and downs — the frustration of not understanding and/or it not working and the joy of small successes. Remember to be patient, forgiving and kind to yourself, your peers, and me. Ask for help from the internet, your favorite LLM, your friends, your TAs, and your professor.\n\nR Installation\nBefore you can use R you must download and install it.\\(^*\\)  So, to get started, download R from CRAN, and follow the associated installation instructions (see below for detailed instructions for your system).\\(^*\\) This is not strictly true. You can use R online via posit cloud. This is a “freemium” service and the free plan is unlikely to meet your needs.\n\nPC install guideMac install guideLinux install guide\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a PC youtube link here.\n“To install R on Windows, click the Download R for Windows link. Then click the base link. Next, click the first link at the top of the new page. This link should say something like Download R 4.4.2 for Windows except the 4.4.2 will be replaced by the most current version of R. The link downloads an installer program, which installs the most up-to-date version of R for Windows. Run this program and step through the installation wizard that appears. The wizard will install R into your program files folders and place a shortcut in your Start menu. Note that you’ll need to have all of the appropriate administration privileges to install new software on your machine.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nIf you want a walk through, see Roger Peng’s tutorial on installing R on a mac].\n“To install R on a Mac, click the Download R for macOS link. Next, click on the [newest package link compatible with your computer]. An installer will download to guide you through the installation process, which is very easy. The installer lets you customize your installation, but the defaults will be suitable for most users. I’ve never found a reason to change them. If your computer requires a password before installing new programs, you’ll need it here.”\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nR comes pre-installed on many Linux systems, but you’ll want the newest version of R if yours is out of date. The CRAN website provides files to build R from source on [Debian], Redhat, SUSE, and Ubuntu systems under the link “Download R for Linux.” Click the link and then follow the directory trail to the version of Linux you wish to install on. The exact installation procedure will vary depending on the Linux system you use. CRAN guides the process by grouping each set of source files with documentation or README files that explain how to install on your system.\n\nFrom Appendix A of Hands-On Programming With R – Grolemund (2014).\n\n\n\n\n\n\nAfter installing R download/update RStudio from here.\n\nAlternatively you can simply join the course via RStudioCloud. This could be desirable if you do not want to or have trouble doing this.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-and-how-will-we-use-it",
    "href": "index.html#what-is-this-book-and-how-will-we-use-it",
    "title": "Applied Biostatistics",
    "section": "What is this ‘book’ and how will we use it?",
    "text": "What is this ‘book’ and how will we use it?\nA fantastic feature of this book is that it does not stand alone. It is neither the entirety of the course content, nor is it all my idea. In addition from lifting from a few other courses online (with attribution), I also make heavy use of these texts:\n\nThe Analysis of Biological Data Third Edition (Whitlock & Schluter, 2020): I taught with this book for years. It is fantastic and shaped how I think about teaching Biostats. It has many useful resources available online. The writing is great, as are the examples. Most of my material originates here (although I occasionally do things a bit differently). Buy the latest edition.\nCalling Bullshit (Bergstrom & West, 2020): This book is not technical, but points to the big picture concerns of statisticians. It is very practical and well written. I will occasionally assign readings from this book, and/or point you to videos on their website. All readings will be made available for you, but you might want to buy a physical copy.\nFundamentals of Data Visualization (Wilke, 2019): This book is free online, and is very helpful for thinking about graphing data. In my view, graphing is among the most important skills in statistical reasoning, so I reference it regularly.\nR for Data Science (Grolemund & Wickham, 2018): This book is free online, and is very helpful for doing the sorts of things we do in R regularly. This is a great resource.\nThe storytelling with data podcast is a fantastic data viz podcast. Be sure to check out Cole Nussbaumer Knaflic’s books too!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-will-this-term-work-look",
    "href": "index.html#how-will-this-term-work-look",
    "title": "Applied Biostatistics",
    "section": "How will this term work / look?",
    "text": "How will this term work / look?\n\nPrep for ‘class’. This class is flipped with asynchronous content delivery and synchronous meetings.\n\nBe sure to look over the assigned readings and/or videos, and complete the short low-stakes homework BEFORE each course.\n\nDuring class time, I will address questions make announcements, and get you started on in-class work. The TA & I will bounce around your breakout rooms to provide help and check-in. If you cannot make the class, you could do this on your own time without help, but we do not recommend this as a class strategy.\n\nThe help of your classmates and the environment they create is one of the best parts of this class. Help each other.\n\nIn addition to low stakes work before and in class, there will be a few more intense assignments, some collaborative projects, some in class exams, and a summative project as the term ends.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-use-of-large-language-models",
    "href": "index.html#the-use-of-large-language-models",
    "title": "Applied Biostatistics",
    "section": "The Use of Large Language Models",
    "text": "The Use of Large Language Models\nWe are in the early days of a truly disruptive technology. Large Language Models (LLMs) like ChatGPT and Claude are transforming how we work and learn. While the impact of these tools on future employment, expertise, and citizenry is yet to be settled, it seems clear that no one will hire you to copy and paste AI-generated output. At the same time, no one will hire you to ignore this technology. Success lies in learning how to critically evaluate and work with LLMs—to validate their output, improve your own understanding, and create high-quality results. Subject-level expertise, in conjunction with strong skills in working with AI, will be essential for the foreseeable future.\n\nYou can use LLMs to learn things or avoid learning things. Choose wisely.\n\nLearning from AI and having it help you solve problems will allow you all to do better and learn more than people have been able to do previously. Using AI to avoid learning – e.g. having it write or code for you without you thinking/learning will always come back to bite you in the ass.\nWhile you are ultimately in charge of your learning, I will provide plenty of opportunities for in-class, computer-free efforts to show your mastery of the subject. I will also provide guidance on individual assignments about the appropriate use of AI to help maximize the impact of the assignment on your learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-path-through-the-term",
    "href": "index.html#the-path-through-the-term",
    "title": "Applied Biostatistics",
    "section": "The path through the term",
    "text": "The path through the term\nI start by assuming you know nothing about R or statistics to start (I know this assumption is wrong – many of you all know a lot!). From this humble beginning I aim to leave you with the skills to conduct standard statistical analyses, and the understanding of statistics and the ability to go beyond what we have learned. We take the following path in Figure 1, below:\n\n\n\n\n\n\n\n\nFigure 1: Our journey through biostatistics begins with (1) gaining comfort in R, then moves on to (2) describing data and (3) considering sampling and uncertainty. Next, we (4) introduce null hypothesis significance testing, (5) build models, and (6) address more advanced topics (aka “the big lake of statistics”, aka Lake Isabella).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Applied Biostatistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nStudents\nFirst and foremost, I would like to thank the more than 500 students who have taken my Applied Biostatistics course. Students provide the most important feedback on whether a particular pedagogical approach is effective. While not every experiment succeeds, I am incredibly grateful to each student who has helped me learn what works and what doesn’t as they engaged with the material.\n\n\nTeaching Assistants (TAs)\nI have been fortunate to work with outstanding graduate teaching assistants over the past ten years:\n\nDerek Nedveck: Derek played a key role in helping me establish the course during its early years.\nGerman Vargas Gutierrez: A highly skilled statistician, German’s assistance was invaluable in refining the course a few years into its development.\nChaochih Liu: A brilliant programmer, Chaochih contributed greatly to the course’s organization and structure.\nHusain Agha: Husain has remarkable insights into statistics, genetics, and teaching. My work has greatly benefited from bouncing ideas off him.\nBrooke Kern: Brooke was not only an exceptional TA but also a valuable collaborator. Much of the data in this book is drawn from her dissertation research.\n\n\n\n\n\n\n\n\n\n\nFigure 2: My incredible TAs who have all helped shape this material.\n\n\n\n\n\n\n\nCollaborators\nBrooke Kern, Dave Moeller and Shelley Sianta have generated much of the data in this book and have been patient with my delays in turning around our research during teaching times. Dave also provided nearly every picture in this book.\n\n\nTeaching Colleagues\nI have learned a lot about statistics and how to teach it from John Fieberg. His book, Statistics for Ecologists is fantastic! I am also deeply indebted to Fumi Katagiri who began this course and worked through a lot of it before I arrived at UMN, and who thinks deeply about stats and how to teach it.\n\n\nPeople who provided comments\nJohn Rotenberry, and Ruth Shaw have provided helpful comments!\n\n\nUnknowing contributors\nThe online community of statistics and R teaching is an amazing place. I have borrowed heavily from the many amazing free resources. Here are the most critical:\n\nAllison Horst has fantastic illustrations for statistics that she makes freely available.\nPeter D.R. Higgins has created a truly marvelous book – Reproducible Medical Research With R (Higgins (2024)). I have learned a lot and stolen some teaching tricks from this work.\nJenny Bryan has helped me think about getting students able to do things in R well and quickly. Her book, STAT 545: Data wrangling, exploration, and analysis with R (Bryan (2020)), is a classic.\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and analysis with r. Bookdown. https://stat545.com\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own functions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r. Bookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of biological data (Third). Macmillan.\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RILs between sympatric and allopatric parviflora\nWouldn’t it be cool if, at this stage, the populations could evolve a mechanism to preferentially mate with their own kind? The adaptive evolution of avoiding mating with a closely related species—a process known as reinforcement—does just that. However, the evolution of reinforcement is complex and has only been conclusively documented in a handful of cases.\nDave Moeller and colleagues (including me) have been investigating one potential case of reinforcement. Clarkia xantiana subspecies parviflora (hereafter parviflora) is an annual flowering plant native to California. Unlike its outcrossing sister subspecies, Clarkia xantiana subspecies xantiana (hereafter xantiana), parviflora predominantly reproduces through self-pollination.\nNot all populations of parviflora self-fertilize at the same frequency. Dave has observed that populations sympatric with (i.e., occurring in the same area as) xantiana appear more likely to self-fertilize than allopatric populations (Figure 1). Over the past few years, we have conducted numerous studies to evaluate the hypothesis that this increased rate of self-fertilization has evolved via reinforcement as a mechanism to avoid hybridizing with xantiana.\nThroughout this book, I will use data related to the topic of divergence, speciation, and reinforcement between Clarkia subspecies as a path through biostatistics. I hope that this approach allows you to engage with the statistics while not having to keep pace with a bunch of different biological examples. Below, I introduce the major datasets that we will explore.\nFigure 2: Making a RIL population: A cross between individuals from two populations is followed by multiple generations of self-fertilization. As a result, each “line” becomes a mosaic of ancestry blocks inherited from either initial parent of the RIL. The figure above (from Behrouzi & Wit (2017)) illustrates this process, with the original parental chromosome segments depicted in green and red.\nTo investigate which traits, if any, help parviflora populations sympatric with xantiana avoid hybridization, Dave generated Recombinant Inbred Lines (RILs). To do so, he crossed a parviflora plant from “Sawmill Road”—a population sympatric with xantiana—with a parviflora plant from “Long Valley,” far from any xantiana populations. After this initial cross, lines were self-fertilized for eight generations. This process breaks up and shuffles genetic variation from the two parental populations while ensuring each line is genetically stable.\nBy setting these RILs out in the field and observing how many pollinators visited each line, we hope to identify which traits influence pollinator visitation and ultimately hybridization. Because parviflora plants often self-pollinate and because pollinators effectively transfer pollen from the plentiful xantiana plants to parviflora, we assume that greater pollinator visitation corresponds to higher hybrid seed set. However, we will test this assumption!!!",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "href": "book_sections/clarkia_and_its_data/clarkia_and_its_data.html#rils-between-sympatric-and-allopatric-parviflora",
    "title": "Motivating biology and datasets",
    "section": "",
    "text": "RIL Data\nBelow is the RIL dataset. You can learn about the columns (in the Data dictionary tab) and browse the data (in the Data set tab). The full data are available at\nthis link. Aside from pollinator visitation and hybrid seed set, all phenotypes measured come not from the plants in the field, but means from replicates of the genotype grown in the greenhouse.\n\nRIL variationData DictionaryData set\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: An illustration o the variabiltiy in the recombinant inbred lines. Pictures by Taz Mueller and arranged by Brooke Kern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable_Name\nData_Type\nDescription\n\n\n\n\nril\nCategorical (Factor/String)\nIdentifier for Recombinant Inbred Line (RIL). This is the 'genotype'.\n\n\nlocation\nCategorical (Factor/String)\nField site where the plant was grown.\n\n\nprop_hybrid\nNumeric (discrete)\nProportion of genotyped seeds that were hybrids (see num_hybrid and offspring_genotyped for more information).\n\n\nmean_visits\nNumeric\nAverage number of pollinator visits per plant over a 15-minute observation.\n\n\ngrowth_rate\nNumeric\nGrowth rate of the plant.\n\n\npetal_color\nCategorical (Binary)\nPetal color phenotype (in this case 'pink' or 'white').\n\n\npetal_area_mm\nNumeric\nDate when the first flower opened (in Julian days, i.e., days since New Year's).\n\n\ndate_first_flw\nDate\nNode position of the first flower on the stem.\n\n\nnode_first_flw\nNumeric\nPetal area measured in square millimeters (mm²).\n\n\npetal_perim_mm\nNumeric\nPetal perimeter measured in millimeters (mm).\n\n\nasd_mm\nNumeric\nThe Anther-Stigma Distance (ASD) is the linear distance between the closest anther (the floral part that releases pollen) and the stigma (the floral part that accepts pollen) in a flower, measured in millimeters (mm). The smaller this distance, the more opportunity for self-fertilization.\n\n\nprotandry\nNumeric\nDegree of protandry (e.g., time difference between male and female phase) measured in days. More protandry means more outcrossing.\n\n\nstem_dia_mm\nNumeric\nStem diameter measured in millimeters (mm).\n\n\nlwc\nNumeric\nLeaf water content (LWC).\n\n\ncrossDir\nCategorical (Binary)\nCross direction\n\n\nnum_hybrid\nNumeric (discrete)\nThe number ofseeds that where hybrid.\n\n\noffspring_genotyped\nNumeric (discrete)\nThe number of seeds genotyped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRIL Hybridization Data\nBelow is the hybridization dataset. For each plant in the field we genotyped eight seeds at species-specific markers to identify if they were the product of hybridization with xantiana. The phenotypes belong to the genotype of the maternal plant (i.e. they are the same as those in the pollinator visitation data set). I include data at both the level of the seed and a summary at the level of the maternal plant.\n\n\nRIL Combined Data\n\n\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with partially observed genotype data using copula graphical models. Journal of the Royal Statistical Society: Series C (Applied Statistics), 68. https://doi.org/10.1111/rssc.12287",
    "crumbs": [
      "Motivating biology and datasets"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html",
    "href": "book_sections/example_mini/varType.html",
    "title": "Types of Variables",
    "section": "",
    "text": "Explanatory and Response Variables\nAlthough these taxa hybridize in nature, they remain quite distinct, even in areas of sympatry where they exchange genes.\nDave Moeller and his colleagues (including me) have been studying this species for decades. Their research addresses fundamental questions in evolution, such as:\nTo answer these big and exciting questions, Dave and his collaborators must break them down into smaller, direct scientific studies that can be addressed through a combination of experiments and observations. To conduct such studies, we must map these complex ideas onto measurable variables.\nFor example, rather than directly comparing the flower images in Figure 1, we simplify these flowers into variables that summarize them. For instance, we could represent a flower using a set of variables such as flower color, petal length, the distance between stigma and style, etc.\nDave and his team have conducted numerous studies to tackle these big questions. In most cases, they examine how the value of a response variable — the outcome we aim to understand — changes with different values of one or more explanatory variables (also called predictor or independent variables), which are thought to influence or be associated with the biological process of interest.\nUnderstanding the distinction between explanatory and response variables is crucial for framing hypotheses, designing experiments, and interpreting statistical results. However, this distinction often depends on how the research question is framed and can even vary within a single study. For example, in a recent study on predictors of pollinator visitation in parviflora",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "href": "book_sections/example_mini/varType.html#explanatory-and-response-variables",
    "title": "Types of Variables",
    "section": "",
    "text": "We first aimed to identify which loci in the genome predicted flower color and petal length in parviflora. Here, genotype was the explanatory variable, while the floral attributes (petal color and petal length) were the response variables.\nWe then asked whether certain floral attributes (petal color and petal length) predicted pollinator visitation to parviflora plants. In this case, the floral attributes became the explanatory variables, and pollinator visitation was the response variable.",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#types-of-variables",
    "href": "book_sections/example_mini/varType.html#types-of-variables",
    "title": "Types of Variables",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\n\n\n\n\n\n\nFigure 2: Which type of variable is color? Some variables (like color) can be treated as either categorical or continuous depending on the question. Because most parviflora plants are either pink or white, we treat color as a binary categorical variable. But, as shown above, color can be measured and analyzed quantiatively as well.\n\n\n\n\n\n# INSERT POLINATOR OBSERVATION VIDEO \n\nVariables come in different flavors, and knowing the flavor of a variable is key to choosing appropriate summary statistics, data visualizations, and statistical models. Our parviflora pollinator visitation example above included both major types of variables (Figure 1):\n\nNumeric variables are quantitative and have magnitude. For instance, we measured pollinator visits as the number of times a pollinator visited a flower during a [5-minute observation period], and petal length in centimeters.\n\nCategorical variables are qualitative. In our example, flower color and genotype were treated as categorical variables.\n\nLike much of stats – the line between these types of variables is blurry. For example, we often treat color as a category, but color can be measured quantitatively (Figure 2). So depending on our question we may want to treat color as either a numeric or categorical variable.\n\nNot All Numbers Are Numeric. For example, a gene ID may be represented as a number, but it is an arbitrary label rather than a measurement. Similarly, in our Clarkia studies, some sites were identified by numbers (e.g., Site 22 or Site 100). However, Site 22 is not “less than” Site 100 — these are categorical variables, despite being represented numerically.\n\nWithin these two categories are further sub-flavors which allow us to further refine our statistical approach:\n\nTypes of Numeric Variables\n\n\n\n\n\n\n\n\n\nFigure 3: Types of numeric variables.  A discrete variable: Xantiana flowers with four, five or six petals (photo courtesy of Dave Moeller).   A continuous variable: Parviflora petal whose length is being measured. Image from The University and Jepson Herbaria University of California, Berkeley. Copyright from ©2020 Chris Winchell. (image link)\n\n\n\n\nNumeric variables can be categorized into two main types:\n\nDiscrete variables come in chunks. For instance, flowers receive zero, one, two, three, and so on, pollinators. Pollinators do not come in fractions, making this variable inherently discrete.\nContinuous variables can take any value within a range. Classic examples include height, weight, and temperature. In our example, petal length is a continuous variable because it can be measured to any level of precision within its range.\n\n\nSubtle Distinctions and Blurry Boundaries. The two cases above represent pretty clear distinctions between discrete and continuous variables, but sometimes such distinctions are more subtle.\nConsider the number of pollinators—these cannot be fractional, but the number of pollinators per minute can be fractional. There are other similarly blurry cases. For example, time to first flower is inherently continuous, but we often check on flowers only once per day, so it is measured as discrete. Similarly, in human studies, age is usually reported in whole months or years (discrete), rather than on the more continuous scale of fractional seconds. In such cases, the appropriate question is not “is my data discrete or continuous?” but rather “what process generated my data? and what statistical distribution do the data follow?\n\n\n\nCategorical variables\n\n\n\n\n\n\n\n\n\nFigure 4: Species is a categorical variable: The Clarkia specialist – Clarkia Evening Bee, (Hesperapis regularis) on a Clarkia flower. Shared by © Gene H under CC BY-NC 4.0 copyright on iNaturalist. In our Clarkia research, pollinator is usually nominal (that is which bee species), but is sometimes binary (Clarkia specialist, or non-specialist), and sometimes ordinal (e.g. frequent pollinator, rare pollinator, never pollinator).\n\n\n\n\nCategorical variables are qualitative, and include, nominal, binary, and ordinal variables.\n\nNominal variables cannot be ordered and have names – like sample ID, species, study site etc…\n\nBinary variables are a type of nominal variable with only two options (or for which we only consider two options. Alive/dead, pass/fail, on/off are classic binary variables). In our example of pollinator visitation in parviflora, we considered only two flower colors (pink/white) so flower color in this case is binary.\n\nOrdinal variables can be ordered, but do not correspond to a magnitude. For example, bronze, silver and gold medals in the Olympics are ranked from best to worst, but first is not some reliable distance away from second or third etc… In our pollinator example, we often may wish to distinguish between frequent pollinators (e.g. specialist bees, Figure 4), common but less frequent pollinators (e.g. non-specialist bees), and rare/incidental pollinators (e.g. flies).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/example_mini/varType.html#closing-resources",
    "href": "book_sections/example_mini/varType.html#closing-resources",
    "title": "Types of Variables",
    "section": "Closing Resources",
    "text": "Closing Resources\n\nSummary\nUnderstanding the types of variables in our data can help us translate complex biological questions into measurable data that can be evaluated with the statistical tools we develop in this book. Variables can be categorized as numeric or categorical, and further subdivided into types like discrete, continuous, nominal, binary, or ordinal. These classifications influence how we summarize, visualize, analyze, and modelize our data.\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you’ve got what you needed from it.\n\n\n\nPractice Questions\nTry the questions below! Likert scales look like this:- How do you feel about Clarkia? (1) Love it  (2) Like it  (3) Don’t care  (4) Do not like (5) Hate it\n\nQ1) In a species with pink or white flowers, flower color is a special kind of categorical variable known as a ___ variable NominalBinaryOrdinalBimodal\n\n\nClick here for explanation\n\nThis was a little tricky! The correct answer is Binary, because there are only two possible values. If you picked either “nominal” or “bimodal,” you’re pretty close and definitely thinking along the right path, but not quite there! So let’s walk through these “not quite right” answers:\n\nNominal: Petal color is nominal in the sense that “pink” isn’t greater or lesser than “white”—the categories have no natural order. But because there are only two options here, the more specific (and better) description is Binary.\nBimodal: A bimodal distribution refers to a numeric variable that has two distinct peaks or clusters. If we had measured flower color quantitatively—say, by recording percent reflectance at 550 nm—and the data clustered around two values (say, “mostly pink” vs. “mostly white”), then the distribution would be bimodal. (But even then, we’d probably simplify it to binary for analysis.)\n\nIf you answered “ordinal,” you should probably take another look at the chapter—ordinal variables have a meaningful order, like “small,” “medium,” and “large.”\n\n.\nQ2) Which of these variables is best described as continuous? Flower color (pink, white)Petal lengthNumber of flowers on a plant\nQ3) The number of offspring produced by a single animal in one breeding season is: BinaryContinuousDiscrete\nQ4) TRUE or FALSE: Populations of Clarkia that we named 100 and 22 are numeric TRUEFALSE.\nQ5) TRUE or FALSE: A variable on a “Likert scale” (see margin for details) is clearly numeric TRUEFALSE.\n\n\nClick here for explanation\n\nThe word “clearly” is the key clue here. A Likert scale (like rating agreement from “Strongly disagree” to “Strongly agree”) is based on numbers (e.g., 1, 2, 3, 4, 5), but those numbers represent ordered categories, not truly continuous or clearly numeric values.\nIn other words, while you can treat Likert scale data like numbers sometimes (e.g., calculating averages), the numbers themselves are standing in for categories with an order—not for measured quantities along a true number line. So while you might have seen Likert data analyzed using means, t-tests, or even regressions—treating them like numeric variables—this is a common (and sometimes reasonable) modeling shortcut. Conceptually, Likert data are still clearly ordinal: they are ordered categories, not continuous measurements.\nIf you missed this, don’t worry — Likert scales can be a little tricky because they look numeric. But always pay close attention to what the numbers mean. If they’re just labeling ordered choices (rather than measuring something truly continuous, like height or weight), the variable is ordinal categorical, not clearly numeric.\n\n.\nQ6) The variable, kingdom (corresponding to one of the six kingdoms of life), is a ___ variable NominalBinaryOrdinalDiscrete\nQ7) TRUE of FALSE: A continuous variable can never be modeled as discrete and vice versa TRUEFALSE\n\n\n\n\nGlossary of Terms\n\n\nVariable: A characteristic or attribute that can take on different values or categories in a dataset.\n\nExplanatory Variable: Also known as a predictor or independent variable, this is a variable that is thought to influence or explain the variation in another variable.\nResponse Variable: Also known as a dependent variable, this is the outcome or effect being studied, which changes in response to the explanatory variable.\n\n\n\n\nNumeric Variable: A variable that represents measurable quantities and has magnitude, either as counts (discrete) or as continuous values.\n\nDiscrete Variable: A numeric variable that represents distinct, separate values or counts (e.g., number of pollinators).\nContinuous Variable: A numeric variable that can take any value within a range and is measured with precision (e.g., petal length).\n\n\n\n\nCategorical Variable: A variable that represents categories or groups and is qualitative in nature.\n\nNominal Variable: A categorical variable without an inherent order (e.g., flower species or study site).\nBinary Variable: A nominal variable with only two possible categories (e.g., alive/dead, pink/white).\nOrdinal Variable: A categorical variable with a meaningful order, but without measurable distances between levels (e.g., gold, silver, bronze).",
    "crumbs": [
      "Types of Variables"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html",
    "href": "book_sections/intro2r_index.html",
    "title": "SECTION I: Intro to R",
    "section": "",
    "text": "A “tidyverse” Approach\nFigure 2: A collection of Tidyverse hex stickers representing key R packages for data science, including dplyr, ggplot2, tidyr, readr, and more—each with a unique thematic design.\nAs R has evolved over time and its capabilities can be extended with packages (we will discuss this soon), different “dialects” of R have emerged. While many of you have likely seen Base R – built on the standard R program you download, here we will use Tidyverse – a specific and highly standardized set of packages designed for data science workflows (Figure 2). As a broad overgeneralization, Base R allows for much more control of what you are doing but requires more programming skill, while tidyverse allows you to do a lot with less programming skill.\nI focus on tidyverse programming, not because it is better than base R, but because learning tidyverse is a powerful way to do a lot without learning a lot of formal programming. This means that you will be well prepared for a lot of complex data analysis. If you continue to pursue advanced programming in R (or other languages) you will have some programming concepts to catch up on.\nIf you already know how to accomplish certain tasks with base R tools, I encourage you to invest the time in learning the equivalent approaches in tidyverse. While it may feel redundant at first, this foundation knowledge will make you a more versatile and effective R programmer in the long term, and will allow you to make sense of what we do throughout the term.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "href": "book_sections/intro2r_index.html#important-hints-for-r-coding",
    "title": "SECTION I: Intro to R",
    "section": "Important hints for R coding",
    "text": "Important hints for R coding\nYears of learning and teaching have taught me the following key points about learning R. These amount to the simple observation that a student’s mindset and attitude towards learning and using R is the most important key to their success. I summarize these tips in the video and bullet points, below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Replace your automatic negative thoughts with balanced alternatives.\n\n\n\n\n\nBe patient with yourself. Every expert R programmer started exactly where you are now. Your understanding will grow naturally as you tackle real problems and challenges. Do not beat yourself up, you are learning. Replace automatic negative thoughts with balanced thoughts (Figure 3).\nR is literally a language. Languages take a while to learn – at first, looking at an unfamiliar alphabet or hearing people speak a foreign language makes no sense. Aluksi vieraan aakkoston katsominen tai vieraan kielen puhumisen kuuleminen ei tunnu lainkaan järkevältä. With time and effort, you can make sense of a bunch of words and sentences but it takes time. You are not dumb for not understanding the sentence I pasted above (and if you do understand it is because you know Finnish, not because you are smart).\nYou don’t need to memorize anything. You have access to dictionaries, translators, LLMs etc etc. That said, these tools or more useful the more you know.\nDo not compare yourself to others. R will come fast to some, and slower to others. This has absolutely nothing to do with either your intelligence or your long-term potential as a competent R user.\nStart small and don’t be afraid to experiment. There is nothing wrong about typing code that is imperfect and/or does not work out. Start with the simplest way of addressing your problem and see how far you get. Start small, maybe with some basic data analysis or creating a simple plot. Each little victory builds your confidence. You can always try new and more complex approaches as you go.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/intro2r_index.html#whats-ahead",
    "href": "book_sections/intro2r_index.html#whats-ahead",
    "title": "SECTION I: Intro to R",
    "section": "What’s ahead?",
    "text": "What’s ahead?\n\n\n\n\n\n\n\n\nFigure 4: A pretty scene of Clarkia’s home showing the world we get to investigate as we get equipped with R.\n\n\n\n\n\nNow we begin our intro to R. While we will keep practicing what we have learned and learning new R stuff all term, the next four chapters, listed below will get you started:\n\nGetting up and Running. This section introduces RStudio, math in R, vectors, variable assignment, using functions, r packages, loading data (from the internet), and data types. There is a lot here!\nData in R. Here we continue on our introduction to R. We first introduce the concept of tidy data, and introduce the capabilities of the tidyverse package, dplyr.\nIntro to ggplot. The ggplot package allows us to make nice plots quickly. We will get started understanding how ggplot thinks, and introduce the wide variety of figures you can make. Later in this term we will make better figures in ggplot.\nReproducible science. We consider how to collect data, and store it in a folder. We then introduce the concept of R projects and loading data from our computer. Finally, we introduce the idea of saving R scripts.",
    "crumbs": [
      "SECTION I: Intro to R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html",
    "href": "book_sections/getting_started.html",
    "title": "1. Getting started with R",
    "section": "",
    "text": "What is R? What is RStudio?\nR is a computer program built for data analysis. As opposed to GUIs, like Excel, or click-based stats programs, R is focused on writing and sharing scripts. This enables us to be shared and replicate analyses, ensuring that data manipulation occurs in a script. This practice both preserving the integrity of the original data, while providing tremendous flexibility. R has become the computer language of choice for most statistical work because it’s free, allows for reproducible analyses, makes great figures, and has many “packages” that support the integration of novel statistical approaches. In a recent paper, we used R to analyze hundreds of Clarkia genomes and learn about the (Figure 1 from Sianta et al. (2024)). RStudio is an Integrated Development Environment (IDE)—a nice setup to interact with R and make it easier to use.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "href": "book_sections/getting_started.html#what-is-r-what-is-rstudio",
    "title": "1. Getting started with R",
    "section": "",
    "text": "More precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\n— From Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Ismay & Kim, 2019)",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "href": "book_sections/getting_started.html#the-shortest-introduction-to-r",
    "title": "1. Getting started with R",
    "section": "The Shortest Introduction to R",
    "text": "The Shortest Introduction to R\nBefore opening RStudio, let’s get familiar with two key ays we use R – (1) Using R as a calculator, and (2) Storing information by assigning values to variables.\nR can perform simple (or complex) calculations. For example, entering 1 + 1 returns 2, and entering 2^3 (two raised to the power of three) returns 8. Try it yourself by running the code below, and then experiment with other simple calculations.Math in R: See posit's recipe for using R as a calculator for more detail.\nCommenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!Commenting code The hash, #, tells R to stop reading your code. This allows you to “comment” your code – keeping notes to yourself and other readers about what the code is doing. Commenting your code is very valuable and you should do it often!\nChallengeSolution\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nStoring values in variables allows for efficient (and less error-prone) analyses, while paving the way to more complex calculations. In R, we assign values to variables using the assignment operator, &lt;-. For example, to store the value 1 in a variable named x, type x &lt;- 1. Now, 2 * x will return 2.\n\n\nx &lt;- 1 # Assign 1 to x\n2 *  x # Multiply x by 2\n\n[1] 2\n\nBut R must “know” something before it can “remember” it. The code below aims to set y equal to five, and see what y plus one is (it should be six). However, it returns an error. Run the code to see the error message, then fix it!\n\nChallengeSolution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nR reads and executes each line of code sequentially, from top to bottom. Think about what y + 1 means to R if it hasn’t seen a definition of y yet.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHint\n\nIn R, variables must be defined before they are used. When you try to use y + 1 before assigning a value to y, R throws an error because it doesn’t know what y is yet. When we switch the order—assigning y &lt;- 5 before using y + 1—R understands the command and evaluates it properly.\n\n\n\n\nNow, try assigning different numbers to x and y, or even using them together in a calculation, such as x + y. Understanding this concept of assigning values is critical to understanding how to use R.",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started.html#lets-get-started-with-r",
    "href": "book_sections/getting_started.html#lets-get-started-with-r",
    "title": "1. Getting started with R",
    "section": "Let’s get started with R",
    "text": "Let’s get started with R\nThe following sections introduce the very basics of R including:\n\nFunctions and vectors in R.\n\nLoading packages and data into R.\n\nData types in R.\n\nAn orientation to RStudio.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data science: A ModernDive into r and the tidyverse. CRC Press.\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of introgression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is determined by temporal environmental variation and mating system. Proceedings of the National Academy of Sciences, 121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121",
    "crumbs": [
      "1. Getting started with R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html",
    "href": "book_sections/getting_started/functions_and_vectors.html",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R Functions\nR comes with tons of built-in functions that do everything from basic math to advanced statistical modeling. So not only can we calculate the mean and variance in Clarkia xantiana petal lengths with the mean() and var() functions, respectively, but we can test the null hypothesis that mean petal size in xantiana is equal to that of parviflora with the t.test() function. Functions are the foundation of how we do things in R – they save time and ensure consistency across your analyses.\nFunctions take arguments, which we put in parentheses. When typing sqrt(25), sqrt() is the function, 25 is the argument, and 5 is the output.\nFunctions can take multiple arguments: If you don’t specify them all, R will either tell you to provide them, or assumes default values. For example, the log function defaults to the natural log (base e), so log(1000) returns 6.908. If you want the logarithm with base 10, you need to specify it explicitly as log(1000, 10), which returns 3. Note that argument order matters:—log(10, 1000) returns 0.3333333, while log(1000, 10) returns 3.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "href": "book_sections/getting_started/functions_and_vectors.html#r-functions",
    "title": "• 1. Functions and vectors",
    "section": "",
    "text": "R functions: See posit's recipe for R functions for more detail.\n\n\n\n\n# Natural log of 1000\nlog(1000)             \n\n[1] 6.907755\n\n\n\n\n# Log base 1000 of 10\nlog(1000, base = 10)  \n\n[1] 3\n\n\nTips for using functions in R\n\nUse named arguments in functions.\nFor example, typing log(1000, base = 10) makes what each value represents obvious (improving code readability), and allows flexibility in argument order (e.g. log(base = 10, 1000) gives the same value as log(1000, base = 10)). Thus, using named arguments makes your code readable and robust.\n\n\nUse = to assign arguments in functions\nWhen specifying arguments inside a function, always use = (e.g., log(1000, base = 10)). Do not use &lt;-, which is for assigning values to variables. Otherwise, R might mistakenly store the argument as a variable, leading to unexpected results.\n\n\nPipe together functions with |&gt;\nThe pipe, |&gt;, provides a clean way to pass the output of one function into another. For example, we can find the square root of the \\(\\text{log}_{10}\\) of 1000, rounded to two decimal places, as follows:\n\nlog(1000, base = 10)   |&gt;  \n    sqrt()             |&gt;  \n    round(digits = 2)\n\n[1] 1.73\n\n\nNotice that we did not explicitly provide an argument to sqrt() — it simply used the output of log(1000, base = 10). Similarly, the round() function then rounded the square root of 3 to two decimal places.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "href": "book_sections/getting_started/functions_and_vectors.html#vectors",
    "title": "• 1. Functions and vectors",
    "section": "Working with vectors",
    "text": "Working with vectors\nIf we observed one Clarkia plant with one flower, a second with two flowers, a third with three flowers, and a fourth with two flowers, we could find the mean number of flowers as (1 + 2 + 3 + 2)/4 = 2, but this would be tedious and error-prone. It would be easier to store these values in an ordered sequence of values (called a vector) and then use the (mean()) function.\nVectors are the primary way that data is stored in R—even more complex data structures are often built from vectors. We create vectors with the combine function, c(), which takes arguments that are the values in the vector.\n\n# A vector of flower numbers\n  # 1st plant has one flower\n  # 2nd plant has two flowers\n  # 3rd plant has three flowers\n  # 4th plant has two flowers\nnum_flowers &lt;- c(1, 2, 3, 2)  # Create a vector for number of flowers per plant\nmean(num_flowers) # finding the mean flower number\n\n[1] 2\n\n\n\n\n# If each flower produces four petals  \nnum_petals &lt;- 4 * num_flowers\nnum_petals\n\n[1]  4  8 12  8\n\n\n# If we wanted the log_2 of petal number \nlog(num_petals, base = 2) |&gt;\n  round(digits = 3)\n\n[1] 2.000 3.000 3.585 3.000\n\n\n\nVariable assignment can be optional: In the code, I assigned observations to the vector, num_flowers, and then found the mean. But we could have skipped variable assignment—variable assignment — mean(c(1, 2, 3, 2)) also returns 2.\nThere are two good reasons not to skip variable assignment:\n\nVariable assignment makes code easier to understand. If I revisited my code in weeks I would know what the mean of this vector meant.\nVariable assignment allows us to easily reuse the information For example, below I can easily find the mean petal number.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Functions and vectors"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html",
    "href": "book_sections/getting_started/loading_packages_and_data.html",
    "title": "• 1. Load packages and data",
    "section": "",
    "text": "R packages\nWhile R has many built-in functions, packages provide even more functions to extend R’s capabilities. Packages can offer alternative (often more efficient and user-friendly) approaches to tasks that can be done with base R functions, or they can enable entirely new functionality that is not included in base R at all. In fact, R packages are a major way that the latest statistical and computational methods in various fields are shared with practitioners.\nBelow I introduce the readr, and dplyr packages. Because these packages are so useful for streamlining data import, manipulation, and cleaning, I use them in nearly every R project. I also introduce the conflicted package, which identifies any functions with shared names across packages, and allows us to tell R which function we mean when more than one function has the same name.\nInstall a package the first time you use it The first time you need a package, install it with the install.packages() function. Here the argument is the package (or vector of packages) you want to install. So, to install the packages above, type:\n# We do this the first time we need a package.\ninstall.packages(c(\"readr\", \"dplyr\", \"conflicted\"))\nLoad installed packages every time you open RStudio You only install a package once, but you must use the library() function, as I demonstrate below, to load installed packages every time you open R.\n# We do this every time we open R and want to use these packages.\nlibrary(conflicted)\nlibrary(readr)\nlibrary(dplyr)",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "href": "book_sections/getting_started/loading_packages_and_data.html#reading-data-into-r",
    "title": "• 1. Load packages and data",
    "section": "Reading data into R",
    "text": "Reading data into R\nRather than typing large datasets into R, we usually want to read in data that is already stored somewhere. For now, we will load data saved as a csv file from the internet with the read_csv(link) structure from the readr package. Later, we will revisit the challenge of importing data from other file types and locations into R.\n\n\nLoading data: See posit's recipe for importing data for more detail. Note also that read.csv() is a base R function similar to read_csv(), but it behaves a bit differently – for example it reads data in as a dataframe, not a tibble.\nBelow, I show an example of reading pollinator visitation data from a link on my GitHub. After loading a dataset, you can see the first ten lines and all the columns that fit by simply typing its name. Alternatively, the View() function opens up the full spreadsheet for you to peruse.\n\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data\n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;\n\n\n\n\n\n\npackage::function() format: I read in the data with the read_csv() function in the readr package by typing: readr::read_csv(), but typing read_csv() gives the same result. The package::function() format comes in handy when two functions in different packages have the same name.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Load packages and data"
    ]
  },
  {
    "objectID": "book_sections/getting_started/data_types.html",
    "href": "book_sections/getting_started/data_types.html",
    "title": "• 1. Data types in R",
    "section": "",
    "text": "Motivating scenario: You have loaded data into R and are curious about what “types” of data R thinks it is working with.\nLearning goals: By the end of this sub-chapter you should be able to\n\nList the different types of variables that R can keep in a vector.\n\nIdentify which type of variable is in a given column.\n\nAsk logical questions of the data to make a logical vector.\n\n\n\n\nR handles different types of data in specific ways. Understanding these data types is crucial because what you can do with your data depends on how R interprets it. For example, although you know that 1 + \"two\" equals 3, R cannot add a number and a word. So, to use R effectively, you will need to make sure the type of data R has in memory matches the type it needs to have to do what you want. This will also help you understand R’s error messages and confusing results when things don’t work as expected.\nLooking back at the pollinator visitation dataset we loaded above, we see that (if you read it in with read_csv()) R tells you the class of each column before showing you its first few values. In that dataset, columns one, two, five, six, and fifteen (note R provides a peek of the first few variables — in this case, seven — and then provides the names and data type for the rest) — location, ril, growth_rate, and petal_color — are of class &lt;chr&gt;, while all other columns contain numbers (data of class &lt;dbl&gt;). What does this mean? Well it tells you what type of data R thinks is in that column. Here are the most common options:\n\nNumeric: Numbers, including doubles (&lt;dbl&gt;) and integers (&lt;int&gt;). Integers keep track of whole numbers, while doubles keep track of decimals (but R often stores whole numbers as doubles).\nCharacter: Text, such as letters, words, and phrases (&lt;chr&gt;, e.g., \"pink\" or \"Clarkia xantiana\").\n\nLogical: Boolean values—TRUE or FALSE (&lt;logi&gt;), often used for comparisons and conditional statements.\n\nFactors: Categorical variables that store predefined levels, often used in statistical modeling. While they resemble character data, they behave differently in analyses. We will ignore them in this chapter but revisit them later.\n\nWhen you load data into R, you should always check to ensure that the data are in the expected format. Here we are surprised to see that growth_rate is a character, because it should be a number. A close inspection shows that in row three someone accidentally entered the letter O instead of the number zero (0) in what should be 1.80.\n\n\n\n# A tibble: 4 × 2\n  ril   growth_rate\n  &lt;chr&gt; &lt;chr&gt;      \n1 A1    1.272      \n2 A100  1.448      \n3 A102  1.8O       \n4 A104  0.816      \n\n\nAsking logical questions We often generate logical variables by asking logical questions of the data. Here is how you do that in R.\n\n\n\nQuestion\nR Syntax\n\n\n\n\nDoes a equal b?\na == b\n\n\nDoes a not equal b?\na != b\n\n\nIs a greater than b?\na &gt; b\n\n\nIs a less than b?\na &lt; b\n\n\nIs a greater than or equal to b?\na &gt;= b\n\n\nIs a less than or equal to b?\na &lt;= b",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Data types in R"
    ]
  },
  {
    "objectID": "book_sections/getting_started/rstudio_orientation.html",
    "href": "book_sections/getting_started/rstudio_orientation.html",
    "title": "• 1. Orientation to RStudio",
    "section": "",
    "text": "Motivating scenario: You have just downloaded R and RStudio, and want to understand all the stuff that you see when you open RStudio.\nLearning goals: By the end of this sub-chapter you should be able to\n\nIdentify the source pane and what to do there.\n\nIdentify the terminal pane and what to see and do there.\n\nIdentify the environment / history pane, what to see and do there, and how to navigate tabs in this pane.\n\nIdentify the file / plot / help / viewer pane, what to see and do there, and how to navigate tabs in this pane.\n\n\n\n\n\n\nAbove, you ran R in this web browser, but more often you will work with R in RStudio. When you open RStudio for the first time, you will see three primary panes. The one on the left works identically to the basic R console. Navigating to ‘File &gt; New File &gt; R Script’ opens a new script and reveals a fourth pane.\n\n\nR Scripts are ways to keep a record of your code so that you can pick up where you left off, build on previous work, and share your efforts. We will introduce R Scripts more formally soon!\n\n\n\nLike the R console above (and all computer languages) RStudio does not “know” what you wrote until you enter it into memory. There are a few ways to do this, but our preferred way is to highlight the code you intend to run, and then click the Run button in the top right portion of the R script pane. Alternatively, press Ctrl+Return for Windows/Linux or ⌘+Return on OS X.\n\n\n\n\n\n\n\n\nFigure 1: More panes = less pain. A brief tour of RStudio’s panes.\n\n\n\n\n\nFigure 1 shows what your RStudio session might look like after doing just a little bit of work:\n\nThe source pane Pane 1 is used for writing and editing scripts, R Markdown files etc. This is where you write reproducible code that can be saved and reused.\nThe console pane Pane 2 is basically the R command prompt from vanilla R, it is where you directly interact with R. You can type commands here to execute them immediately. It will display output, messages, and error logs.\nThe environment / history pane Pane 3 shows what R has in working memory and what it has done.\n\nThe Environment Tab shows all objects (e.g., data frames, vectors) currently in memory.\n\nThe History Tab shows all the commands you have run in your session. You can even search through your history, which can be easier than scrolling through the console.\n\nThe files / plots / help / viewer pane. Pane 4 is remarkably useful!\n\nThe Plots Tab shows the plots generated during your session. You can delete an individual plot by clicking the red X button, or delete all plots by clicking the broom button.\nThe Help Tab: allows you to access documentation and help files.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Orientation to RStudio"
    ]
  },
  {
    "objectID": "book_sections/getting_started/getting_started_summary.html",
    "href": "book_sections/getting_started/getting_started_summary.html",
    "title": "• 1. Getting started summary",
    "section": "",
    "text": "Figure 1: Some pretty R from Allison Horst.\n\n\n\n\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. Additional resources.\n\nChapter summary\nR is (much more than just) a simple calculator – it can keep track of variables, and has functions to make plots, summarize data, and build statistical models. R also has many packages that can extend its capabilities. Now that we are familiar with R, RStudio, vectors, functions, data types and packages, we are ready to build our R skills even further to work with data!\n\n\nChatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\n\n\n\n\n\n\n\n\n\nFigure 2: Some encouragement from Allison Horst.\n\n\n\n\nThe interactive R environment below allows you to work without switching tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) Entering \"p\"^2 into R produces which error?\n\n What error? It works great? Error: object p not found Error: object of type closure is not subsettable Error in “p”^2 : non-numeric argument to binary operator\n\nQ2) Which logical question provides an unexpected answer?\n\n (2.0 + 1.0) == 3.0 (0.2 + 0.1) == 0.3 2^2 &gt; 8 (1/0) == (10 * 1/0)\n\n\n\nClick here for an explanation\n\nThis is a floating-point precision issue. In R (and most programming languages), some decimal values cannot be represented exactly in the binary code that they use under the hood. To see this, try (0.2 + 0.1) - 0.3:\n\n(0.2 + 0.1) - 0.3\n\n[1] 5.551115e-17\n\n\nIf you are worried about floating point errors, use the all.equal() function instead of ==, or round to 10 decimal places before asking logical questions.\n\nQ3) R has a built-in dataset called iris. You can look at it or give it to functions by typing iris. Which variable type is the Species in the iris dataset?\n\n numeric logical character factor\n\nFor the following questions consider the diabetes dataset available at: https://rb.gy/fan785\nQ4) Which variable in the diabetes dataset is a character but should be a number?:\n\n ratio location age frame none\n\nQ5) True OR False: The numeric variable, bp.1d, is a double, but could be changed to an integer without changing any of our analyses: TRUEFALSE\nQ6) Which categorical variable in the dataset is ordinal?\n\n id location gender frame\n\nQ7) You collected five leaves of the wild grape (Vitis riparia) and measured their length and width. You have a table of lengths and widths of each leaf and a formula for grape leaf area (below).\nThe area of a grape leaf is: \\[\\text{leaf area } = 0.851 \\times \\text{ leaf length } \\times \\text{ leaf width}\\] The data are here, each column is a leaf:\n\n\n\n\n\nlength\n5.0\n6.1\n5.8\n4.9\n6.0\n\n\nwidth\n3.2\n3.0\n4.1\n2.9\n4.5\n\n\n\n\n\nThe mean leaf area is \n\n\nClick here for a hint\n\n\nFirst make vectors for length and width\n\nlength = c(5, 6.1, 5.8, 4.9, 6)\nwidth = c(3.2, 3, 4.1, 2.9, 4.5)\nThen multiply these vectors by each other and 0.851.\nFinally find the mean\n\n\n\n\nClick here for the solution\n\n\n# Create length and width vectors\nlength &lt;- c(5, 6.1, 5.8, 4.9, 6)\nwidth &lt;- c(3.2, 3, 4.1, 2.9, 4.5)\n\n\nleaf_areas &lt;- 0.851 * length * width # find area\nmean(leaf_areas)                     # find mean\n\n[1] 16.89916\n\n# or in one step:\n(0.851 * length * width) |&gt;\n  mean()\n\n[1] 16.89916\n\n\n\n\n\n\n\nGlossary of Terms\n\n\nR: A programming language designed for statistical computing and data analysis.\nRStudio: An Integrated Development Environment (IDE) that makes using R more user-friendly.\nVector: An ordered sequence of values of the same data type in R.\nAssignment Operator (&lt;-): Used to store a value in a variable.\nLogical Operator: A symbol used to compare values and return TRUE or FALSE (e.g., ==, !=, &gt;, &lt;).\nNumeric Variable: A variable that represents numbers, either as whole numbers (integers) or decimals (doubles).\nCharacter Variable: A variable that stores text (e.g., \"Clarkia xantiana\").\nPackage: A collection of R functions and data sets that extend R’s capabilities.\n\n\n\n\n\nNew R functions\n\n\nc(): Combines values into a vector.\ninstall.packages(): Installs an R package.\nlibrary(): Loads an installed R package for use.\nlog(): Computes the logarithm of a number, with an optional base.\nmean(): Calculates the average (mean) of a numeric vector.\nread_csv() (readr): Reads a CSV file into R as a data frame.\nround(): Rounds a number to a specified number of decimal places.\nsqrt(): Finds the square root of a number.\nView(): Opens a data frame in a spreadsheet-style viewer.\n\n\n\n\n\nR Packages Introduced\n\n\nbase: The core R package that provides fundamental functions like c(), log(), sqrt(), and round().\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\nconflicted: Helps resolve function name conflicts when multiple packages have functions with the same name.\n\n\n\n\nAdditional resources\nThese optional resources reinforce or go beyond what we have learned.\n\nR Recipes:\n\nDoing math in R.\n\nUsing R functions.\n\nImporting data from a .csv.\n\nVideos:\n\nCoding your Data Analysis for Success (From Stat454).\n\nWhy use R? (Yaniv Talking).\n\nAccessing R and RStudio (Yaniv Talking).\n\nRStudio orientation (Yaniv Talking).\n\nR functions (Yaniv Talking).\n\nR packages (Yaniv Talking).\n\nLoading data into R (Yaniv Talking).\n\nData types (Yaniv Talking). Uses compression data as an example.",
    "crumbs": [
      "1. Getting started with R",
      "• 1. Getting started summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html",
    "href": "book_sections/data_in_R.html",
    "title": "2. Data in R",
    "section": "",
    "text": "Tidy data\nFigure 2: A visual explanation of tidy data. Modified from Wickham (2014).\nData can be structured in different ways: in a tidy format, each variable has its own column, and each row represents an observation. In contrast, messy data might combine multiple variables into a single column or store observations in a less structured format. Figure 2 A shows “long” data with one variable per column. Figure 2 B contains boxes (rather than rows or columns) with petals from a given flower laid out neatly, and information about the flower and plant written beneath it. Both formats have their costs and benefits:\nNote that the tidy data format is not necessarily “prettier” or easier to read – in fact, in visual presentation of data for people, we often choose an untidy format. But when analyzing data on our computer, a tidy format simplifies our work. For this reason we will work with tidy data when possible in this book.\nFigure 3: An example of tidy versus untidy data. A) A table where each row is an observation (a petal), and each column is a variable (e.g. pop, plant, image etc…). B) A nicely arranged (but not tidy) sheet of Clarkia xantiana petals - arranged by flower.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tidy-data",
    "href": "book_sections/data_in_R.html#tidy-data",
    "title": "2. Data in R",
    "section": "",
    "text": "Like families, tidy datasets are all alike but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).\nHadley Wickham. Tidy data. Wickham (2014).\n\n\n\nFigure 2 A is “tidy”: Each row is an observation (a petal), and each column is a variable related to that observation. Because this style is so predictable, this format simplifies computational analyses.\n\nFigure 2 B is not “tidy”: There are not simple rows and columns, and variables are combined in a long string. This format is useful in many ways—for example, humans can easily identify patterns, and data can be stored compactly.\n\n\n\n\nBecause all untidy data are different, there is no way to uniformly tidy an untidy dataset. However, the tidyr package has many useful functions. Specifically, the pivot_longer() function allows for converting data from wide format to long format.",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#tibbles",
    "href": "book_sections/data_in_R.html#tibbles",
    "title": "2. Data in R",
    "section": "Tibbles",
    "text": "Tibbles\nA tibble is the name for the primary structure that holds data in the tidyverse. A tibble—much like a spreadsheet—does not automatically make data tidy, but encourages a structured, consistent format that works well with tidyverse functions.\n\nIn a tibble, each column is a vector. This means that all entries in a column must be of the same class. If you mix numeric and character values in a column, every entry becomes a character.\nIn a tibble, each row unites observations. A row can have any mix of data types.\n\n\n\nTibbles vs. Data Frames For base R users – A tibble is much like a data frame, but some minor features distinguish them. See Chapter 10 of Grolemund & Wickham (2018) for more info.\n\n\n\n\n\n\n\n\nFeature\nTibble\nData Frame\n\n\n\n\nWhat you see on screen\nFirst ten rows & cols that fit\nEntire dataset\n\n\nData Types Displayed\nYes – &lt;dbl&gt;, &lt;chr&gt;, etc\nNo\n\n\nSubsetting to one column returns\nA tibble\nA vector\n\n\n\nThe read_csv() function that we introduced earlier to load data imports data as a tibble. Looking at the data below, you are probably surprised to see that growth rate is a character &lt;chr&gt;, because it should be a number &lt;dbl&gt;. A little digging reveals that the entry in the third row has a growth rate of 1.8O (with the letter, O, at the end) which should be 1.80 (with the number 0 at the end)\n\nlibrary(readr)\nlibrary(dplyr)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)\nril_data \n\n# A tibble: 593 × 17\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0     1.272       white                44.0\n 2 A100  GC             0.125       0.188 1.448       pink                 55.8\n 3 A102  GC             0.25        0.25  1.8O        pink                 51.7\n 4 A104  GC             0           0     0.816       white                57.3\n 5 A106  GC             0           0     0.728       white                68.6\n 6 A107  GC             0.125       0     1.764       pink                 66.3\n 7 A108  GC            NA          NA     1.584       &lt;NA&gt;                 51.5\n 8 A109  GC             0           0     1.476       white                48.1\n 9 A111  GC             0          NA     1.144       white                51.6\n10 A112  GC             0.25        0     1           white                89.8\n# ℹ 583 more rows\n# ℹ 10 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "href": "book_sections/data_in_R.html#lets-get-ready-to-deal-with-data-in-r",
    "title": "2. Data in R",
    "section": "Let’s get ready to deal with data in R",
    "text": "Let’s get ready to deal with data in R\nThe following sections introduce the very basics of R including:\n\nAdding columns with mutate.\n\nSelecting columns.\n\nSummarizing columns.\n\nChoosing rows.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "2. Data in R"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html",
    "href": "book_sections/data_in_R/add_vars.html",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Changing or adding variables with mutate()\nOften we want to change the values in a column, or make a new column. For example in our data we may hope to:\nThe mutate() function in the dplyr package can solve this. You can overwrite data in an existing column or make a new column as follows:\nril_data      |&gt;\n  dplyr::mutate(growth_rate = as.numeric(growth_rate),   # make numeric\n                visited = mean_visits &gt; 0)\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `growth_rate = as.numeric(growth_rate)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25       NA     pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;\nAfter confirming this worked, we can assign it to R’s memory: In doing so, I even converted 1.8O into 1.80 so we have an observation in that cell rather than missing data.\nril_data       &lt;- ril_data      |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\n# A tibble: 593 × 18\n   ril   location prop_hybrid mean_visits growth_rate petal_color petal_area_mm\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1 A1    GC             0           0           1.27  white                44.0\n 2 A100  GC             0.125       0.188       1.45  pink                 55.8\n 3 A102  GC             0.25        0.25        1.8   pink                 51.7\n 4 A104  GC             0           0           0.816 white                57.3\n 5 A106  GC             0           0           0.728 white                68.6\n 6 A107  GC             0.125       0           1.76  pink                 66.3\n 7 A108  GC            NA          NA           1.58  &lt;NA&gt;                 51.5\n 8 A109  GC             0           0           1.48  white                48.1\n 9 A111  GC             0          NA           1.14  white                51.6\n10 A112  GC             0.25        0           1     white                89.8\n# ℹ 583 more rows\n# ℹ 11 more variables: date_first_flw &lt;dbl&gt;, node_first_flw &lt;dbl&gt;,\n#   petal_perim_mm &lt;dbl&gt;, asd_mm &lt;dbl&gt;, protandry &lt;dbl&gt;, stem_dia_mm &lt;dbl&gt;,\n#   lwc &lt;dbl&gt;, crossDir &lt;chr&gt;, num_hybrid &lt;dbl&gt;, offspring_genotyped &lt;dbl&gt;,\n#   visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "href": "book_sections/data_in_R/add_vars.html#changing-or-adding-variables-with-mutate",
    "title": "• 2. Adding columns w mutate",
    "section": "",
    "text": "Convert growth_rate into a number, with the as.numeric() function.\nAdd the logical variable, visited, which is TRUE if a plant had more than zero pollinators visit them, and is FALSE otherwise.\n\n\n\n\nWarning… ! NAs introduced by coercion:\nYou can see that R gave us a warning. Warnings do not mean that something necessarily went wrong, but they do mean we should look and see what happened. In this case, we see that when trying to change the character string, 1.8O, into a number R did not know what to do and converted it to NA. In the next bit of code I convert it into \"1.80\" with the case_when() function.\n\n\n\n\n\n\n\n\n\n\nBe careful combining classes with case_when() Click the arrow to learn more\n\n\n\n\n\nWhen I was trying to change the character “1.8O” into the 1.80, R kept saying: Error in dplyr::mutate()… Caused by error in case_when(): ! Can’t combine ..1 (right)  and ..2 (right) . Unlike warnings, which tell you to watch out, errors tell you R cannot do what you’re asking of it. It turns out that I could not assign the number 1.80 to the vector held in petal_area_mm because I could not blend characters add numbers. So, as you can see, I replaced \"1.8O\" with \"1.80\", and then I used as.numeric() to convert the vector to numeric.",
    "crumbs": [
      "2. Data in R",
      "• 2. Adding columns w mutate"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html",
    "href": "book_sections/data_in_R/select_vars.html",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "select()ing columns of interest\nThe dataset above is not tiny – seventeen columns accompany the 593 rows of data. To simplify our lives, let’s use the dplyr function, select(), to limit our data to a few variables of interest:\nril_data |&gt; \n  dplyr::select(location,   prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm, \n                growth_rate, visited)\n\n# A tibble: 593 × 8\n   location prop_hybrid mean_visits petal_color petal_area_mm asd_mm growth_rate\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 GC             0           0     white                44.0  0.447       1.27 \n 2 GC             0.125       0.188 pink                 55.8  1.07        1.45 \n 3 GC             0.25        0.25  pink                 51.7  0.674       1.8  \n 4 GC             0           0     white                57.3  0.959       0.816\n 5 GC             0           0     white                68.6  1.41        0.728\n 6 GC             0.125       0     pink                 66.3  0.788       1.76 \n 7 GC            NA          NA     &lt;NA&gt;                 51.5  0.6         1.58 \n 8 GC             0           0     white                48.1  0.561       1.48 \n 9 GC             0          NA     white                51.6  1.02        1.14 \n10 GC             0.25        0     white                89.8  0.618       1    \n# ℹ 583 more rows\n# ℹ 1 more variable: visited &lt;lgl&gt;",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "href": "book_sections/data_in_R/select_vars.html#selecting-columns-of-interest",
    "title": "• 2. Selecting columns",
    "section": "",
    "text": "location: The plant’s location. The pollinator visitation experiment was limited to two locations (either SR or GC), while the hybrid seed formation study was replicated at four locations (SR, GC, LB or US). This should be a &lt;chr&gt; (character), and it is!\nprop_hybrid: The proportion of genotyped seeds that were hybrids.\n\nmean_visits: The mean number of pollinator visits recorded (per fifteen minute pollinator observation) for that RIL genotype at that site. This should be a number &lt;dbl&gt; (double), and it is.\npetal_area_mm: The area of the petals (in mm). This should be a number &lt;dbl&gt; (double), and it is!\n\nasd_mm: The distance between anther (the place where pollen comes from) and stigma (the place that pollen goes to) on a flower. The smaller this number, the easier it is for a plant to pollinated itself. This should be a number &lt;dbl&gt; (double), and it is.\ngrowth_rate: The variable we should have just fixed now it should be a number.\n\nvisited: A logical variable indicating if the plant received any visits at all.\n\n\n\n\n\n\n\n\nWarning: R does not remember this change until you assign it.\n\n\n\nSo, now that we see that our code worked as expected, enter:\n\nril_data &lt;- ril_data |&gt; \n  dplyr::select(location,  prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm,  \n                growth_rate, visited)",
    "crumbs": [
      "2. Data in R",
      "• 2. Selecting columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html",
    "href": "book_sections/data_in_R/summarize_vars.html",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "summarize()ing data\nWe rarely want to look at entire datasets, we want to summarize() them (e.g. finding the mean, variance, etc..).\nWe previously used the mean function to find the mean of a vector. When we want to summarize a variable in a tibble we use the function inside of summarize().\nComputing summary statistics with summarize(). The top table contains two columns: prop_hyb (proportion of hybrids) and n_hyb (the number of hybrids). The summarize(mean_hyb = mean(n_hyb)) function is applied to calculate the mean of n_hyb, producing a single-row output where mean_hyb represents the average number of hybrids across the dataset. The final result, shown in the bottom table, contains a single value of 1.\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1         NA\nWe notice two things.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "href": "book_sections/data_in_R/summarize_vars.html#summarizeing-data",
    "title": "• 2. Summarizing columns",
    "section": "",
    "text": "The answer was NA. This is because there are NAs in the data.\nThe results are a tibble. This is sometimes what we want and sometimes not. If you want a vector you can pull() the value.\n\n\nSummarize data in the face of NAs with the na.rm = TRUE argument.\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\npull() columns from tibbles to extract vectors.\nMany R functions require vectors rather than tibbles. You can pull() them out as follows:\n\nril_data      |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE))|&gt;\n  pull()\n\n[1] 0.693259\n\n\n\n\n\n\n\n\n\n“A visual explanation of the summarize() function from R for the rest of us.\n\n\n\n\n\n\n\n\n\n\nA visual explanation of the summarize() function from R for the rest of us.",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "href": "book_sections/data_in_R/summarize_vars.html#combine-group_by-andsummarize-to-describe-groups",
    "title": "• 2. Summarizing columns",
    "section": "Combine group_by() andsummarize() to describe groups",
    "text": "Combine group_by() andsummarize() to describe groups\nSay we were curious about differences in pollinator visitation by location. The code below combines group_by() and summarize() to show that site SR had nearly 10 times the mean pollinator visitation per 15 minute observation than did site GC. We also see a much stronger correlation between petal area and visitation in SR than in GC, but a stronger correlation between proportion hybrid and visitation in GC than in SR. Note that the NA values for LB and US arise because we did not conduct pollinator observations at those locations.\n\nril_data      |&gt;\n  group_by(location) |&gt;\n  summarize(grand_mean = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"),\n            cor_visits_prop_hybrid =  cor(mean_visits , prop_hybrid, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n# A tibble: 5 × 4\n  location grand_mean cor_visits_petal_area cor_visits_prop_hybrid\n  &lt;chr&gt;         &lt;dbl&gt;                 &lt;dbl&gt;                  &lt;dbl&gt;\n1 GC            0.116                0.0575                  0.458\n2 LB          NaN                   NA                      NA    \n3 SR            1.27                 0.367                   0.281\n4 US          NaN                   NA                      NA    \n5 &lt;NA&gt;        NaN                   NA                      NA    \n\n\nWe can group by more than one variable. Grouping by location and color reveals not only that white flowers are visited less than pink flowers, but also that petal area has a similar correlation with pollinator visitation for pink and white flowers.\n\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\")) # Like na.rm = TRUE, but for correlations\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 13 × 4\n# Groups:   location [5]\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA     \n\n\n\n\n\n\n\n\nA visual explanation of group_by() + summarize() from R for the rest of us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nungroup()\nAfter summarizing, the data above are still grouped by location. You can see this under #A tibble: 6 x 4 where it says # Groups:   location [5]. This tells us that the data are still grouped by location (these groups correspond to GC, SR, US, LB and missing location information NA). It’s good practice to ungroup() next, so that R does not do anything unexpected.\n\n\nPeeling of groups: Above we grouped by location and petal_color in that order. When summarize data, by default, R peels off one group, following a “last one in is the first one out” rule. This is what is meant when R says: “summarise() has grouped output by ‘location’…”.\n\n# re-running code above and then ungrouping it.\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 13 × 4\n   location petal_color avg_visits cor_visits_petal_area\n   &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1 GC       pink            0.193                 0.0899\n 2 GC       white           0.0104                0.0886\n 3 GC       &lt;NA&gt;            0.2                   0.443 \n 4 LB       pink          NaN                    NA     \n 5 LB       white         NaN                    NA     \n 6 LB       &lt;NA&gt;          NaN                    NA     \n 7 SR       pink            1.76                  0.387 \n 8 SR       white           0.733                 0.458 \n 9 SR       &lt;NA&gt;            1.04                  0.717 \n10 US       pink          NaN                    NA     \n11 US       white         NaN                    NA     \n12 US       &lt;NA&gt;          NaN                    NA     \n13 &lt;NA&gt;     &lt;NA&gt;          NaN                    NA",
    "crumbs": [
      "2. Data in R",
      "• 2. Summarizing columns"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html",
    "href": "book_sections/data_in_R/choose_rows.html",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "Remove rows with filter()\nThere are reasons to remove rows by their values. For example, we could remove plants from US and LB locations. We can achieve this with the filter() function as follows:\nFigure 1: Using filter() to subset data based on a condition. The top table contains two columns: prop_hyb (proportion of hybrids) and petal_color (flower color), with values including both “white” and “pink” flowers. The function filter(petal_color == \"pink\") is applied to retain only rows where petal_color is “pink.” The resulting dataset, shown in the bottom table, excludes the “white” flower row and keeps only the observations where petal color is “pink.”\nOR, equivalently\n# rerunning the code summarizing visitation by \n# location and petal color and then ungrouping it\n# but filtering out plants from locations US and LB\n# note that the output no longer says `# Groups:   location [2]`\nril_data      |&gt;\n  filter(location != \"US\" & location != \"LB\")|&gt;\n  group_by(location, petal_color) |&gt;\n  summarize(avg_visits = mean(mean_visits, na.rm = TRUE),\n            cor_visits_petal_area =  cor(mean_visits ,petal_area_mm, \n                use = \"pairwise.complete.obs\"))|&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 4\n  location petal_color avg_visits cor_visits_petal_area\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n1 GC       pink            0.193                 0.0899\n2 GC       white           0.0104                0.0886\n3 GC       &lt;NA&gt;            0.2                   0.443 \n4 SR       pink            1.76                  0.387 \n5 SR       white           0.733                 0.458 \n6 SR       &lt;NA&gt;            1.04                  0.717",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "href": "book_sections/data_in_R/choose_rows.html#remove-rows-with-filter",
    "title": "• 2. Choose rows",
    "section": "",
    "text": "ril_data |&gt; filter(location == \"GC\" | location == \"SR\"): To only retain samples from GC or (noted by |) SR. Recall that == asks the logical question, “Does the location equal SR?” So combined, the code reads “Retain only samples with location equal to SR or location equal to GC.”\n\n\n\nril_data |&gt; filter(location != \"US\" & location != \"LB\"): To remove samples from US and (noted by &) LB. Recall that != asks the logical question, “Does the location not equal US?” Combined the code reads “Retain only samples with location not equal to US and with location not equal to LB.”\n\n\n\nWarning! Remove one thing can change another:\nThink hard about removing things (e.g. missing data), and if you decide to remove things, consider where in the pipeline you are doing so. Removing one thing can change another. For example, compare:\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.705\n\n\nand\n\nril_data |&gt;\n  summarise(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_visits\n       &lt;dbl&gt;\n1      0.693\n\n\nThese answers differ because when we removed plants with no petal color information we also removed their pollinator visitation values.",
    "crumbs": [
      "2. Data in R",
      "• 2. Choose rows"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html",
    "href": "book_sections/data_in_R/data_in_R_summary.html",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions, R packages introduced, and Additional resources.\nA beautiful Clarkia xantiana flower.\nKeeping data in the tidy format—where each column represents a variable and each row represents an observation—allows you to fully leverage the powerful tools of the tidyverse. In the tidyverse, data are stored in tibbles, a modern update to data frames that enhances readability and maintains consistent data types. The dplyr package offers a suite of intuitive functions for transforming and analyzing data. For example, mutate() lets you create or modify variables, while summarize() computes summary statistics. When paired with group_by(), you can easily generate summaries across groups. Other essential functions include select() for choosing columns, filter() for subsetting rows, rename(), and arrange() for ordering data. Together—and especially when used with the pipe operator (group_by(...) |&gt; summarize(...))—these tools enable clear, reproducible workflows. In the next chapter, you’ll see how tidy data also powers beautiful and flexible plots using ggplot2.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "href": "book_sections/data_in_R/data_in_R_summary.html#data_in_R_summarySummary",
    "title": "• 2. Data in R summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions! Use the R environment below to work without changing tabs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) The code above returns the error: \"Error: could not find function \"summarise\"\". How can you solve this?\n\n Change “summarise” to “summarize” load the dplyr library\n\n\nQ2) Revisit the pollinator visitation dataset we explored. Which location has a greater anther stigma distance (asd_mm)? They are the sameSMGCS6\n\nQ3) Consider the table below. The data are tidynot tidy\n\n\n\n\n\nlocation\nGC\nGC\nGC\nGC\nGC\nGC\n\n\nril\nA1\nA100\nA102\nA104\nA106\nA107\n\n\nmean_visits\n0.0000\n0.1875\n0.2500\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere the data are transposed, so the data are not tidy. Remember in tidy data each variable is a column, not a row. This is particularly hard for R because there are numerous types of data in a column.\n\n.\n\nQ4 Consider the table below. The data are tidynot tidy \n\n\n\n\n\nlocation-ril\nmean_visits\n\n\n\n\nGC-A1\n0.0000\n\n\nGC-A100\n0.1875\n\n\nGC-A102\n0.2500\n\n\nGC-A104\n0.0000\n\n\nGC-A106\n0.0000\n\n\nGC-A107\n0.0000\n\n\n\n\n\n\n\nClick here for explanation\n\nHere location and ril are combined in a single column, so the data are not tidy. Remember in tidy data each variable is its own column. It would be hard to get e.g. means for RILs of locations in this format.\n\n\nQ5 Consider the table below. The data are tidynot tidy\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.0000\n0.6667\n\n\nA100\n0.1875\n0.5833\n\n\nA102\n0.2500\n0.6667\n\n\nA104\n0.0000\n1.7500\n\n\nA106\n0.0000\n0.5000\n\n\nA107\n0.0000\n1.5000\n\n\n\n\n\n\n\nClick here for explanation\n\nThis is known as “wide format” and is not tidy. Here the variable, location, is used as a column heading. This can be a fine way to present data to people, but it’s not how we are analyzing data.\n\n\nQ6 You should always make sure data are tidy when (pick best answer)\n\n collecting data presenting data analyzing data with dplyr all of the above\n\n\nQ7 What is wrong with the code below (pick the most egregious issue).\n\n I overwrote iris and lost the raw data I did not show the output I used summarise() rather than summarize() I did not tell R to remove missing data when calculating the mean.\n\n\niris &lt;- iris |&gt; \n  summarise(mean_sepal_length =  mean(Sepal.Length))\n\n\nQ8 After running the code below, how many rows and columns will the output tibble have? NOTE The original data has 593 rows, 7 columns and 186 unique RILs* 1 row, 1 column1 row, 2 columns2 rows, 2 columns186 rows, 2 columns186 rows, 7 columns593 rows, 2 columns593 rows, 7 columns\n\nril_data   |&gt;\n    group_by(ril) |&gt;\n    summarize(avg_visits = mean(mean_visits, na.rm = TRUE))\n\n\n\n\n\nGlossary of Terms\n\n\nTidy Data A structured format where:\n\nEach row represents an observation.\n\nEach column represents a variable.\n\nEach cell contains a single measurement.\n\nTibbles: A modern form of a data frame in R with:\n\nCleaner printing (only first 10 rows, fits columns to screen).\n\nExplicit display of data types (e.g., , ).\n\nStrict subsetting (prevents automatic type conversion).\n\nCharacter data is not automatically converted to factors.\n\nPiping (|&gt;) functions: A way to chain operations together, making code more readable and modular.\n\nGrouping in Data Analysis: Grouped operations allow calculations within subsets of data (e.g., mean visits per location).\nMissing Data (NA): R uses NA to represent missing values. Operations with NA return NA unless handled explicitly (e.g., na.rm = TRUE to ignore missing values, use = \"pairwise.complete.obs\", etc).\n\nWarnings: Indicate a possible issue but allow code to run (e.g., NAs introduced by coercion).\n\nErrors: Stop execution completely when something is invalid.\n\n\n\n\n\nKey R functions\n\n\nread_csv() (readr): Reads a CSV file into R as a tibble, automatically guessing column types.\nselect() (dplyr): Selects specific columns from a dataset.\nmutate() (dplyr): Creates or modifies columns in a dataset.\ncase_when() (dplyr): Replaces values conditionally within a column.\nas.numeric(): Converts a vector to a numeric data type.\nsummarize() (dplyr): Computes summary statistics on a dataset (e.g., mean, sum).\nmean(): Computes the mean (average) of a numeric vector.\n\nArgument: na.rm = TRUE: An argument used in functions like mean() and sd() to remove missing values (NA) before computation.\n\npull() (dplyr): Extracts a single column from a tibble as a vector.\ngroup_by() (dplyr): Groups data by one or more variables for summary operations.\n|&gt; (Base R Pipe Operator): Passes the result of one function into another, making code more readable.\n\n\n\n\n\nR Packages Introduced\n\n\nreadr: A tidyverse package for reading rectangular data files (e.g., read_csv()).\ndplyr: A tidyverse package for data manipulation, including mutate(), glimpse(), and across().\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nSelecting columns.\n\nAdd a new column (or modify an existing one).\n\nSummarize data.\nSummarize data by group.\n\nOther web resources:\n\nChapter 10: Tidy data from R for data science (Grolemund & Wickham (2018)).\n\nAnimated dplyr functions from R or the rest of us.\n\nVideos:\n\nBasic Data Manipulation (From Stat454).\nCalculations on tibble (From Stat454).\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "2. Data in R",
      "• 2. Data in R summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html",
    "href": "book_sections/intro_to_ggplot.html",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Always visualize your data.\nAnscombe’s quartet famously displays four data sets with identical summary statistics but very different interpretations (Figure 1, Table 1). The key lesson from Anscombe’s quartet is that statistical summaries can miss the story in our data, and therefore must be accompanied by clear visualization of patterns in the data.\ndataset\nsd.x\nsd.y\ncor\nmean.x\nmean.y\n\n\n\n\n1\n3.32\n2.03\n0.82\n9\n7.5\n\n\n2\n3.32\n2.03\n0.82\n9\n7.5\n\n\n3\n3.32\n2.03\n0.82\n9\n7.5\n\n\n4\n3.32\n2.03\n0.82\n9\n7.5\n\n\n\n\n\n\nTable 1: Summary statistics for Anscombe’s quartet.\nFigure 1: Four datasets with identical summary statistics but distinct visual patterns. Each panel represents a dataset with nearly identical means (shown in red), variances, correlations, and regression lines (shown in blue), yet their scatterplots reveal strikingly different structures.\nTherefore before diving into formal analysis, we always generate exploratory plots to uncover key patterns, detect data quality issues, and reveal the underlying structure of the data. One quick exploratory plot is rarely enough though, as you get to know and model your data, you will develop additional visualizations to dig deeper into the story.\nUltimately, after a combination of exploratory plots, summary statistics, and statistical modelling has helped you understand the data, you will generate well-crafted explanatory plots to communicate your insight elegantly to your audience. We will focus on the process of making explanatory plots in a later chapter.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "href": "book_sections/intro_to_ggplot.html#always-visualize-your-data.",
    "title": "3. Introduction to ggplot",
    "section": "",
    "text": "Detect data quality issues early It is common to split up data collect to a team and maybe someone enters data in centimeters, and someone else in inches (etc). Or maybe for a data point or two a decimal point was lost, a value is way bigger than it should be. Everytime you collect and enter data you should make some plots to help identify any such data issues.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "href": "book_sections/intro_to_ggplot.html#focus-on-biological-questions.",
    "title": "3. Introduction to ggplot",
    "section": "Focus on biological questions.",
    "text": "Focus on biological questions.\nBefore starting your R coding, consider the plots you want to make and why. It is all too easy to get stuck in the R vortex—making plots simply because they’re easy to create, visually appealing, fun, or challenging, and before you know it you’ve wasted an hour doing something that did not move your analysis or understanding forward. So before make a (set of) plot(s) always consider\n\nThe thing you want to know, and how it relates to the motivating biology. This includes\n\nIdentifying explanatory and response variables, and\nDistinguishing between key explanatory variables from covariates that you may not care about but need to include.\n\n\nThat the visualization of data reflects your biological motivation.This is particularly tricky for categorical variables which R loves to put in alphabetical order but may likely have a reasonable order to highlight patterns or biology.\n\n\nFor the Clarkia RIL datasets:.\nWe primarily want to know which (if any) phenotypes influence the amount of pollinator visitation and hybrid seed formation.\n\nOur response variables are hybrid seed production and pollinator visitation (and perhaps we would like to know the extent to which pollinator visitation predicts the proportion of hybrid seed).\n\nOur explanatory variables that we care a lot about include: petal area, petal color, herkogamy, protandry etc… We also want to account for differences in the location of the experiment, even though this is not motivating our study.\n\nWe also may want to evaluate the extent to which correlation between traits were broken up as we made the RILs. If trait correlations persists in the RILs it means that we cannot fully dissect the contribution of each trait to the outcome we care about, and that the genetic and/or physiological linkage between traits may have prevented evolution from acting independently on these traits.\n\nIn this case there is not a natural order to our categorical variables, so we do not need to think too hard about that.\n\n\n\n\n\n\n\nVideo\n\n\nFigure 2: Tweet from Shasta E. Webb (@webbshasta) about how she makes a plot. “My approach to figure-making in #ggplot ALWAYS begins with sketching out what I want the final product to look like. It feels a bit analog but helps me determine which #geom or #theme I need, what arrangement will look best, & what illustrations/images will spice it up. #rstats”\n\n\n\n\nLet’s sketch some potential plots to address these questions.\nI recommend sketching out what you want your plot to look like and what alternative results would look like (Figure 2). This helps to ensure that you are making the plot you want, not the one R gives you. In this case, some potentially important questions are:\n\nWhat is the distribution of the number of pollinator visits?\nDo we see different visitation by location?\n\nAre pink flowers more likely to be visited than white flowers?\n\nHow does the number of visits change with petal area, and does this depend on petal color?\n\nDoes pollinator visitation predict hybridization rate?\n\nSee Figure 3 for examples of how these may look.\n\n\n\n\n\n\n\n\nFigure 3: Brainstorming potential figures.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "href": "book_sections/intro_to_ggplot.html#the-idea-of-ggplot",
    "title": "3. Introduction to ggplot",
    "section": "The idea of ggplot",
    "text": "The idea of ggplot\n\n\n\n\nAs described in the video above ggplot is based on the grammar of graphics, a framework for constructing plots by mapping data to visual aesthetics.. A major idea here is that plots are made up of data that we map onto aesthetic attributes, and that we build up plots layer by layer.\nLet’s unpack this sentence, because there’s a lot there. Say we wanted to make a very simple plot e.g. observations for categorical data, or a simple histogram for a single continuous variable. Here we are mapping this variable onto a single aesthetic attribute – the x-axis.\n\nWe are using the ggplot2 package to make plots.\nIf you do not have ggplot2 installed, type:\n\ninstall.packages(\"ggplot2\") # Do this the 1st time!\n\nIf you have ggplot2 installed or you just installed it, every time you start a new R session you still need to enter\n\nlibrary(ggplot2)\n\n\n\n\nShow code to load and format the data set, so we are where we left off.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link)|&gt;\n  dplyr::select(location,  ril, prop_hybrid,  mean_visits,  \n                petal_color, petal_area_mm,  asd_mm)|&gt;\n  dplyr::mutate(visited = mean_visits &gt; 0)",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot.html#making-ggplots",
    "href": "book_sections/intro_to_ggplot.html#making-ggplots",
    "title": "3. Introduction to ggplot",
    "section": "Making ggplots:",
    "text": "Making ggplots:\nThe following sections show how to make plots that:\n\nVisualize the distribution of a single continuous variable.\n\nSaving a ggplot.\n\nCompare a continuous variable for different categorical variables.\n\nVisualize associations between two categorical variables.\n\nVisualize associations between two continuous variables.\n\nVisualize multiple explanatory variables.\n\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.\nFinally, for the true aficionados, I introduce ggplotly as a great way to get to know your data.",
    "crumbs": [
      "3. Introduction to ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/one_continuous.html",
    "href": "book_sections/intro_to_ggplot/one_continuous.html",
    "title": "• 3. A continuous variable",
    "section": "",
    "text": "Motivating scenario: You want to see the distribution of a single continuous variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nFamiliarize yourself with making plots using the ggplot2 package.\nUse geom_histogram() to make a histogram.\n\nSpecify the width (with binwidth) or number (with bins) of bins in a histogram.\n\n\nUse geom_density() to make a density plot.\nChange the outline color (with color) and the fill color (with fill).\n\n\n\n\nVisualizing Distributions\nLet’s first consider the distribution of a single continuous variable—pollinator visitation. There are some natural questions we would want answered early on in our analysis:\n\nAre most flowers visited frequently, or do visits tend to be rare?\n\nIs the distribution symmetric, or is it skewed, with many flowers receiving few or no visits and a small number receiving many?\n\nAs we will see throughout the term, understanding the shape of our data helps guide our analysis. Let’s start with two common visualizations for distributions:\n\n\n\n\n\n\n\n\n\n\nA histogram bins the x-axis variable (in this case, visitation) into intervals and shows the number of observations in each bin on the y-axis. This allows us to see how frequently different levels of visitation occur.\n\nA density plot fits a smooth function to the histogram, providing a continuous representation of the distribution. This smoothing can sometimes make patterns easier (or harder) to see. Later, we’ll see that density plots can also help compare distributions.\n\nMaking histograms & density plots: A Step-by-Step Guide. To create these visualizations in ggplot2, we need to (minimally) specify three key layers:\n\nThe data layer: This is the dataset we’re plotting—in this case, ril_data. We pass this as an argument inside the ggplot() function, e.g., ggplot(data = ril_data).\n\nThe aesthetic mapping: This tells R how to map each variable onto the plot. In a histogram, we map the variable whose distribution we want to visualize (in this case, mean_visits) onto the x-axis. We define this inside the aes() argument within ggplot(), e.g., ggplot(data = ril_data, aes(x = mean_visits)).\n\nThe geometric object (geom) that displays the data: In this case, we use geom_histogram() for a histogram or geom_density() for a density plot. These are added to the plot using a +, as shown in the examples below.\n\n\nSet upAdding a geomElaborationsDensity plot\n\n\n\nInside the ggplot() function, we tell R that we are working with the ril_data, and we map mean_visits onto x inside aes(). The result is a boring canvas, but it is the canvas we build on.\n\nggplot(data = ril_data, aes(x = mean_visits))\n\n\n\n\n\n\n\n\n\n\n\n\nBut now we want to show the data. To do so we need to add our geom. In this case we show the data as a histogram.\nThe resulting plot shows that most plants get zero visits, but some get up to five visits.\n\nggplot(ril_data,aes(x = mean_visits))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\nTo take a bit more control of my histogram, I like to\n\nSpecify the number or width of bins (with the binwidth and bins arguments, respectively).\n\nAdd white lines between bins to make them easier to see (with color = \"white\").\n\nSpruce up the bins by specifying the color that fills them (with e.g. fill = \"pink\").\n\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can easily make a density plot if you prefer!\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_density(color = \"white\", fill = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating questions, we see that most plants receive no visits, but this distribution is skewed with some plants getting lots of visits.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. A continuous variable"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "href": "book_sections/intro_to_ggplot/saving_ggplots.html",
    "title": "• 3. Saving a ggplot",
    "section": "",
    "text": "Motivating scenario: You have made a plot and want to save it!\nLearning goals: By the end of this sub-chapter you should be able to\n\nSave a ggplot using either a screen grab, RStudio’s point-and-click options, or the ggsave() function.\n\nKnow the costs and benefits of each approach and when to use which.\n\n\n\n\nSaving Your ggplot.\nYou probably want to save your plot once you’ve made one you like. There are several ways to do this, each with its own pros and cons.\n\n\n\n\n\n\n\n\n\nThe quickest approach is to simply take a screenshot – I do this quite often, and it is great because the plot you save looks exactly like the one on your screen. However, these plots are not vectorized and can lose quality in other ways, so I usually use these as a quick first pass for exploratory data analysis, rather than a refined solution.\nThe next fastest way is to use RStudio’s built-in tools – simply click on the plot in the Plots panel, then use the Export button to copy or save the image. This allows you to choose a file format (like PNG or PDF), set the dimensions, and adjust the resolution. This method is convenient and good for quick outputs, but it’s manual, which means it doesn’t lend itself to reproducible workflows — if you make changes and regenerate your plot later, you’ll have to go through the same export process again.\nFor more control and reproducibility, I suggest using the ggsave() function. This function saves the most recently displayed plot by default, or you can specify a plot object manually. You can choose the file type simply by specifying the file extension (e.g., .png, .pdf, .svg) and control the size and resolution of the output. For example, the code below will save a high-resolution pdf file, called ril_visit_hist.pdf:\n\nggplot(ril_data, aes(x = mean_visits))+\n  geom_histogram(binwidth = .2, color = \"white\", fill = \"pink\")\n\nggsave(\"ril_visit_hist.pdf\", width = 6, height = 4)\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using either ggsave() or RStudio’s built-in tools always check that your saved plot looks as expected, as it is often slightly different than what you saw in your R session.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Saving a ggplot"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/cont_cat.html",
    "href": "book_sections/intro_to_ggplot/cont_cat.html",
    "title": "• 3. Continuous y/categorical x",
    "section": "",
    "text": "Motivating scenario: You want to compare a continuous variable across different levels of a categorical explanatory variable.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUnderstand the challenge of overplotting.\n\nUse geom_point, geom_jitter() and/or geom_boxplot() to visualize the distribution.\n\nUse the size and alpha arguments to adjust the size and transparency of points.\n\n\nCombine geom_jitter() and/or geom_boxplot(), noting that\n\nOrder matters - always plot jittered points on top of (i.e. after) boxplots.\n\nWhen showing boxplots and jittered points, make sure the boxplot does not show outliers, otherwise those points will be shown twice.\n\n\n\n\n\nVisualizing associations between a continuous response and a categorical explanatory variable.\nWe often want to know more than just the distribution of variables, we want to know which, if any, explanatory variables are associated with this variation. So, for example, we may want to know if pollinator visitation differs by location. In this case we map the categorical variable (location) onto the x-axis and the continuous response (visitation) onto the y-axis.\n\n\n\n\n\n\n\n\n\nTo compare visitation by site, we start with this setup:\n\nFilter our data for samples from \"GC\" and \"SR\" (because we did not conduct pollinator observations at the other sites).\n\nPipe this directly into our ggplot() function.\n\nNote: We do not need to specify the data argument when piping data. That is because ggplot inherits data from the pipe.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))\n\nContinuous response to a categorical explanatory variable: A Step-by-Step Guide: In the tabs below I show some options for our geom.\n\npoint()jitter()boxplot() + jitter()\n\n\n\nThe Simplest Plot\n\nUses the geom_point() function to display the data.\n\nWe can add a mean using stat_summary().\n\nNote: I plot the mean in red to make it stand out.\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt;  # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits)) +\n  geom_point(size =2, alpha = 0.5) +\n  stat_summary(size = 1.2, color = \"red\")\n\n\n\n\n\n\n\n\nHowever, these figures can be difficult to interpret when many points overlap, making it hard to distinguish individual data points—this issue is known as over-plotting.\nOne way to address over-plotting is to adjust the transparency of points using the alpha argument. There are several other techniques to handle this, which we’ll explore in the next section.\n\n\n\n\nJitter plots spread out data along the x-axis. This works well with categorical predictors but can be misleading when used with continuous predictors. To improve clarity, I make a few adjustments in the geom_jitter() function:\n\nI set height = 0 to keep the y-values unchanged. I always do this.\n\nI set width = 0.3 to prevent points from overlapping too much between categories. You may experiment with this value to find the best fit for your plot.\n\nI make the points larger (size = 3) and partially transparent (alpha = 0.5) to help visualize overlap more effectively.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)+\n  stat_summary(size = 1.2,color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add multiple geom layers to a plot. For example, we can combine a boxplot (geom_boxplot()) with a jitter plot. However, there are a few important things to keep in mind:\n\nOrder matters. geom_boxplot() + geom_jitter() places points over the boxplot, making the data visible, while geom_jitter() + geom_boxplot() places the boxplot on top, potentially obscuring the points.\n\nHandling outliers. geom_boxplot() automatically displays outliers as individual points, which is useful when we’re not showing the raw data. However, if we add jittered points with geom_jitter(), these outliers will appear twice, potentially misleading us. To avoid this, I set outlier.shape = NA in the boxplot.\n\n\nril_data |&gt; \n  filter(location == \"GC\" | location == \"SR\") |&gt; # Pipe data into ggplot\n  ggplot(aes(x = location, y = mean_visits))+\n  geom_boxplot(outlier.shape = NA) + \n  geom_jitter(height = 0, width = .3, size = 3, alpha = .5)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that plants at the SR population receive way more visits by pollinators than do plants at GC.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Continuous y/categorical x"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_categorical.html",
    "href": "book_sections/intro_to_ggplot/two_categorical.html",
    "title": "• 3. Two categorical variables",
    "section": "",
    "text": "Motivating scenario: You want to explore how two categorical variables are associated.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake barplots with geom_bar() and geom_col().\n\nMake stacked and grouped barplots.\n\nKnow when to use geom_bar() and when to use geom_col().\n\n\n\n\nCategorical explanatory and response variables\nAbove, we saw that most plants received no visits, so we might prefer to compare the proportion of plants that did and did not receive a visit from a pollinator by some explanatory variable (e.g. petal color or location). Recall that we have added the logical variable, visited, by typing mutate(visited = mean_visits &gt; 0).\n\n\n\n\n\n\n\n\n\nMaking bar plots: A Step-by-Step guide. There are two main geoms for making bar plots, depending on the structure of our data:\n\nIf we have raw data (i.e. a huge dataset with values for each observation) use geom_bar().\n\nIf we have aggregated data (i.e. a summary of a huge dataset with counts for each combination of variables) use geom_col()\n\nNote: Here we map petal color onto the x-axis, and visited (TRUE / FALSE) onto the fill aesthetic.\n\nStacked barplotGrouped barplotAggregated data\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\nril_data |&gt; \n    filter(!is.na(petal_color), !is.na(mean_visits))|&gt;\n    mutate(visited = mean_visits &gt;0)|&gt;\n  ggplot(aes(x = petal_color, fill = visited))+\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\nIf you had aggregated data, like that below. We need to plot these data somewhat differently. There are two key differences:\n\nWe map our count (in this case n) onto the y aesthetic.\n\nWe use geom_col() instead of geom_bar().\n\n\n\n\n\n\nlocation\npetal_color\nvisited\nn\n\n\n\n\nGC\npink\nFALSE\n32\n\n\nGC\npink\nTRUE\n23\n\n\nGC\nwhite\nFALSE\n46\n\n\nGC\nwhite\nTRUE\n2\n\n\nSR\npink\nFALSE\n1\n\n\nSR\npink\nTRUE\n56\n\n\nSR\nwhite\nFALSE\n11\n\n\nSR\nwhite\nTRUE\n39\n\n\n\n\n\n\nggplot(data = aggregated_pollinator_obs, \n       aes(x = petal_color, y = n, fill = visited))+\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: We see that a greater proportion of pink-flowered plants receive visits compared to white-flowered plants.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two categorical variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/two_continuous.html",
    "href": "book_sections/intro_to_ggplot/two_continuous.html",
    "title": "• 3. Two continuous variables",
    "section": "",
    "text": "Motivating scenario: You want to visualize the relationship between two continuous variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nMake a scatterplot with geom_point().\n\nShow trends with geom_smooth().\n\nShow a linear regression with method = \"lm\"\n\nShow a moving average with method = \"loess\"\nOptionally suppress the grey ribbon by setting se = FALSE.\n\nTransform the scale of x or y axis with e.g. scale_x_continuous(trans = \"log1p\").\n\n\n\n\nVisualizing associations between continuous variables.\nWhen Brooke watched pollinators visit parviflora recombinant inbred lines (RILs), she was hoping that these observations also informed the probability of hybrid seed set. The first step in evaluating this hypothesis is to generate a scatterplot – a visualization that shows the association between continuous variables.\n\n\n\n\n\n\n\n\n\nWe use geom_point() to make such a plot and add trends with geom_smooth(). There are numerous types of trendlines we could add with the method argument:\n\nTo add the best fit linear model type method = lm.\n\nTo add a smoothed moving average type method = loess.\n\nBy default ggplot adds uncertainty about its guess of the line with a grey background. This is sometimes helpful but can get in the way. To remove it type se = FALSE.\nAt times transforming the data makes patterns easier to see. We can transform our presentation of the data with the trans argument in the scale_x_continuous() (or scale_y_continuous()) functions.\n\npoint()transformlinear trendsmoothed average\n\n\n\nWe present the data with geom_point(). Because some data points overlapped\n\nI increased the point size (size = 3) and\n\nI made the points partially transparent (alpha = 0.4).\n\nA small jitter could have been ok, but jittering continuous values gives me the ick, because I want my data presented faithfully.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes nonlinear scales better reveal trends. Transforming the scale on which the data are presented (rather than transforming the data itself) is nice because we retain original values.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\nTake care not to lose data when transforming: The log of any number less than or equal to zero is undefined. To avoid losing these data points, I transformed the data as log(x+1) with the \"log1p\" transformation. If the data were all greater than one, I could have used the log log10 or sqrt transform.\n\n\n\n\n\nThe geom_smooth() allows us to highlight patterns in our data. There are lots of ways to draw trends through data, and we can specify how we want to do so with the method argument.\nHere I present the standard “best-fit” line with method = \"lm\". The grey area around that line represents plausible lines that would also have been statistically acceptable (more on that later).\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes a simple line does not do a great job of highlighting patterns. Specifying method = loess presents a smoothed moving average.\nIn addition to showing that moving average, I show you how to suppress that grey area with se = FALSE. I tend to like to include the uncertainty in our estimated trend so I usually don’t do this, but sometimes showing the uncertainty hides other features of the data, so I wanted to empower you.\n\nggplot(ril_data, aes(x = mean_visits, y  = prop_hybrid))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"loess\")+\n  scale_x_continuous(trans = \"log1p\")\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to our motivating question, we see that the proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Two continuous variables"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/many_explanatory.html",
    "href": "book_sections/intro_to_ggplot/many_explanatory.html",
    "title": "• 3. Many explanatory vars",
    "section": "",
    "text": "Motivating scenario: You want to visualize associations between a response variable and numerous explanatory variables.\nLearning goals: By the end of this sub-chapter you should be able to\n\nUse the color or fill aesthetics to map categorical variables.\n\nUse “small multiples” with facet_wrap() and facet_grid()\n\n\n\n\nThe challenge of many explanatory variables\nThere is usually more than one thing going on in a scientific study, and we may want to see how different combinations of explanatory variables are associated with a response. This can be a challenge over the term, you will encounter many tips and tricks to help in this task. For now we will look at two useful approaches:\n\nMapping different explanatory variables to different aesthetics.\n\nThe use of “small multiples”.\n\n\n\nMultiple aesthetic mappings\nWe saw that the number of pollinators increased with petal size and was greater for pink than white flowers. However, visualizing a response variable as a function of its multiple explanatory variables together can help us home in on the patterns.\n\nTo do so we can make a scatterplot and map petal color onto the color aesthetic.\n\nWe can even use size and color as extra aesthetics to map onto. I show you how to do this, but use it sparingly, because too many extra aesthetics may provide more distraction than insight.\n\n\n\n\n\n\n\n\n\n\n\nTwo explanatory variablesThree explanatory variables\n\n\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nHere I added anther stigma distance as another explanatory variable by mapping it onto size. Note that:\n\nI removed the size = 3 argument from geom_point() otherwise it would not map asd onto size.\n\nIn this case, it did not reveal much and was probably not worth doing.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = mean_visits, \n             color  = petal_color))+\n  geom_point(aes(size = asd_mm), alpha = .4)+\n  geom_smooth(method = \"lm\",show.legend = FALSE, se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Returning to the motivating question, we see that visitation increases with both petal area (positive slope) and pink petals. The proportion of seeds that are hybrids appears to increase with pollinator visitation. Later in the term we will address this question more rigorously.\n\n\nSmall multiples\n\n\n\n\n\n\n\n\nFigure 1: Using small multiples to show the lunar phase moon over a month. From this link\n\n\n\n\n\n\n\nAt the heart of quantitative reasoning is a single question: Compared to what? Small multiple designs, multivariate and data bountiful, answer directly by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives. For a wide range of problems in data presentation, small multiples are the best design solution. \n\nEdward Tufte (Tufte, 1990)\n\nEdward Tufte, a major figure in the field of data visualization - popularized the concept of “small multiples” – showing data with the same structure across various comparisons. He argued that such visualizations can help our eyes make powerful comparisons (See quote above). For example, the lunar phase can be well visualized by using small multiples (Figure 1).\nFor our analyses, we may find that adding location as a variable helps us better see and understand patterns. Additionally, small multiples are sometimes preferable to mapping different categorical variables onto different aesthetics. I show these examples below:\n\nOne explanatory “facet”Two explanatory “facets”\n\n\n\nWe can add a small multiple with a “facet”. For a single categorical variable, use the facet_wrap() function. Here are some notes and options:\n\nThe first argument is ~ &lt;Thing to facet by&gt;, e.g. ~ location.\n\nYou can specify the number of rows or columns with nrow or ncol arguments.\n\nThe labeller = \"label_both\" shows the variable name in addition to its value.\n\nOccasionally, you may want different facets on different scales. You can use the scales argument with options, free_x, free_y, and free.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid, \n             color  = petal_color))+\n  geom_point(size = 3, alpha = .4)+\n  facet_wrap(~ location, nrow = 1, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nFor two categorical variables, use the facet_grid() function. Here are some additional notes and options:\n\nThe first argument is &lt;Row to facet by&gt; ~ &lt;Column to facet by&gt;, e.g. petal_color ~ location.\n\nYou can no longer specify the number of rows or columns.\n\nI have shown the same data as the previous plot in a different way. But you can see that this adds room for mapping another variable.\n\n\nril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. Many explanatory vars"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html",
    "title": "• 3. ggplot summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources + BONUS CONTENT: ggplotly.\nEffective data visualization begins with curiosity and clear biological questions. Before writing a single line of code, it’s worth thinking about what you want to learn from your data and how best to visualize that information. Once we have developed some starting ideas and sketches, we are ready to use ggplot2’s flexible framework to bring these ideas to life. Data visualization with ggplot2 is built around the idea that plots are constructed by layering components: you begin by mapping variables to aesthetic properties like position, color, or size, and then choose how to display those mapped variables using geometric elements like histograms, barplots, or points. With this approach you can iteratively build visualizations that reflect your questions and highlight meaningful patterns.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "href": "book_sections/intro_to_ggplot/ggplot_summary.html#ggplot_summaryGgploty",
    "title": "• 3. ggplot summary",
    "section": "Bonus content: Interactive plots with ggplotly",
    "text": "Bonus content: Interactive plots with ggplotly\nOften data have strange outliers, or patterns you think you see but aren’t sure about, or simply interesting data points. When I run into these issues during exploratory data analysis I often want to know more about individual data points. To do so, I make interactive graphs with the ggplotly() function in the plotly package.\nThe example below shows how to do this. Note that you can make up random aesthetics that you never use and they show up when you hover over points – this helps with understanding outliers. You can also zoom in!\n\nlibrary(plotly)\nbig_plot &lt;- ril_data |&gt;\n  filter(!is.na(petal_color))|&gt;\n  ggplot(aes(x = petal_area_mm, \n             y = prop_hybrid,\n             ril = ril,\n             mean_visits =  mean_visits))+ \n  geom_point(size = 3, alpha = .4)+\n  facet_grid(petal_color ~ location, labeller = \"label_both\")+\n  geom_smooth(method = \"lm\", se = FALSE)\n\nggplotly(big_plot)\n\n\n\n\n\n\n\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.",
    "crumbs": [
      "3. Introduction to ggplot",
      "• 3. ggplot summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html",
    "href": "book_sections/reproducible_science.html",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Making science reproducible\nThis section includes background on:\nIn my roles as a biostatistics professor and Data Editor at The American Naturalist, I have found that the greatest beneficiary of reproducible research is often the lead author themselves. In this chapter, we will work through the process of creating reproducible research—from collecting data in the field to writing and sharing R scripts that document your analyses.\nThis chapter walks you through the key steps for making your science reproducible—from field notes to final scripts. You will learn how to:\nThen we summarize the chapter, present a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science.html#making-science-reproducible",
    "href": "book_sections/reproducible_science.html#making-science-reproducible",
    "title": "4. Reproducible Science",
    "section": "",
    "text": "Appropriately collect and store data, including Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\n\nDevelop reproducible computational strategies, including: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).",
    "crumbs": [
      "4. Reproducible Science"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html",
    "href": "book_sections/reproducible_science/collecting_data.html",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Making rules for data collection\nThis section includes background on: Making rules for data collection, Making a spreadsheet for data entry, Making data (field) sheets, A checklist for data collection, Making a README and/or data dictionary, and Long term public data storage.\nSometimes, data come from running a sample through a machine, resulting in a computer readout of our data (e.g., genotypes, gene expression, mass spectrometer, spectrophotometer, etc.). Other times, we collect data through counting, observing, and similar methods, and we enter our data by hand. Either way, there is likely some key data or metadata for which we are responsible (e.g., information about our sample). Below,we focus on how we collect and store our data, assuming that our study and sampling scheme have already been designed. Issues in study design and sampling will be discussed in later chapters.\nBefore we collect data, we need to decide what data to collect. Even in a well-designed study where we want to observe flower color at a field site, we must consider, “What are my options?”, for example:\nSome of these questions have answers that are more correct, while for others, it’s crucial to agree on a consistent approach and ensure each team member uses the same method.\nSimilarly, it’s important to have consistent and unique names – for example, my collaborators and I study Clarkia plants at numerous sites – include Squirrel Mountain and SawMill – which one should we call SM (there is a similar issue in state abbreviation – we live in MiNnesota (MN), not MInnesota, to differentiate our state from MIchigan (MI)). These questions highlight the need for thoughtful data planning, ensuring that every variable is measured consistently.\nOnce you have figured out what you want to measure, how you will measure and report it, and other useful info (e.g., date, time, collector), you are ready to make a data sheet for field collection, a database for analysis, and a README or data dictionary.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "href": "book_sections/reproducible_science/collecting_data.html#making-rules-for-data-collection",
    "title": "• 4. Collecting data",
    "section": "",
    "text": "Should the options be “white” and “pink,” or should we distinguish between darker and lighter shades?\n\nIf we’re measuring tree diameter, at what height on the tree should we measure the diameter?\n\nWhen measuring something quantitative, what units are we reporting?",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "href": "book_sections/reproducible_science/collecting_data.html#making-a-spreadsheet-for-data-entry",
    "title": "• 4. Collecting data",
    "section": "Making a spreadsheet for data entry",
    "text": "Making a spreadsheet for data entry\nAfter deciding on consistent rules for data collection, we need a standardized format to enter the data. This is especially important when numerous people and/or teams are collecting data at different sites or years. Spreadsheets for data entry should be structured similarly to field sheets (described below) so that it is easy to enter data without needing too much thought.\nSome information (e.g., year, person, field site) might be the same for an entire field sheet, so it can either be written once at the top or explicitly added to each observation in the spreadsheet (copy-pasting can help here).",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "href": "book_sections/reproducible_science/collecting_data.html#making-data-field-sheets",
    "title": "• 4. Collecting data",
    "section": "Making Data (field) sheets",
    "text": "Making Data (field) sheets\nWhen collecting data or samples, you need a well-designed data (field) sheet. The field sheet should closely resemble your spreadsheet but with a few extra considerations for readability and usability. Ask yourself: How easy is this to read? How easy is it to fill out? Each column and row should be clearly defined so it’s obvious what goes where. Consider the physical layout too—does the sheet fit on one piece of paper? Should it be printed in landscape or portrait orientation? Print a draft to see if there’s enough space for writing. A “notes” column can be useful but should remain empty for most entries, used only for unusual or exceptional cases that might need attention during analysis.\nIt’s also smart to include metadata at the top—things like the date, location, and who collected the data. Whether this metadata gets its own column or is written at the top depends on practical needs—if one person is responsible for an entire sheet, maybe it belongs at the top, not repeated for every sample (contrary to the example in Figure 2).\n\n\n\n\n\n\n\n\nFigure 2: Example field data sheet designed for recording ecological and biological observations. The form includes sections for site information, environmental conditions, personnel details, and a structured table for recording species, individual IDs, sex, location descriptions, time, GPS coordinates, photos, collector name, and additional notes.\n\n\n\n\n\n\nData collection and data entry checklist:\n\nBe consistent and deliberate: You should refer to a thing in the same thoughtful way throughout a column. Take, for example, gender as a nominal variable. Data validation approaches, above, can help.\n\n\n\nA bad organization would be: male, female, Male, M, Non-binary, Female.\nA good organization would be: male, female, male, male, non-binary, female.\nA just as good organization would be: Male, Female, Male, Male, Non-binary, Female.\n\nUse good names for things: Names should be concise and descriptive. They need not tell the entire story. For example, units are better kept in a data dictionary (Figure 4) than column name. This makes downstream analyses easier. Avoiding spaces and special characters makes column names easier to work with in R.\n\n\n\n\n\n\n\n\n\n\nFigure 3: A comparison of bad and good variable naming conventions in a dataset. The top section (labeled “Bad” in red) displays poor naming choices, including spaces, special characters, unclear abbreviations, and inconsistent formatting. The bottom section (labeled “Good” in blue) demonstrates improved naming conventions with consistent formatting, clear descriptions, and no special characters.\n\n\n\n\n\nSave in a good place. Make a folder for your project. Keep your data sheet (and data dictionary) there. Try to keep all that you need for the project in this folder, but try not to let it get too complex. Some people like to add more order with additional subfolders for larger projects.  \nSave early and often: You are welcome.  \nBackup your data, do not touch the original data file and do not perform calculations on it. In addition to scans of your data (if collected on paper) also save your data on both your computer and a locked, un-editable location on the cloud (e.g. google docs, dropbox, etc.). When you want to do something to your data do it in R, and keep the code that did it. This will ensure that you know every step of data manipulation, QC etc.  \nDo Not Use Font Color or Highlighting as Data. You may be tempted to encode information with bolded text, highlighting, or text color. Don’t do this! Be sure that all information is in a readable column. These extra markings will either be lost or add an extra degree of difficulty to your analysis. Reserve such markings for the presentation of data. \nNo values should be implied Never leave an entry blank as “shorthand for same as above”. Similarly, never denote the first replicate (or treatment, or whatever) by its order in a spreadsheet, but rather make this explicit with a value in a column. Data order could get jumbled and besides it would take quite a bit of effort to go from implied order to a statistical analysis. It is ok to skip this while taking data in the field, but make sure no values are implied when entering data into a computer. \nData should be tidy (aka rectangular): Ideally data should be entered as tidy (Each variable must have its own column. Each observation must have its own row. Each value must have its own cell). However, you must balance two practice considerations – “What is the best way to collect and enter data?” vs “What is the easiest way to analyze data?” So consider the future computational pain of tidying untidy data when ultimately deciding on the best way to format your spreadsheet (above).  \nUse data validation approaches: When making a spreadsheet, think about future-you (or your collaborator) who will analyze the data. Typos and inconsistencies in values (e.g., “Male,” “male,” and “M” for “male”) create unnecessary headaches. Accidentally inputting the wrong measure (e.g., putting height where weight should be, or reporting values in kg rather than lb) sucks.  Both Excel and Google Sheets (and likely most spreadsheet programs) have a simple solution: use the “Data validation” feature in the “Data” drop-down menu to limit the potential categorical values or set a range for continuous values that users can enter into your spreadsheet. This helps ensure the data are correct.\n\n\n\nMaking a README and/or “data dictionary”\nA data dictionary is a separate file (or sheet within a file, if you prefer) that explains all the variables in your dataset. It should include the exact variable names from the data file, a version of the variable name that you might use in visualizations, and a longer explanation of what each variable means. Additionally, it is important to list the measurement units and the expected minimum and maximum values for each variable, so anyone using the data knows what to expect.\nAlongside your data dictionary, you should also create a README file that provides an overview of the project and dataset, explaining the purpose of the study and how the data were collected. The README should include a description of each row and column in the dataset. While there may be some overlap with the data dictionary, this is fine. The data dictionary can serve as a quick reference for plotting and performing quality control, whereas the README provides a higher-level summary, designed for those who may not have been directly involved in the project but are seriously interested in the data.\n\n\n\n\n\n\n\n\nFigure 4: Example data dictionary from (Broman & Woo, 2018). This table provides structured metadata for a dataset, including variable names, descriptions, and group classifications. It ensures clarity in data documentation by defining each column’s purpose and expected values. See the Clarkia RIL data dictionary here for another example.\n\n\n\n\n\n\n\nLong-term public data storage\nA key to the scientific method is reproducibility. This is why scientific papers have a methods section. Nowadays - the internet allows for even more detailed sharing of methods. Additionally it is the expectation in most fields that data is made available after publication on repositories like data DRYAD or DRUM.\n\n\n\n\nBabbage, C. (1864). Passages from the life of a philosopher. Longman; Co.\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Collecting data"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html",
    "href": "book_sections/reproducible_science/reproducible_analyses.html",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "R Projects, Storing, and Loading data\nThis section includes background on: Making an R project, Loading data into R, Writing and saving R scripts (with comments), Cleaning and tidying data, and finally a reproducible code checklist (modified from The American Naturalist).\nIn addition to the high-minded values of transparency and sharing scientific progress, there is a pragmatic and selfish reason to make you work reproducible.\nThis humorous observation highlights a very real challenge. Research is complex and can stretch over months or even years. Well-documented data and code, allows you to retrace your steps, understand your past analyses, and explain your results. By keeping a good record of your workflows, you make your future self’s life much easier.\nFigure 1: A screenshot of the files pane of RStudio. This displayes the folder containing my RProject, my data, and data dictionary. If you do this, there is no need to use the setwd() function or to use a long, “absolute path” to the data.\nI have previously introduced an easy way to load data from the internet into R, but often data is on our computer, not the internet. Loading data from your computer to R can be a difficult challenge, but if you follow the guide below you will see that it can be very easy.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-projects-storing-and-loading-data",
    "title": "• 4. Reproducible analyses",
    "section": "",
    "text": "Making an R project\nThe first step towards reproducible analyses is making an R project. I do this by clicking “File” &gt; “New Project” and I usually use an existing directory (Navigating to the folder with my data and data dictionary that we just made).\n\nNow every time you use R to work on these data, open R by double clicking on this project (or if R is already open, navigate to “Open Project”).\n\n\n\nLoading data\nNow if your project and data are in the same folder, loading the data is super easy. Again just used the read_csv() function in the readr package with an argument of the filename (in quotes) to read in the data!\n\n\nSome people like a different structure of files in their R project – they like the project in the main folder and then a bunch of sub-folders like data and scripts and figures. If you are using this organization, just add the subfolder in the path (e.g. read_csv(\"data/my_data.csv\") ).\n\nlibrary(readr)\nril_data &lt;- read_csv(\"clarkia_rils.csv\")\n\n\nRead excel files into R with the read_xlsx() from the readxl package). You can even specify the sheet as an argument!",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#r-scripts-commenting-code-cleaning-tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "R Scripts, Commenting Code, + Cleaning & Tidying data",
    "text": "R Scripts, Commenting Code, + Cleaning & Tidying data\nWe have just run a few lines of code – loading the readr library and assigning the data in clarkia_rils.csv to the variable, ril_data.\nWe will likely want to remember that we have done this, and build off it in the future. To do so let’s open up an R Script – by navigating to File, New File, R Script.\nA new file, likely called Untitled 1 will show up in the R Scripts pane of your R Studio session. After giving it a more descriptive name, be sure save all of the code you are using for these analyses in this file.\nHere I introduce the best practices for writing an R script (Part I). I revisit these in greater depth later in the chapter.\n\nFirst include your name, and the goal of the script as comments on the opening lines.\n\nNext load all packages you will use. Don’t worry, you can go back and add more later.\n\nDo not include install.packages() in your script: You only want to do this once, so it should not be part of a reproducible pipeline. People can see they need to install the package from you loading it.\n\n\nNext load all the data you will use.\n\nThen get started coding.\n\nBe sure to comment your code with the #.\n\n\nCleaning data\nWe previously said not to make column names too long, yet in our dataset, we have the variable petal_area_mm. We can use the rename() function in the dplyr package to give columns better names.\nI do this below in my complete and well-commented R script, which I conclude with a small plot. Notice that anyone could use this script and get exactly what I got.\n\n\n\n\n\n\n\n\n\n\n# Yaniv Brandvain. March 07 2025\n# Code to load RIL data, and make a histogram of petal area facetted by petal color\n\nlibrary(readr)                                      # Load the readr package to load our data\nlibrary(ggplot2)                                    # Load the ggplot2 package to make graphs\n\nril_data &lt;- read_csv(\"clarkia_rils.csv\") |&gt;         # load the RIL data\n    filter(!is.na(petal_color))          |&gt;         # remove missing petal color data\n  rename(petal_area = petal_area_mm)                # rename petal_area_mm as petal_area\n\nggplot(ril_data, aes(x = petal_area))+              # set up a plot with petal_area on x\n  geom_histogram(binwidth = 10, color = \"white\")+   # make it a histogram, with a bins of width 10 and separated by white lines \n  facet_wrap(~petal_color, ncol = 1)                # wrap by petal color, with one column\n\n\nIf your data set has many column names that are difficult to use in R, the clean_names() function in the janitor package fixes a bunch of them at once.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#tidying-data",
    "title": "• 4. Reproducible analyses",
    "section": "Tidying data",
    "text": "Tidying data\nWe have previously seen that there are many ways for data to be untidy. One common untidy data format is the “wide format”, below. Here, the columns SR and GC describe the proportion of hybrid seed on each ril (row) observed in each of these two locations. You can use the pivot_longer() function in the tidyr package, to tidy such data:\n\n\nCode to create untidy_ril\nlibrary(tidyr)\nlibrary(dplyr)\nuntidy_ril &lt;- ril_data |&gt; \n  select(ril, location,prop_hybrid)|&gt;\n  filter(location %in% c(\"GC\",\"SR\"))|&gt; \n  pivot_wider(id_cols = \"ril\",\n              names_from = location,\n              values_from = prop_hybrid)\n\nuntidy_ril|&gt;head(n = 5)|&gt; kable()\n\n\n\n\n\nril\nGC\nSR\n\n\n\n\nA1\n0.000\n0.00\n\n\nA100\n0.125\n0.25\n\n\nA102\n0.250\n0.00\n\n\nA104\n0.000\n0.00\n\n\nA106\n0.000\n0.00\n\n\n\n\n\n\nlibrary(tidyr)\npivot_longer(untidy_ril, \n             cols = c(\"GC\",\"SR\"),\n             names_to = \"location\", \n             values_to = \"prop_hybrid\")|&gt;\n  head(n = 10)|&gt;\n  kable()\n\n\n\n\nril\nlocation\nprop_hybrid\n\n\n\n\nA1\nGC\n0.000\n\n\nA1\nSR\n0.000\n\n\nA100\nGC\n0.125\n\n\nA100\nSR\n0.250\n\n\nA102\nGC\n0.250\n\n\nA102\nSR\n0.000\n\n\nA104\nGC\n0.000\n\n\nA104\nSR\n0.000\n\n\nA106\nGC\n0.000\n\n\nA106\nSR\n0.000",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "href": "book_sections/reproducible_science/reproducible_analyses.html#reproducible-code-a-checklist-from-amnat",
    "title": "• 4. Reproducible analyses",
    "section": "Reproducible Code: A Checklist From AmNat",
    "text": "Reproducible Code: A Checklist From AmNat\nWhat you do to data and how you analyze it is as much a part of science as how you collect it. As such, it is essential to make sure your code:\n\nReliably works – even on other computers\n\nAnd can be understood.\n\nThe principles from The American Naturalist’s policy about this are pasted in the box below:\n\nREQUIRED:\n\nScripts should start by loading required packages, then importing raw data from files archived in your data repository.\nUse relative paths to files and folders (e.g. avoid setwd() with an absolute path in R), so other users can replicate your data input steps on their own computers.\nMake sure your code works. Shut down your R. (or type rm(list=ls()) into the console and run the code again. You should get the same results. If not, go back and fix your mistakes.\nAnnotate your code with comments indicating what the purpose of each set of commands is (i.e., “why?”). If the functioning of the code (i.e., “how”) is unclear, strongly consider re-writing it to be clearer/simpler. In-line comments can provide specific details about a particular command.\n\nNote that ChatGPT is very good at commenting your code.\n\nAnnotate code to indicate how commands correspond to figure numbers, table numbers, or subheadings of results within the manuscript.\nIf you are adapting other researcher’s published code for your own purposes, acknowledge and cite the sources you are using. Likewise, cite the authors of packages that you use in your published article.\n\nRECOMMENDED:\n\nTest code ideally on a pristine machine without any packages installed, but at least using a new session.\nUse informative names for input files, variables, and functions (and describe them in the README file).\nAny data manipulations (merging, sorting, transforming, filtering) should be done in your script, for fully transparent documentation of any changes to the data.\nOrganize your code by splitting it into logical sections, such as importing and cleaning data, transformations, analysis and graphics and tables. Sections can be separate script files run in order (as explained in your README) or blocks of code within one script that are separated by clear breaks (e.g., comment lines, #————–), or a series of function calls (which can facilitate reuse of code).\nLabel code sections with headers that match the figure number, table number, or text subheading of the paper.\nOmit extraneous code not used for generating the results of your publication, or place any such code in a Coda at the end of your script.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducible analyses"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html",
    "href": "book_sections/reproducible_science/reproducible_summary.html",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary, Chatbot Tutor, Practice Questions, Glossary, R functions and R packages, and Additional resources.\nReproducibility is a cornerstone of good science, ensuring that research is transparent, reliable, and easy to build upon. This chapter covered best practices for collecting, organizing, and analyzing data in a reproducible manner.\nBefore collecting data, establish clear rules for measurement, naming conventions, and data entry to maintain consistency. Field sheets should be well-structured, and tidy. Additionally, creating a data dictionary and README document ensures that variables and project details are well-documented. Finally, storing data and scripts in public repositories supports transparency and open science.\nIn analysis, using an R Project helps keep files organized, and loading data with relative paths avoids location issues. Writing well-structured R scripts with clear comments makes workflows understandable and repeatable. By prioritizing reproducibility you not only strengthen the integrity of your work, but also make future analyses smoother for yourself and others.",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "href": "book_sections/reproducible_science/reproducible_summary.html#reproducible_summarySummary",
    "title": "• 4. Reproducibility summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.\n\n\n\nPractice Questions\nTry these questions!\n\nQ1) What is the biggest mistake in the table below?\n\n ID should be lower case Its perfect, change nothing the column name, weight is not sufficiently descriptive, it should include the units. date_colleted_empty_means_same_as_above is too wordy, replace with date Values for date_collected_empty_means_same_as_above are implied. date is in Year-Month-Day format, while Month-Day-Year format is preffered.\n\n\n\n\n\n\nID\nweight\ndate_collected_empty_means_same_as_above\n\n\n\n\n1-A1\n104\n2024-03-01\n\n\n1-1B\n210\n\n\n\n3-7\n150\n\n\n\n2-B\n176\n2024-03-15\n\n\n1-A5\n110\n\n\n\n\n\n\n\n\nClick here for explanation\n\nWhile some of these (like the long name for date) are clearly shortcomings, spreadsheets should never leave values implied.\n\n.\n\nQ2) What would you expect in a data dictionary accompanying the table above? (select all correct)\n\n The units for weight. A statement that date is in Year-Month-Day format A statement explaining that in the date colleted column, empty means same as above.\n\n\nQ3) How do you read data from a Excel sheet, called raw_data in an Excel filed named bird_data.xlsx located inside the R project you are working in?\n\n You cannot load excel files into R. You must save it as a csv, and read it in with read_csv(). Assuming the readxl package is installed and loaded, type read_xlsx(file = “bird_data.xlsx”, sheet = “raw_data”). While you can read excel into R, you cannot specify the sheet.\n\n\nQ4) What should you do to make code reproducible? (pick the best answer)\n\n Specify the working directory with setwd() Show the packages installed with install.packages() Restart R once your done, and rerun your script to see if it works\n\n\n\n\n\nGlossary of Terms\n\nAbsolute Path – A file location specified from the root directory (e.g., /Users/username/Documents/data.csv), which can cause issues when sharing code across different computers. Using relative paths instead is recommended.\nData Dictionary – A structured document that defines each variable in a dataset, including its name, description, units, and expected values. It helps ensure data clarity and consistency.\nData Validation – A method for reducing errors in data entry by restricting input values (e.g., dropdown lists for categorical variables, ranges for numerical values).\nField Sheet – A structured data collection form used in the field or lab, designed for clarity and ease of data entry.\nMetadata – Additional information describing a dataset, such as when, where, and how data were collected, the units of measurement, and details about the variables.\nR Project – A self-contained environment in RStudio that organizes files, code, and data in a structured way, making analysis more reproducible.\nRaw Data – The original, unmodified data collected from an experiment or survey. It should always be preserved in its original form, with any modifications performed in separate scripts.\nREADME File – A text file that provides an overview of a dataset, including project details, data sources, file descriptions, and instructions for use.\nReproducibility – The ability to re-run an analysis and obtain the same results using the same data and code. This requires careful documentation, structured data storage, and clear coding practices.\nRelative Path – A file path that specifies a location relative to the current working directory (e.g., data/my_file.csv), making it easier to share and reproduce analyses.\nTidy Data – A dataset format where each variable has its own column, each observation has its own row, and each value is in its own cell.\n\n\n\n\nKey R functions\n\n\nclean_names(data) – Standardizes column names (from the janitor package).\ndrop_na(data) – Removes rows with missing values (from the tidyr package)).\nread_csv(\"file.csv\") – Reads a CSV file into R as a tibble (from the readr package).\nread_xlsx(\"file.xlsx\", sheet = \"sheetname\") – Reads an excel sheet into R as a tibble (from the readxl package).\nrename(data, new_name = old_name) – Renames columns in a dataset (from the dplyr package).\npivot_longer(data, cols, names_to, values_to) – Converts wide-format data to long format (from the tidyr package).\nsessionInfo() – Displays session details, including loaded packages (useful for reproducibility).\n\n\n\n\n\nR Packages Introduced\n\n\nreadr – Provides fast and flexible functions for reading tabular data (here we revisited read_csv() for CSV files).\ndplyr – A grammar for data manipulation. Here we introduced the rename(data, new_name = old_name) function to give columns better names.\ntidyr – Helps tidy messy data. Here we introduced pivot_longer() to make wide data long.\njanitor – Cleans and standardizes data, including clean_names()](https://sfirke.github.io/janitor/reference/clean_names.html) for formatting column names.\n\n\n\n\nAdditional resources\n\nR Recipes:\n\nRead a .csv: Learn how to read a csv into R as a tibble.\n\nRead an Excel file: Learn how to read an excel file into R as a tibble.\n\nObey R’s naming rules: You want to give a valid name to an object in R.\n\nRename columns in a table: You want to rename one or more columns in a data frame.\n\nOther web resources:\n\nData Organization in Spreadsheets (Broman & Woo, 2018).\nTidy Data: (Wickham, 2014).\nTen Simple Rules for Reproducible Computational Research: (Sandve, 2013).\nNYT article: For big data scientists hurdle to insights is janitor work.\nStyle guide: Chapter 9 of Data management in large-scale education research by Lewis (2024). Includes sections on general good practices, file naming, and variable naming.\nData Storage and security: Chapter 13 of Data management in large-scale education research by Lewis (2024).\n\nVideos:\n\nData integrity: (By Kate Laskowski who was the victim of data fabrication by her collaborator (and my former roommate) Jonathan Pruitt).\nTidying data with pivor_longer (From Stat454)\n\n\n\n\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989\n\n\nLewis, C. (2024). Data management in large-scale education research. CRC Press.\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules for reproducible computational research. PLOS Computational Biology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nWickham, H. (2014). Tidy data. Journal of Statistical Software, Articles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "4. Reproducible Science",
      "• 4. Reproducibility summary"
    ]
  },
  {
    "objectID": "book_sections/summarizing_data_index.html",
    "href": "book_sections/summarizing_data_index.html",
    "title": "Section II: Summarizing data",
    "section": "",
    "text": "The major goals of statistics are to: (1) Summarize data, (2) Estimate uncertainty, (3) Test hypotheses, (4) Infer cause, and (5) Build models. Now that we can do things in R, we are ready to begin our journey through these goals. In this section, we focus on the first goal—Summarizing data.\n\n\nIt is somewhat weird to start with summarizing data without also describing uncertainty because, in the real world, data summaries should always be accompanied by some estimate of uncertainty. However, biting off both of these challenges at once is too much, so for now, as we move forward in summarizing data, remember that this is just the beginning and is inadequate on its own.\nEven though we aren’t tackling uncertainty yet, summarizing data on its own is already incredibly useful. Understanding and interpreting summaries helps us find patterns, spot errors, and build towards deeper statistical analysis.\nWhy summarize data?\nSummarizing data serves several purposes:\n\nCondensing large datasets: Raw data often contains thousands of observations—summaries make the patterns manageable.\n\nIdentifying trends and variability: Are the values clustered? Is there a lot of spread?\n\nDetecting errors: Unexpected values often signal data entry issues or outliers.\n\nComparing groups: Does one group tend to have larger values than another?\n\nLaying the foundation for modeling: Before building models, we need a clear picture of the data’s structure.\n\n\n\n\n\n\n\n\n\nFigure 1: A pretty scene of Clarkia’s home showing the world we get to summarize.\n\n\n\n\n\nIn this section, we’ll not only learn how to compute summaries but also how to think about them in a meaningful way. That means:\n\nGet everyone up to speed with standard data summaries:\n\nChapter Five introduces univariate summaries of Shape, and how to change the shape of our data! Central tendency (e.g. mean, median, mode), and Variability (e.g. sum of squares, standard deviation, and variance), and more!\nChapter 6 summarizes associations between variables. This includes: conditional means, conditional proportions, covariance and correlation. Students will also get a sense of the difference between parametric and non-parametric summaries. For each summary, we aim to (1) build intuition, (2) provide the mathematical nuts and bolts, and (3) show you how to find it in R.\nIn Chapter 7, we build from these simple summaries to linear models that allow us to predict the value of a response variable from a (combination) of explanatory variable(s). Later in the book we will dig much more deeply into linear models.\n\nIntroduce multivariate summaries and dimension reduction techniques: Summarizing data in one or two dimensions is relatively straightforward. However, nowadays, it is pretty standard to collect high-dimensional data, and we want to be able to work with such complex data. Chapter 8 introduces a few techniques to tackle this real-world challenge.\nFurther your data visualization and interpretation skills: Visual summaries of data are perhaps the most valuable summaries because, when done well, they tell a clear and honest story of the data. We introduced ggplot previously, but that was the minimum to get up and running. Chapter 9 shows what goes into making a good plot, and Chapter 10 shows how to make good plots with ggplot!",
    "crumbs": [
      "Section II: Summarizing data"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html",
    "href": "book_sections/univariate_summaries.html",
    "title": "5. Simple Summaries",
    "section": "",
    "text": "Histograms\nWe can similarly display discrete numerical variables as a bar plot. So, for example, the x-axis of Figure 2 (in the Num hybrid bar plot tab) shows the number of a mother’s seeds that we found to be hybrids, and the y-axis shows the number of mothers with x seeds shown to be hybrid.\nUnfortunately, Figure 2 is somewhat misleading – we aimed to genotype eight seeds per plant, but we sometimes missed this goal. Figure 3 (in the Prop hybrid bar plot tab) is more honest, as it shows the proportion, but it is a bit distracting and confusing – there are occasional weird dips which represent – not a biological fact of parviflora hybridization, but experimental weirdness of a non-standard sample size across mothers.\nFigure 4 (in the Prop hybrid hist tab) is a histogram that displays the distribution of the proportion of hybrid seeds per RIL. A histogram is much like a bar plot, but rather than referring to a single value of the x-variable, in a histogram, values of x are binned.\nIt helps to remember that in contrast to a bar plot, where the value on the x axis is the value of all the observations in that bar, in a histogram this value is the center of the range of values in the bin. So in Figure 4 the first bin corresponds to values between -0.0625 and 0.0625, and the second bin (centered on 1/8th) corresponds to proportions between 0.0625 and 0.1875, etc… . The x-axis of Figure 5 shows this more explicitly, but it is too busy, and complex for standard presentation.\nFigure 5: A histogram showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom. This is identical to Figure Figure 4, except, the x-label shows the range of x values rather than the center of thebins.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html#histograms",
    "href": "book_sections/univariate_summaries.html#histograms",
    "title": "5. Simple Summaries",
    "section": "",
    "text": "Num hybrid bar plotProp hybrid bar plotProp hybrid hist\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = num_hybrid))+\n  geom_bar()+\n  scale_x_continuous(breaks = 0:8)\n\n\n\n\n\n\n\nFigure 2: A bar plot showing the number of genotyped seeds of each mom shown to be hybrid.\n\n\n\n\n\n\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = prop_hybrid))+\n  geom_bar()\n\n\n\n\n\n\n\nFigure 3: A bar plot showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom.\n\n\n\n\n\n\n\n\n\n\ngc_rils |&gt; \n  ggplot(aes(x = prop_hybrid))+\n  geom_histogram(binwidth = 1/8, color = \"white\")+\n  scale_x_continuous(breaks= seq(0,1,1/8))\n\n\n\n\n\n\n\nFigure 4: A histogram showing the proportion of genotyped plants shown to be hybrid. Most of the time, we genotyped eight seeds per mom.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen making a histogram, you are in charge of the bin size. In ggplot2 you can specify this with one of two arguments:\n\nbinwidth: Which tells R how big to make the the bins.\n\nbins: Which tells R how many equally sized bins to split the data into.\n\nThere is not a universally correct answer for the appropriate bin size – it will vary by data set. It takes thought and expertise. It is best to experiment some until you find the binning that honestly reflects the variability in your data. This reflects a trade-off between capturing the variability, without distracting the reader with every bump or dip. I usually start with about thirty bins and then dial that number up or down until I feel like the variability in the data is well-communicated by the plot.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries.html#lets-get-started-with-univariate-summaries",
    "href": "book_sections/univariate_summaries.html#lets-get-started-with-univariate-summaries",
    "title": "5. Simple Summaries",
    "section": "Let’s get started with univariate summaries!",
    "text": "Let’s get started with univariate summaries!\nThe following sections introduce how to summarize a single variable by:\n\nDescribing its shape including its skew, and number of modes.\n\nChanging its shape (if necessary), including when to transform, rules of transformation, how to transform variables in R, and a table of common transformations.\n\nDescribing its center, including standard non-parametric and parametric summaries, when to use which, and some useful but used-less summaries.\n\nDescribing its width including nonparametric and parametric summaries, with a worked example with mathematical calculations, R functions, and an illustrative plot or visualization.\n\nThen we summarize the chapter, present practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources.",
    "crumbs": [
      "5. Simple Summaries"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_shape.html",
    "href": "book_sections/univariate_summaries/summarizing_shape.html",
    "title": "• 5. Summarizing shape",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\n\nMotivating Scenario:\nYou have a fresh new dataset and want to check it out. Before providing numeric summaries you want to poke around the data so you are prepared to responsibly present appropriate summaries.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nIdentify the “skew” of data: You should be able to distinguish between:\n\nRight skewed data – Most values small, some are very large.\nLeft skewed data – Most values large, some are very small.\nSymmetric data – There are roughly as many small as large values.\n\nIdentify the number of modes in a data set. Differentiate between one, two and more modes.\nExplain why we must consider the shape of data when summarizing it.\n\n\n\n\nWe start this chapter with bar plots and histograms instead of numerical summaries like the mean or variance because we must understand the shape of our data to properly present and interpret classic numeric summaries.\n\n\n\n\n\n\n\n\n\nFigure 1: Measuring Clarkia xantiana flowers. Image from CalPhotos shared by Chris Winchell with a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 (CC BY-NC-SA 3.0) license.\n\n\n\n\n\n\n\nSkew\nOne key aspect of a dataset’s shape is skewness—whether the data is symmetrical or leans heavily toward smaller or larger values.\n\nThe proportion of hybrid seeds per RIL mom (Figure 4, and shown again in Figure 2 A) is strongly right-skewed—most values are small, but some are very large. Right-skewed data is common in biology and everyday life. For example, income is right-skewed: most people earn relatively little money, but some make loads of ca$h.  \nThe number of offspring we genotyped (Figure 2 B) is strongly left-skewed—most values are large, but some are very small. Left-skewed data is also common in real-world settings. For example, age at death follows a left-skewed distribution: most people live long lives (often between 70 and 90), but some individuals die at much younger ages due to childhood mortality, diseases, accidents, or suicide.  \nThe distributions in Figure 2 A and B represent extreme cases of skewness, where most values are either very small or very large. In contrast, Figure 2 C shows that on a \\(\\text{log}_{10}\\) scale, petal area is nearly symmetric, with an even distribution of large and small values and most observations concentrated in the middle. \n\n\n\n\n\n\n\n\n\nFigure 2: A bar plot showing the number of genotyped seeds of each mom shown to be hybrid.\n\n\n\n\n\n\nData transformations and skew: Figure 2 showed that \\(\\text{log}_{10}\\) petal area was roughly symmetric. The careful might be suspicious and wonder why I transformed the data. The answer is that area is usually right skewed and log transforming often removes such skew.\nWe discuss changing the shape of distributions by transformation in the next section. Later we will see that symmetric data are often easier for statistical models than skewed data so such transformations are common.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of modes\nOne of the first things to notice when visualizing a dataset is whether the values cluster around a single peak or multiple peaks—this number of modes can reveal important patterns, such as distinct subgroups or natural variation in biological traits.\n\n\n\n\n\n\n\n\n\nFigure 3: The distribution of petal area across four xantiana / parviflora hybrid zones. Data are available here.\n\n\n\n\n\nIn unimodal distributions, there is a single, clear peak, with the number of observations in other bins decreasing as we move away from this central point. All distributions in Figure 2 are unimodal.\nIn bimodal distributions, there are two distinct peaks separated by a trough. Bimodal distributions are particularly interesting because they suggest that the dataset is composed of two distinct groups or categories.\nOf course, distributions can have more than two modes. Trimodal distributions have three peaks, and so on. However, be careful not to over-interpret small fluctuations—small dips can create false peaks from random variation. It’s worth experimenting with bin sze to avoid overinterpreting such small blips.\n\nThe number of modes is particularly important in the study of speciation, especially in populations that may be hybridizing.\n\nA unimodal hybrid zone suggests that two species merge when they come back into contact, implying they may not be distinct, stable species.\n\nBimodal or trimodal hybrid zones suggest that the two species largely maintain their distinctiveness when they have the opportunity to hybridize.\n\nIn a bimodal hybrid zone, the “trough” between peaks may include some hybrids.\nIn a trimodal hybrid zone, the middle peak might represent F1 hybrids.\n\n\nWe were particularly interested in examining the distribution of phenotypes in seeds collected from parviflora / xantiana hybrid zones. Figure 4 shows that—unlike petal area in our RILs-most phenotypes from natural hybrid zones are largely bimodal. However, Figure 3 suggests that these distributions may themselves be a blend of different underlying distributions. While petal area appears bimodal in most populations, it may be unimodal at site S6. Figure 4 and Figure 3 highlight the benefit of digging into the data visually. We must visualize the distribution o values of a variable before we can provide a meaningful and interpretable summary statistic. .\n\n\n\n\n\n\n\n\nFigure 4: Distributions of three floral traits in Clarkia xantiana hybrid zones: log10-transformed average petal area (sq. cm) (left), average protandry (middle), and average herkogamy (right). The petal area distribution is bimodal, with two distinct peaks. In contrast, both protandry and herkogamy are bimodal and strongly right-skewed, with most values clustered near zero and a secondary peak at higher values. These distributions suggest potential underlying biological structure, such as genetic variation or environmental influences shaping floral trait expression. Data are available here.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing shape"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/changing_shape.html",
    "href": "book_sections/univariate_summaries/changing_shape.html",
    "title": "• 5. Changing shape",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nlibrary(tweetrmd)\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(webexercises)\nlibrary(ggplot2)\nlibrary(tidyr)\nsource(\"../../_common.R\") \nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\n\nMotivating Scenario:\nYour data are not symmetric, you are wondering about potentially changing the shape of your data to make it easier to deal with. Here I introduce some thoughts and guidance about common data transformations and when to use them.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nUnderstand what a transformation is and know when they are (or are not) a good idea, and how they connect to the process that generated your data.\nKnow the rules for a legit transformation and be aware of common “gotchas” which accidentally break these rules.\nRecognize which transformations are appropriate for different skews of data.\n\nRight skewed data – Try log, square root, or reciprocal transformations.\nLeft skewed data – Try exponential or square transformation.\n\n\n\n\n\nStatistical approaches should follow biology\n\n\n\n\n\n\n\n\n\nFigure 1: Different biological traits follow different natural distributions. Panel A shows the distribution of stem diameter (mm) in a recombinant inbred line (Clarkia RIL) dataset, which is approximately symmetric and normally distributed. In contrast, panel B displays the distribution of petal area (mm²), which exhibits a right-skewed, exponential-like distribution.\n\n\n\n\nThere is nothing inherently “natural” about the linear scale — in fact, some research (review by Asmuth et al. (2018)) suggests that humans naturally think in log scale, and only begin thinking in linear scale with more formal schooling. That is, kids tend to think that the difference between seventy-five and eighty is smaller than the difference between five and ten. In a sense, they’re right — ten is double five, while eighty is only \\(\\frac{16}{15}\\) of seventy-five. Of course, in another sense, they’re wrong — the difference is five in both cases.\nIt turns out that different variables naturally vary on different scales. For example, growth rate, area, volume, and other such processes often generate right-skewed data (growth is exponential — at least initially — area scales with the square of length, volume scales with the cube, etc.). It’s therefore not surprising that Clarkia stem diameter in the RILs has a fairly symmetric distribution, while petal area is right-skewed (Figure 1).\nBy contrast, variables that exhibit diminishing returns or are constrained by hard limits often result in left-skewed distributions (as we saw in our genotyping efforts, where we aimed to genotype eight seeds per mom — and no more — but sometimes ended up with fewer).\n\n\n\nTo transform or not to transform? That is the question\nWhen the underlying biological process results in nonlinear data with asymmetric distributions, transforming the data is often appropriate — even if you don’t know the specific biological mechanism. So: transforming your data is often OK, but is it a good idea?\n\nWhy transform? Because linear and symmetric data are often easier to interpret and typically better suited for statistical modeling.\n\nWhy not transform? Truth and clear understanding of data is more important than having symmetric distributions. So while transforming is perfectly legitimate, it can make your results harder to communicate (I, for one, find thinking on the log scale difficult — even if four-year-olds do not), since most people are accustomed to a linear world. You may also worry that knee-jerk transformation might hide the very biological processes that we are aiming to understand. Finally, some data can’t be made symmetric or unimodal no matter what transformation you apply. For example, it’s rare for transformations to make multimodal data unimodal. So take this all into consideration before jumping to transform your data to make it have an appealing shape.\n\n\n\n\nRules for Transforming Data\nIn addition to letting biology guide your decisions, there are some actual rules that determine when a transformation is legitimate:\n\nTransformed values must have a one-to-one correspondence with the original values. For example, don’t square values if some are negative and others positive — you’ll lose information.\n\nTransformed values must maintain a monotonic relationship with the original values. This means that the order of values should be preserved — larger values in the original data should remain larger after transformation.\n\nTransformations must not bias results by inadvertently dropping data. For example, this can happen when a log transformation fails on zero or negative values, or if extreme values are removed during transformation.\n\n\n\nTransformation in R\n\n\n\n\n\n\n\n\n\nFigure 2: Applying a log10 transformation can change the shape of a dataset. Panel A shows the distribution of Clarkia petal area (mm²) on a linear scale, where the data are right-skewed. Panel B shows the same data after a log10 transformation, which reduces skewness and results in a more symmetric distribution.\n\n\n\n\nTransformation in R is remarkably straightforward. Simply use dplyr's mutate() function to add a transformed variable. Below, I show a log base 10 transformation — one of the most common and useful approaches for right-skewed data (see the next section for a more thorough description of this and other common transformations). As discussed in the previous chapter, Figure 2 shows that this transformation changes the shape of the petal area distribution from right-skewed to roughly symmetric.\n\ngc_rils  &lt;- gc_rils |&gt;\n  mutate(log10_petal_area = log10(petal_area_mm))\n\n\n\n\n\n\n\n\n\nCommon Transformations\nApplying the right transformation can improve interpretability, meet statistical assumptions, and connect patterns to process. The table below introduces some of the more common transformations you will run into.\n\n\n\nTransform\nFormula\nWhat it does\nData it’s good for\nExample datasets\nR function (use with mutate())\nLimitations\n\n\n\n\nLog\nlog(x)\nCompresses large values, spreads small values\nRight-skewed data\nBody mass, gene expression, reaction times\nlog()\nValues ≤ 0 cause errors\n\n\nLog + 1\nlog(x + 1)\nSimilar to log but works for zero values\nRight-skewed data with zeros\nPopulation counts, RNA-seq read counts\nlog1p()\nValues ≤ -1 cause errors\n\n\nLog Base 10\nlog10(x)\nSimilar to log but base 10, this can be more interpretable.\nRight-skewed data\npH, sound intensity, scientific measurements\nlog10()\nValues ≤ 0 cause errors\n\n\nSquare Root\nsqrt(x)\nReduces range while preserving order\nRight-skewed data\nEnzyme activity\nsqrt()\nValues &lt; 0 cause errors\n\n\nSquare\n\\(x^2\\)\nIncreases spread, emphasizes large values\nLeft-skewed data\n\nx^2\nMakes values more extreme\n\n\nCube\n\\(^3\\)\nFurther increases spread\nLeft-skewed data\nTree volume, growth rates\nx^3\nStrongly affects scale\n\n\nInverse\n\\(1/x\\)\nEmphasizes small values, compresses large ones\nRight-skewed data\nReaction times, waiting times\n1/x\nValues = 0 cause errors\n\n\nArcsin Square Root\n\\(arcsin(\\sqrt{x})\\)\nNormalizes proportions\nProportions (e.g., survival, germination rates)\nAllele frequencies, % cover\nasin(sqrt())\nWorks poorly near 0 and 1\n\n\n\n\n\n\n\nAsmuth, J., Morson, E. M., & Rips, L. J. (2018). Children’s understanding of the natural numbers’ structure. Cognitive Science, 42(6), 1945–1973. https://doi.org/https://doi.org/10.1111/cogs.12615",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Changing shape"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_center.html",
    "href": "book_sections/univariate_summaries/summarizing_center.html",
    "title": "• 5. Summarizing the center",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\n\n\n\nMotivating Scenario:\nYou are continuing your exploration of a fresh new dataset. You have figured out the shape and made the transformations you thought appropriate. You now want to get some numerical summaries of the center of the data.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nDifferentiate between parametric and nonparametric summaries: and know what shapes of data make one more appropriate than the other.\nCalculate and interpret standard summaries of center in R. These include:\n\nMedian: The middle.\nMean: The center of gravity.\n\nMode(s): The common observation(s).\n\nLook up / use less common summaries of the center. These include:\n\nThe trimmed mean: The average after removing a fixed percentage of the smallest and largest values (i.e., trimming the “tails”).\n\nThe harmonic mean: The reciprocal of the arithmetic mean of reciprocals, useful for averaging rates.\n\nThe geometric mean: The \\(n^{th}\\) root of the product of all values, often used for multiplicative data.\n\n\n\n\n\nWe hear and say the word, “Average”, often. What do we mean when we say it? “Average” is an imprecise term for a middle or typical value.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Step-by-step process of finding the median petal area in parviflora RILs. The animation begins with unordered petal area measurements plotted against their dataset order. The values are then sorted in increasing order, and a vertical dashed line appears at the middle value, marking the median. The median is highlighted, illustrating how it divides the dataset into two equal halves.\n\n\n\n\nThere are many ways to describe the center of a dataset, but we can broadly divide them into two categories – “nonparametric” or “parametric”. We will first show these summaries for petal area in our parviflora RILS, then compare them for numerous traits in these RILs.\n\nNonparametric summaries\nNonparametric summaries describe the data as it is, without assuming an underlying probability model that generated it. The most common non-parametric summaries of center are:\n\nMedian: The middle observation, which is found by sorting data from smallest to biggest (Shown visually in Figure 1).\n\nSelecting the value of the \\(\\frac{n+1}{2}^{th}\\) value if there are an odd number of observations,\nSelecting the average of the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{(n+2)}{2}^{th}\\) observations if there are an even number of observations.\n\nOr just use the median() function in R – usually inside summarize() (revisit the chapter on summarizing columns in dplyr for a refresher). Remember to specify na.rm = TRUE.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of the mode in petal areas of parviflora RILs. The histogram displays the distribution of petal area (mm), with the mode marked by a blue vertical line and labeled in blue text. The mode represents the most frequently occurring value in the dataset, corresponding to the tallest bar in the histogram.\n\n\n\n\n\nMode(s): The most common observation(s) or observation bin (Figure 2).\n\nWhen reporting the mode, make sure your bin size is appropriate so as to make this a meaningful summary.\n\nCommunicating the modes is particularly important bimodal and multimodal data.\n\n\n\n\nParametric summaries\nParametric summaries describe the data in a way that aligns with a probability model (often the normal distribution), allowing us to generalize beyond the observed data.\n\nMean: The mean is the most common description of central tendency, and is known as the expected value or the weight of the data.\n\nWe find this by adding up all values and dividing by the sample size. In math notation the mean, \\(\\overline{X} = \\frac{\\Sigma x_i}{n}\\), where \\(\\Sigma\\) means that we sum over the first \\(i = 1\\), second \\(i = 2\\) … up until the \\(n^{th}\\) observation of \\(x\\), \\(x_n\\). and divide by \\(n\\), where \\(n\\) is the size of our sample. Remember this size does not count missing values.\nOr just use the mean() function in R – usually inside summarize() (revisit the chapter on summarizing columns in dplyr for a refresher). Remember to specify na.rm = TRUE.\n\n\nRevisiting our examples above, we get the following simple summaries of mean and median. To do so, I type something like the code below (with elaborations for prettier formatting etc).\n\n\nBut remember mean and/or median may not be the best ways to summarize the center of either data set.\n\ngc_rils|&gt;\n  mutate(log10_petal_area_mm = log10(petal_area_mm))|&gt;\n  summarise(mean_log10_petal_area_mm = mean(log10_petal_area_mm, na.rm=TRUE),\n            median_log10_petal_area_mm = median(log10_petal_area_mm, na.rm = TRUE))\n\n# and\n\ngc_rils|&gt;\n  mutate(mean_mean_visits = mean(mean_visits, na.rm=TRUE),\n         median_mean_visits = median(mean_visits, na.rm = TRUE))\n\n\n\n\n\n\nsummary\nlog10 petal area in hybrid zones (cm^2)\nlog10 petal area in RILs (mm^2)\nPollinator visitation in GC\n\n\n\n\nmean\n-0.099\n1.781\n0.12\n\n\nmedian\n0.064\n1.789\n0.00\n\n\n\n\n\n\n\n\n\nWhich Summaries to Use When?\n\nMeans are best when data are roughly symmetric and plausibly generated by a well-understood distribution. Parametric summaries like the mean integrate easily with most statistical methods, and in many cases, the mean, median, and mode are roughly equivalent.\nMedians are most appropriate when data are skewed. A classic example is income data—if Bill Gates walks into a room, the mean wealth increases dramatically, but the typical person in the room does not become wealthier. The median, which is less affected by extreme values, provides a more representative summary in such cases.\nModal peaks are most appropriate when data have multiple peaks (modes) or a large, dominant peak, the mode is often the most relevant measure of central tendency. For example, in our investigation of petal area in a Clarkia hybrid zone, the mean and median of log₁₀ petal area (cm²) were both close to zero (which corresponds to 1 cm²). However, this value falls in the trough between two peaks in the histogram—one corresponding to Clarkia xantiana xantiana and another to Clarkia xantiana parviflora. This means that neither the mean nor the median represents an actual plant particularly well, and the modal peaks give a clearer picture of what values are most typical.\n\n\n\n\nCool down\n\n\n\n\n\n\n\n\nFigure 3: Distributions of select traits in Clarkia datasets. This figure shows histograms of three different variables from two datasets: Recombinant Inbred Line (RIL) populations and a hybrid zone dataset. The left panel displays the distribution of petal area (log10 mm^2) in the RIL dataset, showing a unimodal distribution. The middle panel presents the log10-transformed petal area (log10 cm^2) in the hybrid zone dataset, which appears bimodal. The right panel illustrates the number of pollinator visits at GC in the RIL dataset, showing a highly right-skewed distribution with many zero observations.\n\n\n\n\n\n\nNow let’s refresh our understanding of standard summaries of central tendency, by reflecting on what we are trying to capture / describe. For each of the plots in Figure 3, above, consider the appropriate summary. Most importantly explain what the summary is getting at, why it is more appropriate than other options, and what (if anything) it fails to capture. Then provide guesstimates of these summaries. \n\n\n\n\n\n\n\nUse-full but used-less summaries\nBelow are a few additional, useful but less commonly used, summaries of central tendency. It is good to know these exist. If this material is too slow / easy. for you, I recommend using your study time to familiarize yourself with these useful summaries, but otherwise don’t worry about them.\nThese assume that you are modelling these non-linear processes on a linear scale. You can decide if transformation or a more relevant summary statistics on a linear scale is more effective for your specific goal.\n\n\nHarmonic mean – Is the reciprocal of the mean of reciprocals. Useful when averaging rates, ratios, or speeds. Unlike the arithmetic mean, which sums values, the harmonic mean gives more weight to smaller values and is particularly useful when values are reciprocals of meaningful quantities. For example in my field population genetics, the harmonic mean is used to calculate effective population size (\\(N_e\\)), as small population sizes have a disproportionate effect on genetic drift.\n\nMathematical calculation of the harmonic mean: The harmonic mean of a vector x is = \\(\\frac{1}{\\text{mean}(\\frac{1}{x})}\\) = \\(\\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\\).\nHarmonic mean in R: You can find the harmonic mean as: 1/(mean(1/x)), or use the Hmean() function in the DescTools package. Watch out for zeros!!\n\nGeometric mean - Is the \\(n^{th}\\) root of the product of \\(n\\) observations. The geometric mean is a useful summary of multiplicative or exponential processes For example: (1) Bacterial growth: If a bacterial population doubles in size daily, the geometric mean correctly summarizes growth trends, and (2) pH values in chemistry: Since pH is logarithmic, the geometric mean is a better measure than the arithmetic mean.\n\nMathematical calculation of the geometric mean: The geometric mean of a vector x is \\(\\left( \\prod_{i=1}^{n} x_i \\right)^{\\frac{1}{n}}\\), where \\(\\prod\\) is the the “cumulative”product operator” i.e. the cumulative product of all observations.\nGeometric mean in R: You can find the geometric mean as: prod(x)^(1/sum(!is.na(x))), or use the Gmean() function in the DescTools package. Watch out for negative values as they make this kind of meaningless.\n\nTrimmed mean – A robust version of the mean that reduces the influence of extreme values by removing a fixed percentage of the smallest and largest observations before calculating the average. A 10% trimmed mean, for example, removes the lowest 10% and highest 10% of values before computing the mean. This is useful when extreme values may distort the mean but full exclusion of outliers isn’t justified (e.g., summarizing body weights where a few exceptionally large or small individuals exist).\n\nTrimmed mean in R: You can find the trimmed mean yourself or by using the trimmed_mean() function in the in the r2spss package.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing the center"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_variability.html",
    "href": "book_sections/univariate_summaries/summarizing_variability.html",
    "title": "• 5. Summarizing variability",
    "section": "",
    "text": "Code for selecting data from a few columns from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                visited = mean_visits &gt; 0)\ngc_rils &lt;- ril_data |&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(petal_color, petal_area_mm, num_hybrid, offspring_genotyped, prop_hybrid, mean_visits , asd_mm )\nlibrary(ggthemes)\n\n\n\nMotivating Scenario: You are continuing your exploration of a fresh dataset. You have examined its shape and applied any necessary transformations. Now, you want to obtain numerical summaries that describe the variability in your data.\nLearning Goals: By the end of this subchapter, you should be able to:\n\nExplain why variability in a dataset is biologically important.\nDifferentiate between parametric and nonparametric summaries and understand which data shapes make one more appropriate than the other.\nVisualize variability and connect numerical summaries to plots. You should be able to read the interquartile range off of a boxplot.\nDistinguish between biased and unbiased estimators.\nCalculate and interpret standard summaries of variability in R, including:\n\nInterquartile range (IQR): The difference between the 25th and 75th percentiles, summarizing the middle 50% of the data.\n\nStandard deviation and variance: The standard deviation quantifies how far, on average, data points are from the mean. Variance is the square of the standard deviation.\n\nCoefficient of variation (CV): A standardized measure of variability that allows for fair comparisons across datasets with different means.\n\n\n\n\n\n\n\n\nIn a world where everything was the same every day (See video above), describing the center would be enough — luckily our world is more exciting than that. In the real world, the extent to which a measure of center is sufficient to understand a population depends on the extent and nature of variability. Not only is understanding variability essential to interpreting measures of central tendency, but in many cases, describing variability is as or even more important than describing the center. For example, the amount of genetic variance in a population determines how effectively it can respond to natural selection. Similarly, in ecological studies, two populations of the same species may have similar average survival rates, but greater variability in survival in one population might indicate environmental instability, predation pressure, or developmental noise.\n\nNonparametric measures of variability {#summarizing_variability_nonparametric-measures-of-variability}.\nPerhaps the most intuitive summary of variation in a dataset is the range—the difference between the largest and smallest values. While the range is often worth reporting, it is a pretty poor summary of variability because it is highly sensitive to outliers (a single unexpectedly extreme observation can strongly influence the range) and is biased with respect to sample size (the more samples you collect, the greater the expected difference between the smallest and largest values).\nAs such, the interquartile range (IQR)—the difference between the 75th and 25th percentiles (i.e., the third and first quartiles)—is a more robust, traditional nonparametric summary of variability. We can read off the interquartile range from a boxplot. A boxplot shows a box around the first and third quartiles, a line at the median, and “whiskers” that extend to the minimum and maximum values (excluding outliers, which are shown as black dots).\n\n\ngc_rils |&gt;\n  ggplot(aes(x = 1,\n      y = petal_area_mm))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nFigure 1: Anatomy of a boxplot. This boxplot displays the distribution of petal area in the Clarkia RIL dataset. The box spans from the first quartile (25th percentile) to the third quartile (75th percentile), highlighting the interquartile range (IQR). The line inside the box marks the median (50th percentile). The “whiskers” extend to the smallest and largest non-outlier values, and individual outliers are shown as separate points.\n\n\n\n\nFigure 1 shows that the third quartile for petal area is a bit above seventy, and the first quartile is a bit above fifty, so the interquartile range is approximately twenty. Or with the IQR() function:\n\ngc_rils |&gt;\n  summarise(Q3 = quantile(petal_area_mm,.75, na.rm = TRUE),\n            Q1 = quantile(petal_area_mm,.25, na.rm = TRUE),\n            iqr_petal_area = IQR(petal_area_mm, na.rm = TRUE))\n\n# A tibble: 1 × 3\n     Q3    Q1 iqr_petal_area\n  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1  71.6  51.4           20.2\n\n\n\n\nParametric summaries of variability\nMathematical summaries of variability aim to describe how far we expect an observation to deviate from the mean. Such summaries start by finding the sum of squared deviations, \\(SS_X\\) (i.e. the sum of squared differences between each observation and the mean). We square deviations rather than taking their absolute value because squared deviations (1) are mathematically tractable, (2) emphasize large deviations, and (3) allow the mean to be the value that minimizes them — which ties directly into least squares methods we’ll use later in regression. We find \\(SS_X\\), know as “the sum of squares” as:\n\\[\\text{Sum of Squares} = SS_X = \\Sigma{(X_i-\\overline{X})^2}\\]\nFrom the sum of squares we can easily find three common summaries of variability:\n\n\nWhy divide by \\(n-1\\)? When we calculate how far values are from the mean, we might think to average the squared deviations by dividing by the number of values, \\(n\\). But, by calculating the mean, we’ve used up a little bit of information. Because the mean pulls the values toward itself, numbers aren’t totally free to vary anymore. Because of that, we divide the sum of squares by \\((n-1)\\) rather than \\(n\\). This gives us a more accurate sense of how spread out the values really are, based on how much they can still vary around the mean.\n\nThe variance, \\(s^2\\) is roughly the average squared deviation, but we divide the sum of squares by our sample size minus 1. That is the \\(\\text{variance} = s^2 = \\frac{SS_x}{n-1} = \\frac{\\Sigma{(X_i-\\overline{X})^2}}{n-1}\\). The variance is mathematically quite handy, and is in squared units relative to the initial observations.\nThe standard deviation, \\(s\\) is simply the square root of the variance. The standard deviation is often easier to think about because it lies on the same (linear) scale as the initial data (as opposed to the squared scale of the variance).\nThe coefficient of variation, CV allows us to compare variance between variables with different means. In general variability increases with the mean, so you cannot meaningful compare the variance in petal area (which equals 203.399) with the variance in anther stigma distance (which equals 0.134). But it’s still biologically meaningful to ask: “Which trait is more variable, relative to its mean?” We answer this question by finding the coefficient of variation which equals the standard deviations divided by the mean: CV = \\(\\frac{s_x}{\\overline{X}}\\). Doing so, we find that anther–stigma distance is nearly twice as variable as petal area.\n\n\nIt’s ok to be MAD. The mean absolute difference (aka MAD, which equals \\(\\frac{\\Sigma{|x_i-\\bar{x}|}}{n}\\)) is a valid, and robust, but non-standard summary of variation. The MAD is most relevant when presenting the median as the median minimizes the sum of absolute deviations, while the mean minimizes the sum of squared deviations.\n\n\n\nParametric summaries of variability: Example\n\n\n\n\n\n\n\n\nFigure 2: Understanding the squared deviation. The “Sum of Squared Deviations” is critical to understanding standard summaries of variability. The animation above aims to explain a “squared deviation.” Left panel: Each individual’s deviation from the mean petal area is visualized as a vertical line, with the iᵗʰ value highlighted in pink. Middle panel: The same individual is plotted in a 2D space – the squared deviation is shown as the area of a pink rectangle. Right panel: The squared deviation is shown for all individuals, with the same focal individual highlighted.\n\n\n\n\n\nTo make these ideas clear, let’s revisit the distribution of petal area in parviflora RILs planted in GC. Figure 1 shows how we calculate the Squared Deviation for each data point.\n\nThe left panel shows the difference between each observed value and its mean.\n\nThe middle panel shows this as a box (or square) away from the overall, “grand” mean.\n\nThe right panel shows the squared deviation.\n\nWe find the sum of squares by summing these values, and then use this to find the variance, standard deviation and coefficient of variation, following the formulae above:\n\ngc_rils |&gt;\n  filter(!is.na(petal_area_mm))|&gt;\n  summarise(mean_petal_area = mean(petal_area_mm),\n            ss_petal_area = sum((petal_area_mm - mean_petal_area)^2),\n            var_petal_area = ss_petal_area / (n()-1),\n            sd_petal_area = sqrt(ss_petal_area ),\n            CV_petal_area = sd_petal_area / mean_petal_area )\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_petal_area\nss_petal_area\nvar_petal_area\nsd_petal_area\nCV_petal_area\n\n\n\n\n62.05\n19933\n203.4\n14.26\n0.23\n\n\n\n\n\nOr we can skip the formulae and just use standard R functions, var() and sd(). We can even find the mean absolute difference with the mad() function:\n\ngc_rils |&gt;\n  filter(!is.na(petal_area_mm))|&gt;\n  summarise(mean_petal_area = mean(petal_area_mm),\n            var_petal_area = var(petal_area_mm),\n            sd_petal_area  = sd(petal_area_mm),\n            CV_petal_area = sd_petal_area / mean_petal_area,\n            mad_petal_area = mad(petal_area_mm ))\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_petal_area\nmad_petal_area\nvar_petal_area\nsd_petal_area\nCV_petal_area\n\n\n\n\n62.05\n15.01\n203.4\n14.26\n0.23",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing variability"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html",
    "title": "• 5. Summarizing summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor Questions. Glossary. R functions. R packages. More resources.\nA beautiful Clarkia xantiana flower.\nBecause they can be used to parameterize an entire distribution, the mean and variance (or its square root, the standard deviation) are the most common summaries of a variable’s center and spread. However, these summaries are most meaningful when the data resemble a bell curve. To make informed choices about how to summarize a variable, we must first consider its shape, typically visualized with a histogram. When data are skewed or uneven, we can either transform the variable to make its distribution more balanced, or use alternative summaries like the median and interquartile range, which better capture the center and spread in such cases.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#chapter-summary",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#chapter-summary",
    "title": "• 5. Summarizing summary",
    "section": "",
    "text": "Chatbot tutor\n\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_practice-questions",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_practice-questions",
    "title": "• 5. Summarizing summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”.\n\nIrisFaithfulRivers\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nThe tabs above – Iris, Faithful, and Rivers – all attempt to make histograms, but include errors, and may have improper bin sizes. (Click the Iris tab if they are initially empty).\nQ1) Iris, Faithful, and Rivers – all attempt to make histograms, but include errors. Which code snippet makes the best version of the Iris plot (ignoring bin size)?\n\n ggplot(iris,aes(x = Sepal.Width, fill = 'white'))+ geom_histogram() ggplot(iris,aes(x = Sepal.Width, color = 'white'))+ geom_histogram() ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(color = 'white') ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(color = white) ggplot(iris,aes(x = Sepal.Width))+ geom_histogram(fill = 'white')\n\n\nBefore addressing this set of questions fix the errors in the histograms of Iris, Faithful, and Rivers, and adjust the bin size of each plot until you think it is appropriate. (Click any of the tabs if they are initially empty).\nQ2a) Which variable is best described as bimodal? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\nQ2b) Which variable is best described as unimodal and symmetric? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\nQ2c) Which variable is best described as unimodal and right skewed? Iris sepal widthWaiting time between eruptions of old faithful.log-10 river length\n\n\n\nPenguins\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ3) I calculate means in two different ways above and get different answers. Which is correct? mean_mass_1mean_mass_2it depends\nQ4) What went wrong in calculating these means?\n\n n() counts the number of entries, but we need the number of non-NA entries. na.rm should be set to TRUE, not T The denominator for the mean is (n - 1), not n. Nothing – they are both potentially correct depedning on your goals.\n\n\nQ5) Accounting for species differences in mean body mass, which penguin species shows the greatest variability in body mass? AdelieChinstrapGentoo\n\n\nClick here for code\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\npenguins |&gt;\n  group_by(species)|&gt;\n  summarize(mean_mass = mean(body_mass_g, na.rm = T),\n            sd_mass   = sd(body_mass_g, na.rm = T),\n            coef_var_mass =   sd_mass / mean_mass \n  )\n\n# A tibble: 3 × 4\n  species   mean_mass sd_mass coef_var_mass\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Adelie        3701.    459.        0.124 \n2 Chinstrap     3733.    384.        0.103 \n3 Gentoo        5076.    504.        0.0993\n\n\n\n.\n\nFor the next set of questions consider the boxplot below, which summarizes the level of Lake Huron in feet every year from 1875 to 1972.\n\n\n\n\n\n\n\n\n\nQ6a) The mean is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the mean from this plot.\n\nQ6b) The median is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the median from this plot.\n\nQ6c) The mode is roughly\n\n Six One and three quarters Five hundred and seventy nine We cannot estimate the mode from this plot.\n\nQ6d) The interquartile range is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the IQR from this plot.\n\nQ6e) The range is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the range from this plot.\n\nQ6f) The variance is roughly\n\n Six One and three quarters The median Five hundred and seventy nine We cannot estimate the variance from this plot.\n\n\nBrooke planted RILs at four different locations, and found tremendous variation in the proportion of hybrid seed across locations. The first step in quantifying this variation is to calculate the sum of squares, so let’s do it. Use the image below (Figure 1) to calculates sum of squares for proportion hybrid seeds across plants planted at four different locations.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7a) The sum of squares for differences between proportion hybrids at each location and the grand mean equals: \nQ7b) So the variance is \nQ7c) The standard deviation is \nQ7d) Accounting for differences in their means, how does the variability in the proportion of hybrid seed across locations compare to the variability in petal area among RILs? (refer to this section for reference)\n\n They are very similar Petal area among RILs is roughly thirty times as variable The proportion of hybrid seed among sites is roughly two times as variable You cannot compare variability for different traits measured on such different scales\n\n\n\nClick here for hint\n\nWe can compare the variability of traits measured on different scales by dividing the standard deviation by the mean. This gives us the coefficient of variation (CV), a standardized measure of spread.\n\n.\nQ8) Why is it important to standardize by the mean when comparing variability between variables?",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_glossary-of-terms",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_glossary-of-terms",
    "title": "• 5. Summarizing summary",
    "section": "Glossary of Terms",
    "text": "Glossary of Terms\n\n\n📐 1. Shape and Distribution\n\nSkewness: A measure of asymmetry in a distribution.\n\nRight-skewed: Most values are small, with a long tail of large values.\n\nLeft-skewed: Most values are large, with a long tail of small values.\n\nMode: The most frequently occurring value (or values) in a dataset.\nUnimodal / Bimodal / Multimodal: Describes the number of peaks (modes) in a distribution.\n\nUnimodal: One peak\n\nBimodal: Two peaks, possibly indicating two subgroups\n\nMultimodal: More than two peaks\n\n\n\n\n\n🔁 2. Transformations and Data Shape\n\nTransformation: A mathematical function applied to data to change its shape or scale. Often used to reduce skew or satisfy model assumptions.\nMonotonic Transformation: A transformation that preserves the order of values (e.g., if \\(x_1 &gt; x_2\\), then \\(f(x_1) &gt; f(x_2)\\)). Required for valid shape-changing operations.\nLog Transformation (log(), log10()): Reduces right skew by compressing large values.\n\n✅ Use for right-skewed data (e.g., area, income, growth).\n\n⚠️ Don’t use with zero or negative values — log is undefined in those cases. A workaround is log(x + 1) for count data.\n\nSquare Root Transformation (sqrt()): Less aggressive than log. Preserves order while compressing large values.\n\n✅ Use for right-skewed data like enzyme activity or count data.\n\n⚠️ Not defined for negative values.\n\nReciprocal / Inverse (1/x): Emphasizes small values and compresses large ones.\n\n✅ Use for rates or time-based data (e.g., reaction time).\n\n⚠️ Undefined for zero values; extremely sensitive to small values.\n\nSquare / Cube (x^2, x^3): Spreads data out, emphasizing large values.\n\n✅ Can reduce left skew.\n\n⚠️ Squaring loses sign if data contains negatives; avoid if data include both positive and negative values.\n\n\n\n\n\n🎯 3. Summarizing the Center (Central Tendency)\n\nMean (mean()): The arithmetic average. Sensitive to outliers.\n\n\\(\\overline{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\)\n\nMedian (median()): The middle value of a sorted dataset. Robust to outliers.\nMode: Most frequent value or value bin.\nTrimmed Mean: Mean after removing fixed percentages of extreme values. Balances robustness and efficiency.\nGeometric Mean: The nth root of the product of values.\n\n✅ Appropriate for multiplicative data (e.g., growth rates, ratios, log-normal data).\n\n⚠️ Don’t use with zeros or negative values — the geometric mean is undefined.\n\n🧠 Tip: Especially useful for right-skewed, strictly positive data that spans multiple orders of magnitude.\n\nHarmonic Mean: The reciprocal of the mean of reciprocals.\n\n✅ Useful when averaging ratios or rates (e.g., speed, population size in genetics).\n\n⚠️ Very sensitive to small values and undefined for zero or negative numbers.\n\n🧠 Tip: Use when the quantity being averaged is in the denominator (e.g., “miles per hour”).\n\n\n\n\n\n📉 4. Summarizing Variability\n\nRange: Difference between maximum and minimum. Sensitive to outliers.\nInterquartile Range (IQR) (IQR()): Middle 50% of data. Robust and often paired with the median.\nMean Absolute Deviation (MAD) (mad()): The average absolute deviation from the mean or median. Robust and intuitive.\n\n\\(\\text{MAD} = \\frac{1}{n} \\sum |x_i - \\bar{x}|\\)\n\nSum of Squares (SS): Total squared deviation from the mean.\n\n\\(SS = \\sum (x_i - \\bar{x})^2\\)\n\nVariance (var()): The average squared deviation from the mean.\n\n\\(s^2 = \\frac{SS}{n - 1}\\)\n\nStandard Deviation (sd()): Square root of variance. Easier to interpret due to linear units.\n\n\\(s = \\sqrt{s^2}\\)\n\nCoefficient of Variation (CV): Standard deviation divided by the mean. Unitless and good for comparing across traits or units.\n\n\\(CV = \\frac{s}{\\bar{x}}\\)\n\n\n\n\n\n📊 5. Visualizing Distributions\n\nHistogram: Shows frequency of values within bins. Useful for assessing shape, skewness, and modes.\nBoxplot: Summarizes median, quartiles, range, and outliers in a compact visual form.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_key-r-functions",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_key-r-functions",
    "title": "• 5. Summarizing summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n\n📊 Visualizing Univariate Data\n\ngeom_histogram() ([ggplot2]): Makes histograms for visualizing distributions.\n\ngeom_boxplot() ([ggplot2]): Visualizes the distribution using a box-and-whisker plot.\n\ngeom_col() ([ggplot2]): Creates bar plots from summarized data.\n\ngeom_bar() ([ggplot2]): Bar plot for raw count data.\n\n\n\n\n📈 Summarizing Center\n\nmean() ([base R]): Computes the arithmetic mean.\n\nmedian() ([base R]): Computes the median.\n\nmutate() ([dplyr]): Adds new variables or transforms existing ones.\n\nsummarise() ([dplyr]): Reduces multiple rows to a summary value per group.\n\n\n\n\n📏 Summarizing Variability\n\nvar() ([base R]): Computes variance.\n\nsd() ([base R]): Computes standard deviation.\n\nmad() ([base R]): Computes the median absolute deviation — a robust summary of variability.\n\nIQR() ([base R]): Computes the interquartile range.\nquantile() ([base R]): Returns sample quantiles. Useful for computing percentiles and quartiles.\n\nsum() ([base R]): Used in calculating the sum of squared deviations (sum((x - mean(x))^2)).\n\n\n\n\n🔁 Transformations\n\nlog() ([base R]): Natural log (base e) transformation.\n\nlog10() ([base R]): Base 10 log transformation.\n\nsqrt() ([base R]): Computes square roots.\n\n^ ([base R]): Exponentiation (x^2, x^3, etc.).\n\n[1/x]: Reciprocal transformation. Beware of dividing by zero!",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_r-packages-introduced",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#summarizing_summaries_r-packages-introduced",
    "title": "• 5. Summarizing summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nggforce: Provides advanced geoms for ggplot2. This chapter uses geom_sina() to reduce overplotting by jittering points while preserving density.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/univariate_summaries/summarizing_summaries.html#additional-resources-summarizing_summaries_additional-resources",
    "href": "book_sections/univariate_summaries/summarizing_summaries.html#additional-resources-summarizing_summaries_additional-resources",
    "title": "• 5. Summarizing summary",
    "section": "Additional resources #summarizing_summaries_additional-resources}",
    "text": "Additional resources #summarizing_summaries_additional-resources}\n\nR Recipes:\n\nCompute summary statistics for a table: Learn how to find summary stats within a summarise() call.\n\nCompute summary statistics for groups of rows within a table Discover how to calculate summary stats by group.\n\nVisualize a Distribution with a Histogram: Learn to plot histograms to visualize the distribution of a continuous variable.\n\nVisualize a Boxplot: Find out how to create boxplots to summarize the distribution of a continuous variable and identify potential outliers.\n\nVideos:\n\nData summaries from Calling Bullshit (Bergstrom & West, 2020). Fun video to help with thinking about various summaries of center, and when to use which.\nThe shape of data from crash course in statistics.\n\n\n\n\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The art of skepticism in a data-driven world. Random House.",
    "crumbs": [
      "5. Simple Summaries",
      "• 5. Summarizing summary"
    ]
  },
  {
    "objectID": "book_sections/associations.html",
    "href": "book_sections/associations.html",
    "title": "6. Associations",
    "section": "",
    "text": "Correlation is not causation\nOf course, it’s not just the association between variables we care about — it’s what such associations imply. We want to:\nIn future chapters we will see when and how we can achieve these higher goals. But for now, know that while such goals are noble,\n“Correlation is not causation.” You’ve probably heard that before — but what does it actually mean? Let’s start by unpacking the two key concepts in that statement:\nVideo\n\n\nFigure 2: This man is not starting and stopping the train. (tweet)\nCorrelation is often confused for causation because it’s easy to assume that if two things are associated, one must be causing the other — especially when the association feels intuitive or lines up with our expectations, but this is wrong. While correlation may hint at causation, a direct cause is neither necessary nor sufficient to generate a correlation. Take the video in Figure 2 – an alien might think this man is starting and stopping the train, but clearly he has nothing to do with the train starting or stopping.\nThere are three basic reasons why and when we can have a correlation without a causal relationship– Coincidence, Confound, and Reverse causation.\nFigure 3: Potential confounding in parviflora RILs. The observed association between proportion hybrid seed and anther–stigma distance (A), might be due to the fact that both anther–stigma distance (B), and proportion hybrid seed increases with petal area (C), rather than a causal effect of anther–stigma distance itself.\nFigure 4: Visualizing an association between petal color and pollinator visitation. Both panels show that pink-petaled flowers are more likely to be visited by pollinators than white-petaled flowers. In the left panel, visit status is on the x-axis and petal color is shown within bars; in the right panel, petal color is on the x-axis and visit status is shown within bars. While the association is the same, the visual framing shifts the way we interpret direction — we typically place and think of explanatory variables (causes) on the x-axis and outcomes on the y-axis. However, these alternative visualizations make it clear that the data cannot speak to cause.\nThis issue isn’t just theoretical — I’m currently grappling with a real case in which the direction of causation is unclear. In natural hybrid zones, white-flowered parviflora plants tend to carry less genetic ancestry from xantiana (their sister taxon) than do pink-flowered parviflora plants. There are two potential explanations for this observation:\nI do not yet know the answer.\n# YANIV ADD FIGURE FROM SHELLEY",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#correlation-is-not-causation",
    "href": "book_sections/associations.html#correlation-is-not-causation",
    "title": "6. Associations",
    "section": "",
    "text": "Correlation means that two variables are associated.\n\nA positive association means that when one variable is large (or small) the other is often big (or large).\n\nA negative association means that when one variable is large (or small) the other is often small (or large).\n\nCausation means that changing one variable produces a change in the other. (For a deeper dive, see Wikipedia.)\n\n\n\n\n\nCoincidence: Chance is surprisingly powerful. In a world full of many possible combinations between variables, some strong associations will arise purely by luck. Later sections of the book will show how to evaluate the “NULL” hypothesis that an observed association arose by chance.\n\n\n\nConfounding: An association between two variables may reflect not a causal connection between them – but rather the fact that both are caused by a third variable (known as a confound). Such confounding may be at play in our RIL data – we observe that anther–stigma distance is associated with the proportion hybrid seed, but anther–stigma distance is also associated petal area (presumably because both are caused by flower growth), which itself is associated with the proportion of hybrids (Figure 3). So, does petal area or anther stigma distance (or both or neither) cause an increase in proportion of hybrid seed? The answer awaits better data, or at least better analyses (see section on causal inference), but I suspect that petal area, not anther stigma distance “causes” proportion hybrid. Unfortunately, we rarely know the confound, let alone its value. So, interpreting any association as causation requires exceptional caution.\n\n\nReverse causation: Figure 4 shows that pink flowers are more likely to receive a pollinator than are white flowers. We assume this means that pink attracts pollinators– and with the caveat that we must watch out for coincidence and confounds, this conclusion makes sense. However, an association alone cannot tell if pink flowers attracted pollinators or if pollinator visitation turned plants pink. In this case the answer is clear – petal color was measured for RILs in the greenhouse, and there’s no biological mechanism by which a pollinator could change petal color. However, these answers require us to bring in biological knowledge – the data alone can’t tell us which way the effect goes.\n\n\n\n\nPerhaps, as the RIL data suggests, white-flowered parviflora plants are less likely to hybridize with xantiana than are pink-flowered parviflora, so white-flowered plants have less xantiana ancestry (pink flowers cause more gene flow).\nAlternatively, all xantiana are pink-flowered, white-flowered parviflora can be white or pink. So maybe the pink flowers are actually caused by ancestry inherited from xantiana (more gene flow causes pink flowers).\n\n\n\n\n\n\n\n\n\nMaking things independent\n\n\n\n\n\nIf we cannot break the association between anther stigma distance and petal area by genetic crosses maybe we could do so by physical manipulation. For example, we could use tape or some other approach to move stigmas closer to or further from anthers.\n\n\n\n\nCausation without correlation\nNot only does correlation not imply causation, but we can have causation with no association.\n\n\n\n\n\n\n\n\n\nFigure 5: No relationship between height and Value over Replacement Player in the 2024-2025 NBA season.\n\n\n\n\nNo one will doubt that height gives a basketball player an advantage. Yet if we look across all NBA players, we see no relationship between height and standard measures of player success (e.g. salary, or “Value of Replacement Player” etc Figure 5), How can this be? The answer is that to make it to the NBA you have to be very good or very tall (and usually both) – so (6 foot 4, Shai Gilgeous-Alexander) has a value just a bit higher than (6 foot 11) Giannis Antetokounmpo.\nA related, but different issue – known as Countergradient variation is observed in ecological genetics. Here, measures of some trait, like growth rate, are similar across the species range (e.g. between northern and southern populations), but when grown in a common environment, the populations differ (e.g. the northern population grows faster). This might reflect divergence among population as a consequence of natural selection that may favor greater efficiency or acquisition of energy in northern regions.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#making-predictions-is-hard",
    "href": "book_sections/associations.html#making-predictions-is-hard",
    "title": "6. Associations",
    "section": "Making predictions is hard",
    "text": "Making predictions is hard\n\nMaking predictions is hard, especially about the future.\n– Attributed to Yogi Berra\n\nAssociations describe data we have – they do not necessarily apply to other data. Of course, understanding such associations might help us make predictions, but we must consider the range and context of our data.\n\n\n\n\nTheir are different kinds of predictions we might want to make.\n\nWe may want to predict what we would expect for unsampled individuals from the same population as we are describing. In this case, a statistical association can be pretty useful.\nWe may want to predict what we would expect for individuals from a different population than what we are describing. In this case, a statistical association might help, but need some care.\nWe may want to predict what we would expect if we experimentally changed one of the value of an explanatory variable (e.g. if “I experimentally decreased anther-stigma distance, would plants set more hybrid seed?”) This is a causal prediction!\n\nMisalignment between expectations and the reality is a common trope in comedy and drama. For example, hilarity may ensue when an exotic dancer in a firefighter or police costume is mistaken for a true firefighter or policeman (See the scene from Arrested Development on the right (youtube link)). Such jokes show that we have an intuitive understanding that predictions can be wrong, and that the context plays a key role in our ability to make good predictions.\nWe again see such a case in our RIL data - leaf water content reliably predicts the proportion of hybrid seed set at three experimental locations, but is completely unrelated to proportion of hybrid seed at Upper Sawmill Road (location: US, Figure 6).\n\n\nCode to make the plot, below.\nfilter(ril_data, !is.na(location)) |&gt; \n  ggplot(aes(x= lwc,y =prop_hybrid))+\n  facet_wrap(~location,labeller = \"label_both\",nrow=1)+\n  geom_point(size = 2, alpha = .2)+\n  geom_smooth(method = \"lm\",se = FALSE, linewidth = 2)+\n  labs(x = \"Leaf water content\", \n       y = \"Proportion hybrid\")+\n  scale_x_continuous(breaks = seq(.78,.88,.04))\n\n\n\n\n\n\n\n\nFigure 6: Predictions might not generalize. The proportion of hybrid seed set reliably decreases with leaf water content in three locations (GC, LB, SR), but at the upper sawmill road site (US), there is no clear relationship. This cautions against assuming generalizable predictive patterns across environments.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#there-is-still-value-in-finding-associations",
    "href": "book_sections/associations.html#there-is-still-value-in-finding-associations",
    "title": "6. Associations",
    "section": "There is still value in finding associations",
    "text": "There is still value in finding associations\nThe caveats above are important, but they should not stop us from finding associations. With appropriate experimental designs, statistical analyses, biological knowledge, and humility in interpretation, quantifying associations is among the most important ways to summarize and understand data.\nThe following sections provide the underlying logic, mathematical formulas, and R functions to summarize associations.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations.html#lets-get-started-with-summarizing-associations",
    "href": "book_sections/associations.html#lets-get-started-with-summarizing-associations",
    "title": "6. Associations",
    "section": "Let’s get started with summarizing associations!",
    "text": "Let’s get started with summarizing associations!\nThe following sections introduce how to summarize associations between variables by:\n\nDescribing associations between a categorical explanatory and numeric response variable including differences in conditional means, and Cohen’s D, and tools for visualizing associations between a categorical explanatory variable and a numeric response.\n\nDescribing associations between two categorical variables, including differences in conditional proportions, and the covariance.\n\nDescribing associations between two continuous variables, including the covariance and the correlation.\n\nThen we summarize the chapter, present practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources.\nLuckily, these summaries are remarkably similar, so much of the learning in each section of this chapter reinforces what was learned in the others.",
    "crumbs": [
      "6. Associations"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html",
    "href": "book_sections/associations/cat_cont.html",
    "title": "• 6. Categorical + numeric",
    "section": "",
    "text": "Summarizing associations: Difference in conditional means\nWe might expect that parviflora plants known to have attracted a pollinator would produce more hybrid seeds than those that were not. After all, pollen must be transferred for hybridization to occur, and visits from pollinators are the main way this happens. That seems biologically reasonable — but in statistics, such expectations must be tested with actual data.\nIn this section, we explore how to visualize and quantify associations between a categorical explanatory variable (e.g., whether a plant was visited by a pollinator) and a numeric response variable (e.g., the proportion of that plant’s seeds that are hybrids). We’ll see how group means and other summaries can reveal patterns in the data — and how to interpret what those patterns might mean biologically.\nIn statistics, we often differentiate between a\nFor example, for our RIL data planted at site GC, the “grand mean” proportion of hybrids formed across all RILs is around 0.15. .\nThe mean proportion of hybrids formed across all RILs is around 0.15.\ngc_rils |&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\ngrand_mean_prop_hybrid\n\n\n\n\n0.1505776\nSimilarly, the means of prop_hybrid conditional on visitation status are around 0.07 for flowers that “weren’t visited” and 0.36 for those that were visited.\ngc_rils |&gt;\n  group_by(visited)|&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\nvisited\ngrand_mean_prop_hybrid\n\n\n\n\nFALSE\n0.0737613\n\n\nTRUE\n0.3611111\nA common summary of the association between a categorical explanatory variable and a numerical response is the difference in conditional means across groups. In this case, the difference in conditional means is approximately 0.29.\nmean_visited &lt;- gc_rils |&gt;\n  filter(visited)|&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\n\nmean_notvisited &lt;- gc_rils |&gt;\n  filter(!visited)|&gt;\n  summarise(grand_mean_prop_hybrid = mean(prop_hybrid, na.rm =TRUE))\n\n(mean_visited   - mean_notvisited) |&gt; pull() |&gt; round(digits = 3)\n\n[1] 0.287",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-difference-in-conditional-means",
    "href": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-difference-in-conditional-means",
    "title": "• 6. Categorical + numeric",
    "section": "",
    "text": "Grand mean: The overall mean of a variable. and the\nConditional mean: The mean of one variable given the value of one (or more) other variables. With a single categorical variable, this is simply the group means.",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-cohens-d",
    "href": "book_sections/associations/cat_cont.html#cat_cont_summarizing-associations-cohens-d",
    "title": "• 6. Categorical + numeric",
    "section": "Summarizing associations: Cohen’s D",
    "text": "Summarizing associations: Cohen’s D\nAbove, we found that on average, visited plants produced 0.287 more hybrids than unvisited ones. That might seem like a big difference — but raw differences can be hard to interpret on their own. Is 0.287 a lot? A little? To better understand how meaningful that difference is, we can compare it to the variability in hybrid seed production. Cohen’s D helps us do just that — it standardizes the difference in means by the vraiabiliuty within groups (the pooled standard deviation), allowing for more intuitive comparisons across studies and systems. For this dataset, that gives us a D of 1.4 (see calculation below) — a very large effect size (see guide in margin). This suggests that being visited (during our observation window) is strongly associated with producing more hybrid seeds.\n\n\nThere aren’t hard and fast rules for interpreting Cohen’s D — this varies by field — but the rough guidelines are presented below. Our observed Cohen’s D of 1.4 is very large.\n\n\n\nSize\nRange of Cohen’s D\n\n\n\n\nNot worth reporting\n&lt; 0.01\n\n\nTiny\n0.01 – 0.20\n\n\nSmall\n0.20 – 0.50\n\n\nMedium\n0.50 – 0.80\n\n\nLarge\n0.80 – 1.20\n\n\nVery large\n1.20 – 2.00\n\n\nHuge\n&gt; 2.00\n\n\n\n\nCohen’s D - the difference in group means divided by the “pooled standard deviation” allows us to better interpret such difference.\n\nThe pooled standard deviation is simply the standard deviation of observations from their group mean. We can find it in R as follows:\n\n\n# finding the pooled standard deviation\npooled_sd &lt;- gc_rils |&gt;\n  group_by(visited)|&gt;\n  mutate(diff_from_mean = prop_hybrid - mean(prop_hybrid) )|&gt;\n  ungroup()|&gt;\n  summarise(sd_group = sd(diff_from_mean)) |&gt;\n  pull()\n\n# Print this out\nsprintf(\"The pooled sd is %s\",round(pooled_sd, digits = 3))\n\n[1] \"The pooled sd is 0.2\"\n\ncohensD &lt;- (mean_visited   - mean_notvisited) /  pooled_sd \n\n# Print this out\nsprintf(\"Cohen's D is (%s - %s)/(%s) = %s\",\n        round(pooled_sd, digits = 3),\n        round(pull(mean_visited)   , digits = 3),\n        round(pull(mean_notvisited), digits = 3),\n        round(pull(cohensD) , digits = 3)\n        )\n\n[1] \"Cohen's D is (0.2 - 0.361)/(0.074) = 1.439\"",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/cat_cont.html#cat_cont_visualizing-a-categorical-x-and-numeric-y",
    "href": "book_sections/associations/cat_cont.html#cat_cont_visualizing-a-categorical-x-and-numeric-y",
    "title": "• 6. Categorical + numeric",
    "section": "Visualizing a categorical x and numeric y",
    "text": "Visualizing a categorical x and numeric y\nVisualizing the difference between means is surprisingly difficult.Visualizing the difference between means is surprisingly difficult. One particular concern is overplotting — because categorical variables have only a few possible values on the x-axis, data points can stack or overlap, which can obscure patterns in the data.\nBelow I work through a brief slide show revealing some challenges and some solutions.",
    "crumbs": [
      "6. Associations",
      "• 6. Categorical + numeric"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html",
    "href": "book_sections/associations/two_categorical_vars.html",
    "title": "• 6. Two categorical vars",
    "section": "",
    "text": "Unconditional proportions\nFigure 1: The proportion of plats that did (light grey) or did not (black) receive a visit from a pollinator.\nBefore describing associations between categorical variables, let us revisit our univariate summaries. The proportion of pink flowers (or plants receiving visits) is simply the number of pink flowers (or plants receiving visits) divided by the total number of plants whose petal color is known (or the number of plants with pollinator observation data).\nA proportion is essentially a mean where one outcome (e.g., pink flowers or being visited) is set to 1 and the other (e.g., white flowers or not being visited) is set to 0. Because visited is logical and R converts TRUE to 1 and FALSE to 0 when making a logical numeric, we can find this with the mean() function in R:\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(mean_visits))|&gt;     # remove NAs\n  summarise(n_pink       = sum(as.numeric(petal_color == \"pink\")),\n            n_visited    = sum(as.numeric(mean_visits &gt; 0)),\n            n            = n(),\n            prop_pink    = mean(as.numeric(petal_color == \"pink\")),\n            prop_visited = mean(as.numeric(visited))) |&gt; \n  kable(digits = 4)                # for pretty formatting (ignore)\n\n\n\n\nn_pink\nn_visited\nn\nprop_pink\nprop_visited\n\n\n\n\n49\n24\n91\n0.5385\n0.2637\nSo our unconditional proportions – that is, our proportions without considering the other variable are:",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html#unconditional-proportions",
    "href": "book_sections/associations/two_categorical_vars.html#unconditional-proportions",
    "title": "• 6. Two categorical vars",
    "section": "",
    "text": "Proportion pink-flowered, \\(P_\\text{pink} = \\frac{49}{91} = 0.5385\\).\n\nProportion visited, \\(P_\\text{visited} = \\frac{24}{91} = 0.2637\\).",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/two_categorical_vars.html#associations-between-categorical-variables",
    "href": "book_sections/associations/two_categorical_vars.html#associations-between-categorical-variables",
    "title": "• 6. Two categorical vars",
    "section": "Associations between categorical variables",
    "text": "Associations between categorical variables\n\n\n\n\n\n\n\n\n\nFigure 2: The association between petal color and pollinator visitation. Petal color is on the x-axis and visit status is shown within bars. We see that pink-flowered plants are more likely to receive a visit from a pollinator.\n\n\n\n\nFigure 2 clearly shows that pink-flowered parviflora RILs planted at GC are more likely to be visited by a pollinator than are white-flowered RILs. At least, this is clear to me! Examine the plot yourself and\n\nArticulate what feature of this plot shows the point above,\n\nDescribe what the plot would look like if there was no association between petal color and pollinator visitation.\n\n\n\n\n\n\nA basic expectation from probability theory is that if two binary variables are independent (i.e. there is no statistical association between them), then the proportion of observations with both A and B equals the product of their proportions:\n\\[P_{AB|\\text{independence}} = P_{A} \\times P_{B}\\]\n\n\nThis is called the multiplication rule.\nTo evaluate how far our data deviate from independence, we can compare the actual joint proportion to what we’d expect if the two variables were independent. Assuming independence, \\(\\frac{49}{91}\\times \\frac{24}{91} =  0.142\\) of plants would be pink-flowered and receive a visit from a pollinator, but in reality nearly a quarter of our plants are pink-flowered and visited!\nTwo standard summaries of associations between categorical variables – conditional proportions and covariance – can be used to quantify this deviation from expectations under independence.\n\n\nDeviation from Independence: Conditional proportions\nPerhaps the most straightforward summary of an association between categorical variables is the conditional proportion. This is the proportion of a given outcome, calculated separately for each value of the explanatory variable. Because a proportion is essentially a mean, a conditional proportion is essentially a conditional mean – so this calculation and logic follows that in the previous section.\nAs we see in our example, this calculation uses the same approach as above, but grouped by petal color.\n\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(visited))|&gt;     # remove NAs\n  group_by(petal_color)                            |&gt;\n  summarise(n_visited    = sum(as.numeric(visited)),\n            n            = n(),\n            prop_visited = mean(as.numeric(visited))) |&gt; \n  kable(digits = 4)                # for pretty formatting (ignore)\n\n\n\n\npetal_color\nn_visited\nn\nprop_visited\n\n\n\n\npink\n22\n49\n0.4490\n\n\nwhite\n2\n42\n0.0476\n\n\n\n\n\nBy conventional notation, we write conditional proportions as \\(P_\\text{A|B}\\), meaning ‘the proportion of A given B’ — where A is the outcome, B is the explanatory variable, and \\(|\\) means “given”. So:\n\n\nIn probability theory \\(|\\) means “given”. In R \\(|\\) means “or”. This is unfortunate and I am sorry. I wish I could change this.\n\nThe proportion of pink flowers receiving a visit from a pollinator is: \\(P_\\text{visited|pink} = \\frac{22}{49} = 0.449\\).\n\nThe proportion of white flowers receiving a visit from a pollinator is: \\(P_\\text{visited|white} = \\frac{2}{42} = 0.0476\\).\n\nIn summary, pink-flowered plants at site GC are roughly 10 times more likely to attract a pollinator than are white-flowered plants!\nWith these conditional proportions we can generalize the multiplication rule to non-independent variables:\n\\[P_{AB} = P_{B} \\times P{A|B}\\]\nApplying this to our case recovers our actual observations!\n\\[P_\\text{pink and visited} = P_\\text{pink} \\times P_\\text{visited|pink} = \\frac{49}{91}\\times \\frac{22}{49} =\\frac{22}{91} = 0.24\\]\n\n\nDividing the conditional proportions for two groups is technically called the “relative risk”.\n\n\n\nDeviations from Independence - The Covariance\nOur final summary of the association between categorical variables is the covariance. There are two ways to calculate the covariance. For now, we focus on the simplest way, and revisit this in the next section.\n\n\nThis formula is slightly wrong because it implicitly has a denominator of \\(n\\), not \\(n-1\\). To get the precise covariance, multiply this by \\(\\frac{n}{n-1}\\) (this is known as Bessel’s correction). But when \\(n\\) is big, this is close enough.\n\nThe first estimate of the covariance is the difference between observations and expectations under independence – i.e. \\(\\text{Covariance}_{A,B} = P_{AB}-P_{A} \\times P_{B}\\).\n\n\ngc_rils |&gt;\n  filter(!is.na(petal_color) & !is.na(visited))|&gt;     # remove NAs\n  mutate(pink_and_visited = petal_color == \"pink\" & visited,\n         pink_and_visited01 = as.numeric(pink_and_visited ))|&gt;    # observed joint proportion\n  summarise(n            = n(),\n            prop_pink_and_visited = mean(pink_and_visited01),\n            prop_visited = mean(as.numeric(visited)),\n            prop_pink    = mean(as.numeric(petal_color == \"pink\")),\n            indep_expect = prop_visited* prop_pink,\n            approx_cov   = prop_pink_and_visited - indep_expect,\n            exact_cov    = approx_cov * n / (n-1),\n            cov_function = cov(as.numeric(petal_color == \"pink\"), \n                               as.numeric(visited)))|&gt;\n  kable(digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\nprop_pink_and_visited\nprop_visited\nprop_pink\nindep_expect\napprox_cov\nexact_cov\ncov_function\n\n\n\n\n91\n0.2418\n0.2637\n0.5385\n0.142\n0.0997\n0.1009\n0.1009\n\n\n\n\n\n\n\nNote that the code above introduced R’s cov() function to find the covariance.\nCovariance gives us a numerical measure of how far our data deviate from what we’d expect under independence. In this case, the value is 0.10 — but is that meaningful? We’ll build up more intuition for interpreting covariances as we shift to continuous variables in the next section.\n\n\nAdditional summaries of associations between categorical variables.\nAt this point many textbooks would introduce two other standard summaries – odds ratios and relative risk (calculated above). I am not spending much time on them here. That is not because they are not useful (they are) – but because\n\nThey can get complicated.\n\nThey don’t lead naturally to the next steps in our learning journey.\n\nFeel free to read more about each on Wikipedia (links above) or in conversation with your favorite large language model.",
    "crumbs": [
      "6. Associations",
      "• 6. Two categorical vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html",
    "href": "book_sections/associations/cont_cont.html",
    "title": "• 6. Two numeric vars",
    "section": "",
    "text": "The covariance\nThe covariance can also be used to describe the association between two numeric variables. For example, in our Clarkia RIL data, we could describe the association between \\(\\text{log}_{10}\\) petal area and the proportion of hybrid seeds using a covariance. As with two categorical variables, the covariance between two numeric variables reflects how much the observed association differs from what we’d expect if the variables were independent. There are two ways to calculate covariance — I introduce both here because each provides a different lens for understanding the concept, and each connects deeply to core ideas in statistics.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#cont_cont_the-covariance",
    "href": "book_sections/associations/cont_cont.html#cont_cont_the-covariance",
    "title": "• 6. Two numeric vars",
    "section": "",
    "text": "The covariance as a deviation from expectations\nIn the previous section, I introduced the covariance as the difference between the proportion of observations with a specific pair of values for two variables (e.g., pink flowers and being visited by a pollinator) and how frequently we would expect to see this pairing if the variables were independent: \\(\\text{Covariance}_{A,B} = (P_{AB}-P_{A} \\times P_{B})\\). Because we can think of proportions as a mean, we can use this same math to describe the covariance of two numeric variables, X and Y, as the difference between the mean of the products and the product of the means:\n\\[\\text{Covariance}_{X,Y} = (\\overline{XY}-\\overline{X} \\times \\overline{Y})\\]\n\n\nAs in the previous section this formula is slightly wrong because it implicitly has a denominator of \\(n\\), not \\(n-1\\). We apply Bessel’s correction to get the precise covariance (multiplying our answer by \\(\\frac{n}{n-1}\\)). But when \\(n\\) is big, this is close enough.\nSo, we can find the covariance between (\\(\\text{log}_{10}\\)) petal area and the proportion of hybrid seeds as the mean of a plant’s (\\(\\text{log}_{10}\\)) petal area times its proportion of hybrid seeds minus the mean (\\(\\text{log}_{10}\\)) petal area times the mean proportion of hybrid seeds, which equals 0.00756 (after applying Bessel’s correction).\n\ngc_rils |&gt;\n  filter(!is.na(log10_petal_area_mm), !is.na(prop_hybrid))|&gt;\n  summarise(mean_product        = mean(log10_petal_area_mm * prop_hybrid),\n            product_of_mean     =  mean(log10_petal_area_mm) * mean(prop_hybrid),\n            approx_covariance   = mean_product - product_of_mean,\n            actual_covariance   = approx_covariance * n() /(n() - 1))\n\n\n\n\nThe covariance as the mean of cross products\nAlternatively, we can think of the covariance as how far an individual’s value of X and Y jointly differ from their means. In this formulation,\n\nFind the deviation of X and Y from their means for each individual– \\((X_i-\\overline{X})\\), and \\((Y_i-\\overline{Y})\\), respectively (Figure 1, left).\n\nTake the product of these values to find the cross product (the area of a given rectangle in Figure 1, left).\n\nSum them to find the sum of cross products (Figure 1, right, top).\n\nDivide by the sample size minus one (Figure 1, right, bottom).\n\n\n\nThe equation for the covariance \\(\\text{Cov}_{X,Y} = \\frac{\\Sigma{(X_i-\\overline{X})(Y_i-\\overline{Y})}}{(n-1)}\\) should remind you of the equation for the variance \\(\\text{Var}_{X} = \\frac{\\Sigma{(X_i-\\overline{X})(X_i-\\overline{X})}}{(n-1)}\\) (compare Figure 1 to Figure 2 from 5. Summarizing variability). In fact the variance is simply the covariance of a variable with itself. See our section on summarizing variability for a refresher link. In fact you, can calcualte the variance as the mean of the square minus the sqaure of the mean.\nThis essentially finds the mean cross products, with Bessel’s correction:\n\\[\\text{Covariance }_{X,Y} = \\frac{\\Sigma{(X_i-\\overline{X})(Y_i-\\overline{Y})}}{(n-1)}\\]\n\n\n\n\n\n\n\n\nFigure 1: An animation to help understand the covariance. Left: We plot each point as the difference between x and y and their means. The area of that rectangle is the cross product. Middle: Shows how these cross products accumulate. Right: The cummulative sum of cross products and the running covariance estimate. The lower plot (covariance) is simply the top plot divided by (x-1).\n\n\n\n\n\nThe flipbook below works you through how to conduct these calculations:\n\n\n\n\n\n\nThe cov() function\nBoth ways of computing the covariance — as the mean of cross-products and as the difference between the product of means and mean of products — are helpful for understanding association. But students are practical and often ask: “Which of these formulae should we use to calculate the covariance?” There are a few answers to this question — the first is “it depends,” the second is “whichever you like,” and the third is “neither, just use the cov() function in R.” Here’s how:\n\n\nThe use = \"pairwise.complete.obs\" argument tells R to ignore NA values when calculating the covariance — just like na.rm = TRUE does when calculating the mean. You can use this argument or filter out NA values first.\n\ngc_rils |&gt;\n  summarise(covariance = cov(log10_petal_area_mm, prop_hybrid, use = \"pairwise.complete.obs\"))\n\n# A tibble: 1 × 1\n  covariance\n       &lt;dbl&gt;\n1    0.00756",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#cont_cont_the-correlation",
    "href": "book_sections/associations/cont_cont.html#cont_cont_the-correlation",
    "title": "• 6. Two numeric vars",
    "section": "The correlation",
    "text": "The correlation\nMuch like the variance and the difference in means, the covariance is a very useful mathematical description, but its biological meaning can be difficult to interpret and communicate. We therefore usually present the correlation coefficient (represented by the letter, r) – a summary of the strength and direction of a linear association between two variables. This also corresponds to how closely the points fall along a straight line in a scatterplot: the stronger the correlation, the more the points cluster along a line (positive or negative).\n\nLarge absolute values of r indicate that we can quite accurately predict one variable from the other (i.e. points are near a line on a scatterplot).\nr values near zero mean that we cannot accurately predict values of one variable from another (i.e. points are not near a line on a scatterplot).\n\nThe sign of r describes if the values increase with each other (\\(r &gt; 0\\), a positive slope), or if one variable decreases as the other increases ($ r &lt; 0$, a negative slope).\n\nMathematically r is simply the covariance divided by the product of standard deviations (\\(s_X\\) and \\(s_Y\\)), and we can find it in R with the cor() function:\n\\[r_{X,Y} = \\frac{\\text{Covariance}_{X,Y}}{s_X \\times s_Y}\\]\n\ngc_rils |&gt;\n  filter(!is.na(log10_petal_area_mm), !is.na(prop_hybrid))|&gt;\n  summarise(covariance   = cov(log10_petal_area_mm, prop_hybrid),\n            cor_from_cov = covariance / (sd(log10_petal_area_mm) * sd(prop_hybrid)),\n            cor_from_function = cor(log10_petal_area_mm, prop_hybrid))\n\n# A tibble: 1 × 3\n  covariance cor_from_cov cor_from_function\n       &lt;dbl&gt;        &lt;dbl&gt;             &lt;dbl&gt;\n1    0.00756        0.317             0.317\n\n\n\n\n\n\n\nSize\nRange of \\(|r|\\)\n\n\n\n\nNot worth reporting\n&lt; 0.005\n\n\nTiny\n0.005 – 0.10\n\n\nSmall\n0.01 – 0.20\n\n\nMedium\n0.2 – 0.35\n\n\nLarge\n0.35 – 0.50\n\n\nVery large\n0.50 – 0.75\n\n\nHuge\n\\(&gt; 0.75\\)\n\n\n\nAs in Cohen’s D, what is a “large” or “small” correlation coefficient depends on the study, the question and the field of study, but there are rough guides (see table on right). So our observed correlation between \\(log_{10}\\) petal area and proportion hybrid is worth paying attention to, but not massive.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/cont_cont.html#coming-up-next",
    "href": "book_sections/associations/cont_cont.html#coming-up-next",
    "title": "• 6. Two numeric vars",
    "section": "Coming up next",
    "text": "Coming up next\nThese summaries — covariance and correlation — give us tools to describe how two numeric variables relate. Later, we’ll return to these ideas in the context of linear models, where we formalize the idea of one variable predicting another.",
    "crumbs": [
      "6. Associations",
      "• 6. Two numeric vars"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html",
    "href": "book_sections/associations/summarizing_associations.html",
    "title": "• 6. Association Summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nA cartoon on correlation from xkcd. The original rollover text says: “Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing look over there”. See this link for a more detailed explanation.\nAssociations reveal how variables relate to one another — whether one tends to increase with another, differ across groups, or cluster. Differences in conditional means (or proportions) describe how a numeric (or categorical) response variable varies across levels of a categorical explanatory variable. For two numeric variables, covariance captures how deviations from their means align, and correlation standardizes this to a unitless scale between –1 and 1. While these summaries can highlight patterns, interpretation requires care: strong associations don’t necessarily imply causation, and predictions may not hold across contexts or datasets.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_chapter-summary",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_chapter-summary",
    "title": "• 6. Association Summary",
    "section": "",
    "text": "Chatbot tutor\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_practice-questions",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_practice-questions",
    "title": "• 6. Association Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the ril data, cleaned it some, an have started some of the code!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ1) Extend the analysis above to examine the association between leaf water content (lwc) and the proportion of hybrid seeds (prop_hybrid). The correlation between lwc and prop_hybrid is: -0.202-0.203-0.000914-0.000403\nQ2) Based on the analysis above, which variable – leaf water content (lwc), or petal area (log10_petal_area_mm) is more plausibly interpreted as influencing proportion hybrid seed set (prop_hybrid)?\n\n Equally likely — because the absolute values of their correlation coefficients are similar Petal area — because it has the stronger correlation coefficient Neither — the covariances are both near zero Petal area There is a substantial association, and because these are experimental RILs, it's plausible that pollinators are attracted to larger petals — not low leaf water content. There is no relevant information here — correlation does not imply causation\n\nQ3) Based on the observed negative association between leaf water content and proportion hybrid seed set, which explanation best accounts for this pattern?\n\n Chance — strange associations sometimes appear randomly. Reverse causation — pollinator visits might reduce leaf water content. A direct causal link — pollinators are attracted to plants with dry leaves. Confounding — low leaf water content might be genetically or physiologically linked with a trait that influences pollinator attraction (e.g., it might be negatively associated with petal area) and ultimately hybrid seed set.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe set of questions below focuses on comparing the association between petal color and pollinator visitation to the association between petal color and proportion hybrid seed. Use the webR console above to work through these!\nQ4) The difference in conditional mean hybrid proportion between pink and white flowers is: \nQ5) The pooled standard deviation of hybrid proportion between pink and white flowers is: 0.2320.1930.214\nQ6) Which trait is more strongly associated with petal color — the proportion of hybrid seeds or visits from a pollinator (visits)?\n\n Pollinator visits — Pink flowers had about 0.6 more visits than white flowers, but only about 0.18 greater proportion of hybrid seeds. Hybrid seeds — Pink flowers had about 4.7 times as many hybrid seeds as white flowers, but only about 2.6 times as many visits. Hybrid seeds — Cohen’s D for the relationship between petal color and proportion hybrid seeds was large, while Cohen’s D for the relationship between petal color and visits was medium. You cannot compare strength of associations when the response variables are measured on different scales.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7 SETUP We collected 131 plants (74 parviflora, 57 xantiana) from a natural hybrid zone between xantiana and parviflora at Sawmill Road. We then genotyped these plants at a chloroplast marker that distinguishes between chloroplasts originating from parviflora and xantiana. All 74 parviflora plants had a parviflora chloroplast, while 49 of the 57 xantiana plants had a xantiana chloroplast (the remaining 8 had a parviflora chloroplast).\nQ7A) If having a xantiana chloroplast and being a xantiana plant were independent, what proportion of plants would you expect to be xantiana and have a xantiana chloroplast? \n\n\nRefresher on the multiplication rule for independent events\n\nIf two binary variables are independent, the expected joint proportion (i.e. the probability of A and B) is the product of their proportions:\n\\[ P(A \\text{ and } B) = P(A) \\times P(B) \\]\n\nQ7B) Quantify the difference between the proportion of plants that are xantiana and have xantiana chloroplasts vs. what we expect if these two binary variables were independent. \nQ7C) What is the covariance between being a xantiana plant and having a xantiana chloroplast? Hint: remember Bessel’s correction. \n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) In the code above, I calculated the correlation and covariance between lwc and prop_hybrid using their mathematical formulas. However, my calculated values don’t match those returned by cor(). Why not?\n\n The manual method failed to remove all rows with missing values — while cov() and cor() used pairwise.complete.obs, the custom code did not. There is a mistake in the correlation formula — the covariance should be divided by the product of the means, not the standard deviations. R sometimes has the wrong formulae – that's why I always type the formula's in myself. The standard deviations used were incorrect because sd() doesn't apply Bessel's correction. The discrepancy is due to numerical precision — it's expected and not worth worrying about.\n\n\n\n\n\n\n\n\n\n\n\n\nQ9 SETUP Consider the plots above\nQ9A) In which plot are x and y most tightly associated? abcd\nQ9B) In which plot are x and y most tightly linearly associated? abcd\nQ9C) In which plot do x and y have the largest correlation coefficient? abcd\nQ9C) In which plot are does x do the worst job of predicting y? abcd",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_glossary-of-terms",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_glossary-of-terms",
    "title": "• 6. Association Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n🔗 1. Types of Association\n\nAssociation: A relationship or pattern between two variables, without assuming causation.\nCorrelation: A numerical summary of how two variables move together.\n\nPositive: As one increases, the other tends to increase.\n\nNegative: As one increases, the other tends to decrease.\n\n\nCausation: A relationship in which changes in one variable directly produce changes in another.\n\n\n\n\n⚖️ 2. Categorical Associations\n\nConditional Proportion: The proportion of a category (e.g., visited flowers) within levels of another variable (e.g., pink or white petals).\n\nWritten as \\(P(A|B)\\), the probability of A given B.\n\nMultiplication Rule: If two variables are independent, then \\(P(A \\text{ and } B) = P(A) \\times P(B)\\).\nRelative Risk: The ratio of conditional proportions between two groups.\nConfounding Variable: A third variable that creates a false appearance of association between two others.\n\n\n\n\n🔢 3. Numeric Associations\n\nCovariance (cov()): Measures how two numeric variables co-vary.\n\nPositive: variables increase together.\n\nNegative: one increases as the other decreases.\n\nSensitive to scale.\n\nCross Product: For two variables, the product of their deviations from their means:\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\nCorrelation Coefficient (cor()): A unitless summary of linear association, ranging from -1 to 1.\n\\(r = \\frac{\\text{Cov}_{X,Y}}{s_X s_Y}\\)\n\nr ≈ 0: No linear relationship\n\nr &gt; 0: Positive linear relationship\n\nr &lt; 0: Negative linear relationship\n\n\n\n\n\n📏 4. Comparing Group Means\n\nConditional Mean: The average of a numeric variable within each group of a categorical variable.\nDifference in Means: A common summary of how a numeric variable differs across groups.\nCohen’s D: Standardized difference between two group means.\n\\(D = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_{pooled}}\\)\nPooled Standard Deviation: A weighted average of within-group standard deviations, used in Cohen’s D.\n\n\n\n\n📈 5. Visual Summaries of Associations\n\nScatterplot: Plots individual observations for two numeric variables. Good for spotting trends and calculating correlation.\nBoxplot: Shows distributions (medians, IQRs) across groups.\nBarplot of Conditional Proportions: Visualizes proportions of one categorical variable within levels of another.\nSina Plot: A jittered density-style plot used to show distributions of numeric values within categories, especially useful when overplotting is an issue.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_key-r-functions",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_key-r-functions",
    "title": "• 6. Association Summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n\n📊 Visualizing Associations\n\nstat_summary(): Adds summary statistics like means and error bars to plots.\ngeom_smooth(): Adds a trend line to scatterplots.\n\n\n\n\n📈 Summarizing Associations Between Variables\n\ngroup_by() ([dplyr]): Groups data for grouped summaries like conditional proportions or means.\nsummarise() ([dplyr]): Summarizes multiple rows into a single value, e.g., a mean, covariance, or correlation.\n\nmean() ([base R]): Computes means (or proportions). In this chapter we combine this with group_by() to find conditional means (or conditional proportions).\ncov(): Calculates covariance between two numeric variables.\ncor(): Calculates the correlation coefficient.\n\nWe often combine these below with the following chain of operations.\n- For conditional means: data|&gt;group_by()|&gt;summarize(mean()).\n- For associations: data |&gt;group_by()|&gt;summarize(cor()).",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_r-packages-introduced",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_r-packages-introduced",
    "title": "• 6. Association Summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nGGally: Extends ggplot2 with convenient functions for exploring relationships among multiple variables. The ggpairs() function produces a matrix of plots showing pairwise associations, including histograms, scatterplots, and correlation coefficients.\nggforce: Provides advanced geoms for ggplot2. This chapter uses geom_sina() to reduce overplotting by jittering points while preserving density.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/associations/summarizing_associations.html#summarizing_associations_additional-resources",
    "href": "book_sections/associations/summarizing_associations.html#summarizing_associations_additional-resources",
    "title": "• 6. Association Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nOther web resources:\n\nRegression, Fire, and Dangerous Things (1/3): A fantastic essay about challenges in going from correlation to causation.\n\nSpurious correlations: A humorous collection of weird correlations from the world.\n\nGuess the correlation: A fun video game in which you see a plot and must guess the correlation. This is great for building an intuition about the strength of a correlation.\n\nVideos:\n\nCorrelation Doesn’t Equal Causation: Crash Course Statistics #8.\nCalling Bullshit has a fantastic set of videos on correlation and causation.\n\nCorrelation and Causation: “Correlations are often used to make claims about causation. Be careful about the direction in which causality goes. For example: do food stamps cause poverty?”\n\nWhat are Correlations? :“Jevin providers an informal introduction to linear correlations.”\n\nSpurious Correlations?: “We look at Tyler Vigen’s silly examples of quantities appear to be correlated over time), and note that scientific studies may accidentally pick up on similarly meaningless relationships.”\n\nCorrelation Exercise” “When is correlation all you need, and causation is beside the point? Can you figure out which way causality goes for each of several correlations?”\nCommon Causes: “We explain how common causes can generate correlations between otherwise unrelated variables, and look at the correlational evidence that storks bring babies. We look at the need to think about multiple contributing causes. The fallacy of post hoc propter ergo hoc: the mistaken belief that if two events happen sequentially, the first must have caused the second.”\n\nManipulative Experiments: “We look at how manipulative experiments can be used to work out the direction of causation in correlated variables, and sum up the questions one should ask when presented with a correlation.",
    "crumbs": [
      "6. Associations",
      "• 6. Association Summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html",
    "href": "book_sections/linear_models.html",
    "title": "7. Linear Models",
    "section": "",
    "text": "Statistical models are not scientific models\nIn this section, we’ll introduce statistical models as simplified descriptions of the world. You’ll notice that you’re already familiar with some simple models. For example, the mean and variance provide a basic model for a single numeric variable, while a conditional mean and pooled variance describe a numeric response variable with a categorical explanatory variable.\nThis is a BIOstatistics book. It is written by and for biologists interested in biological ideas. We are inspired by biological questions generated by scientific understanding and scientific models of the world. However, a major way in which we evaluate such scientific models is via statistical models and hypotheses. Confusing a statistical model for a scientific model is a common and understandable mistake that we should avoid. Scientific models and statistical models are different:\nBecause statistical models know nothing about science it is our job to build scientific studies best suited for clean statistical interpretation, build statistical models that best represent our biological questions, and interpret statistical results as statistical. We must always recenter biology in interpreting any statistical outcome.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#statistical-models-are-not-scientific-models",
    "href": "book_sections/linear_models.html#statistical-models-are-not-scientific-models",
    "title": "7. Linear Models",
    "section": "",
    "text": "Scientific models are based on our understanding of the science – in this case biology. Biological models come from us making simplified abstractions of complex systems (like considering predator-prey interactions, plant pollination, cancer progression, or meiosis (Figure 1 A)). Great scientific models explain what we see, make interesting predictions, and are consistent with our broader scientific understanding.\nStatistical models on the other hand, are mathematical ways to describe patterns in data (e.g. Figure 1 B). Statistical models know nothing about Lotka-Voltera, pollination or human physiology.\n\n\n\nScientific & statistical models: The Clarkia case study\nIn our Clarkia example, the big-picture scientific model is that when parviflora came back into contact with its close relative, xantiana, it evolved traits — such as smaller petals — to avoid producing hybrids. No single statistical model or study fully captures this scientific model. Instead, we design experiments to evaluate pieces of the model. For example, we:\n\nCompare petal area between parviflora plants from populations that occur with xantiana (sympatric populations) and those from populations far away from xantiana (allopatric populations).\nConduct an experiment by planting individuals from sympatric and allopatric parviflora populations in the same environment as xantiana, and comparing the amount of hybrid seed set by plants from each origin.\nGenerate Recombinant Inbred Lines (RILs) between sympatric and allopatric parviflora populations, and examine whether petal area is associated with the proportion of hybrid seeds produced.\n\nAs you can see, statistical models don’t “know” anything about biology — they simply describe patterns in data. It’s up to us, as biologists, to design experiments carefully, choose the right statistical models, and interpret results in light of our biological hypotheses.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#linear-models",
    "href": "book_sections/linear_models.html#linear-models",
    "title": "7. Linear Models",
    "section": "Linear models",
    "text": "Linear models\nLinear models are among the most common types of statistical model. Linear models estimate the conditional mean of the \\(i^{th}\\) observation of a uous response variable, \\(\\hat{Y}_i\\) for a (combination) of value(s) of the explanatory variables (\\(\\text{explanatory variables}_i\\)):\n\\[\\begin{equation}\n\\hat{Y}_i = f(\\text{explanatory variables}_i)\n\\end{equation}\\]\n\n\nConditional mean: The expected value of a response variable given specific values of the explanatory variables (i.e., the model’s best guess for the response based on the explanatory variables).\nThese models are “linear” because we get this estimate of the conditional mean, \\(\\hat{Y}_i\\), by adding up all components of the model. That is, each explanatory variable \\(y_{j,i}\\) is multiplied by its effect size \\(b_j\\). So, for example, \\(\\hat{Y}_i\\) equals the parameter estimate for the “intercept”, \\(a\\) plus its value for the first explanatory variable, \\(y_{1,i}\\), times the effect of this variable, \\(b_1\\), plus its value for the second explanatory variable, \\(y_{2,i}\\) times the effect of this variable, \\(b_2\\), and so on for all included predictors.\n\\[\\begin{equation}\n\\hat{Y}_i = a + b_1  y_{1,i} + b_2 y_{2,i} + \\dots{}\n\\end{equation}\\]\n\nOPTIONAL / ADVANCED, FOR MATH NERDS:. If you have a background in linear algebra, it might help to see a linear model in matrix notation.\nThe first matrix below is known as the design matrix. Each row corresponds to an individual, and each entry in the \\(i\\)th row corresponds to that individual’s value for a given explanatory variable. We take the dot product of this matrix and our estimated parameters to get the predictions for each individual. The equation below has \\(n\\) individuals and \\(k\\) explanatory variables. Note that every individual has a value of 1 for the intercept.\n\\[\\begin{equation}\n\\begin{pmatrix}\n    1 & y_{1,1} & y_{2,1} & \\dots  & y_{k,1} \\\\\n    1 & y_{1,2} & y_{2,2} & \\dots  & y_{k,2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & y_{1,n} & y_{2,n} & \\dots  & y_{k,n}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n    a \\\\\n    b_{1}\\\\\n    b_{2}\\\\\n    \\vdots  \\\\\n     b_{k}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    \\hat{Y}_1 \\\\\n    \\hat{Y}_2\\\\\n    \\vdots  \\\\\n    \\hat{Y}_n\n\\end{pmatrix}\n\\end{equation}\\]",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#residuals-and-variance-in-linear-models",
    "href": "book_sections/linear_models.html#residuals-and-variance-in-linear-models",
    "title": "7. Linear Models",
    "section": "Residuals and Variance in Linear Models",
    "text": "Residuals and Variance in Linear Models\nAs with a simple mean, actual observations usually differ from their predicted (conditional mean) values.\nThe difference between an observed value for an individual (\\(Y_i\\)) and the predicted value from the linear model (\\(\\hat{Y}_i\\)) is called the residual, \\(e_i\\):\n\n\nResidual: The difference between an observed value and its predicted value from a linear model (i.e., the conditional mean given that observation’s values for the explanatory variables).\n\\[e_i = Y_i - \\hat{Y}_i\\]\nYou can also rearrange this to think about it the other way around:\n\\[Y_i = \\hat{Y}_i + e_i\\]\nJust like when we summarized variability around a simple mean, we can summarize variability around a model’s predictions by looking at the spread of the residuals. That is, linear models don’t just give us a conditional mean — they also give us a way to estimate how far observations tend to vary around that mean.\nWe calculate the residual variance (and residual standard deviation) of the residuals like this:\n\\[\\text{Residual variance} = \\frac{ \\sum e_i^2}{n-1}\\]\n\n\nA small caveat: The equation for the residual variance is a bit off — the denominator should actually be \\(n - p\\), where \\(p\\) is the number of parameters estimated in our model (including the intercept). But it’s close enough for now. We can worry about being precise later — in much of statistics, being approximately right is good enough.  \nWe can use the residual variance to find the residual standard deviation: \\(\\text{Residual standard deviation} = \\sqrt{\\frac{\\sum e_i^2}{n-1}}\\).\nWe can think of the residual standard deviation as telling us, on average, how far away from their predicted value individuals are expected to be. In fact, when we calculated the pooled standard deviation earlier, we were already doing a special case of what we now call the residual standard deviation — just in the simple situation of two groups.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#assumptions-caveats-and-our-limited-ambitions",
    "href": "book_sections/linear_models.html#assumptions-caveats-and-our-limited-ambitions",
    "title": "7. Linear Models",
    "section": "Assumptions, Caveats and our limited ambitions",
    "text": "Assumptions, Caveats and our limited ambitions\nIn this section, we focus on building and interpreting linear models as descriptions of data. We will put off a formal discussion of what makes a linear model reasonable — and how to diagnose whether a model fits its assumptions — until later. In the meantime, as we build and interpret models, we’ll keep an informal eye out for clues that something might be wrong — but we’ll learn formal tools to evaluate if a specific model meets assumptions of a linear model later.\nThis doesn’t mean that interpreting linear models is more important than evaluating whether they are any good (in fact, in real research, interpreting and evaluating models are inseparable). It just means it’s hard to teach a bunch of interconnected ideas and approaches all at once — and after years of experimentation, I’ve found this approach to be the most successful. That said, before you conduct any serious linear modeling work, you should definitely jump ahead and read PART 3 of this book, where we discuss model assumptions and how to check them carefully.",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models.html#lets-get-started-introducing-linear-models-as-summaries",
    "href": "book_sections/linear_models.html#lets-get-started-introducing-linear-models-as-summaries",
    "title": "7. Linear Models",
    "section": "Let’s get started introducing linear models as summaries!",
    "text": "Let’s get started introducing linear models as summaries!\nThroughout this chapter we will model a single response variable – the proportion of hybrids that a mom produced. As you see below, much of this chapter recasts concepts you have already learned in the framework of a linear model.\n\nWe will start with the simplest linear model – the mean. While we have already introduced the mean, introducing it in the context of a linear model helps us prepare for our next steps!\nWe will then introduce linear models with an explanatory categorical predictor. Starting with a binary predictor (petal color), we will recast our understanding of a conditional mean in the context of a linear model. We will then expand to a categorical explanatory variable with multiple levels (location).\nNext we show how linear models can predict one numeric variable from another – in this case we predict proportion hybrid seed from petal area. Building on our understanding of covariance and correlation, we introduce the slope and intercept.\nFinally we introduce linear models with two predictors. Specifically, we extend beyond what we have learned so far - we build a linear model to predict a numeric response (proportion of hybrid seeds) from a categorical (petal color) and numeric (petal area) predictor.\n\nWe conclude (as usual) with a chapter summary, practice questions, a glossary, a review of R functions and R packages introduced, and present additional resources. On the whole, this chapter extends our ability to summarize data while preparing us for the more sophisticated linear modeling and hypothesis testing we will pursue later in the book.\n\n\n\n\nSuzuki, Y., Endo, M., Cañas, C., Ayora, S., Alonso, J. C., Sugiyama, H., & Takeyasu, K. (2014). Direct analysis of holliday junction resolving enzyme in a DNA origami nanostructure. Nucleic Acids Research, 42(11), 7421–7428. https://doi.org/10.1093/nar/gku320",
    "crumbs": [
      "7. Linear Models"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html",
    "href": "book_sections/linear_models/mean.html",
    "title": "• 7. Mean",
    "section": "",
    "text": "The mean again?\nWe are beginning our tour of interpreting linear models with the mean. We start with the mean, not because I doubt that you understand what a mean and variance are — I know that you know how to calculate the mean (\\(\\overline{y} = \\frac{\\sum y_i}{n}\\)) and the variance (\\(s^2 = \\frac{\\sum (y_i - \\overline{y})^2}{n-1}\\)). Instead, we are starting here because your solid understanding of these concepts will help you better understand linear models.\nIn a simple linear model with no predictors, the intercept is the mean and the only other term is the residual variation. So we predict the \\(i^{th}\\) individual’s value of the response variable to be:\n\\[\\hat{y}_i = b_0\\]\nWhere \\(b_0\\) is the intercept (i.e. the sample mean). In reality, of course, observations rarely match prediction perfectly. So an individual \\(i\\)’s actual value, \\(y_i\\) is its predicted value plus the difference between its observed and predicted value \\(e_i\\) (aka the residual).\n\\[{y}_i = b_0 + e_i\\]",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-mean-again",
    "href": "book_sections/linear_models/mean.html#the-mean-again",
    "title": "• 7. Mean",
    "section": "",
    "text": "“But we already had a section on the mean, and besides I’ve known what a mean was for years. Why another section on this?”\n\nYou, probably.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-lm-function-in-r",
    "href": "book_sections/linear_models/mean.html#the-lm-function-in-r",
    "title": "• 7. Mean",
    "section": "The lm() function in R",
    "text": "The lm() function in R\nIn R you build linear models with the lm() syntax:\nlm(response ~ explanatory1 + explanatory2 + ..., data = data_set). In a simple model with no predictors you type: lm(response ~ 1, data = data_set).\nSo to model the proportion of hybrid seed in GC with no explanatory variables, type:\n\n\n\n\n\n\n\n\n\nFigure 1: A bunch of Clarkia seeds. How many do you think are hybrids?\n\n\n\n\n\nlm(prop_hybrid ~ 1, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ 1, data = gc_rils)\n\nCoefficients:\n(Intercept)  \n     0.1506  \n\n\nThe output gives us the estimated intercept — which, in this case with no predictors, is simply the mean (see above). The code below verifies this (except that R provides a different number of digits).\n\nsummarise(gc_rils, mean_p_hyb = mean(prop_hybrid, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  mean_p_hyb\n       &lt;dbl&gt;\n1      0.151\n\n\nInterpretation This means that we model the \\(i^{th}\\) individual’s proportion of hybrid seed as the sample mean, 0.1506,\n\\[\\hat{Y}_i = 0.1506\\]\nand understand that its value will deviate from the sample mean by some amount, \\(e_i\\).\n\\[Y_i = 0.1506 + e_i\\]",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#residuals",
    "href": "book_sections/linear_models/mean.html#residuals",
    "title": "• 7. Mean",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\nRecall that the residual \\(e_i\\) is the difference between a data point and its value predicted from a model.\n\n\n\nWhat is \\(e_i\\) in the math above? It is the residual the distance between the predicted value of the \\(i^{th}\\) observation in a linear model, \\(\\hat{y}_i\\), and its actual value \\(y_i\\). Hovering over a point in Figure 2 reveals its residual value.\n\n\nCode\nlibrary(plotly)\nprop_hybrid_plot &lt;-  gc_rils                          |&gt;\n  filter(!is.na(prop_hybrid))                         |&gt;\n  mutate(i = 1:n(),\n         e_i = prop_hybrid - mean(prop_hybrid),\n         e_i = round(e_i, digits = 3),\n         y_hat_i = round(mean(prop_hybrid),digits = 3),\n         y_i = round(prop_hybrid, digits = 3))     |&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i, color = i==3))+\n  geom_point(size = 4, alpha = .6)+\n  scale_color_manual(values = c(\"black\",\"darkgreen\"))+\n  geom_hline(yintercept = summarise(gc_rils,mean(prop_hybrid)) |&gt; pull(),\n             linetype = \"dashed\", color = \"red\", size = 2)+\n  labs(y = \"Proportion hybrid\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme(legend.position = \"none\")\n\nggplotly(prop_hybrid_plot)\n\n\n\n\n\n\n\n\nFigure 2: An interactive plot showing the proportion of hybrid seeds for each Clarkia xantiana subspecies parviflora recombinant inbred line (RIL) planted at GC. Each point represents a line’s observed proportion hybrid seed, and the dashed red line shows the sample mean across all lines. Hovering over a point reveals its residual — the difference between the observed value and the mean. Individual 3 is shown in green as an example to focus on.\n\n\n\n\n\nWorked example.\nTake, for example, individual 3 (\\(i = 3\\), green point in the plot above), where 1/4 of its seeds are hybrids:\n\nIts observed value, \\(y_i\\), is the proportion of its seeds that are hybrids, which is 0.25.\n\nIts predicted value, \\(\\hat{y}_i\\), is the proportion of all seeds that are hybrids (dashed red line), which is 0.151.\n\nIts residual value, \\(e_i = y_i - \\hat{y}_i\\), is the difference between the proportion of its seeds that are hybrids and proportion of all seeds that are hybrids, which is \\(0.250 -0.151 = 0.099\\). Scroll over the third data point in Figure 2 to verify this.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#use-augment-to-see-residuals-and-more",
    "href": "book_sections/linear_models/mean.html#use-augment-to-see-residuals-and-more",
    "title": "• 7. Mean",
    "section": "Use augment() to see residuals (and more!)",
    "text": "Use augment() to see residuals (and more!)\nYou can use the augment() function in the broom package to see predictions and residuals. The code to the right uses this capability to calculate the sum of squared residuals, while the code below shows (some of) the output from augment().\n\nlibrary(broom)\nlm(prop_hybrid ~ 1, data = gc_rils)     |&gt;\n  augment()                             |&gt;\n  select(prop_hybrid, .fitted, .resid)\n\n\n\n\n\n\n\n\nCalculating \\(\\text{SS}_\\text{residual}\\) from augment() output\nYou can use the output of augment() to calculate the sum of squared residuals by taking residuals (from .resid), squaring them, and then summing them, as shown below:\n\nlibrary(broom)\nlm(prop_hybrid ~ 1, data = gc_rils)        |&gt;\n augment()                |&gt;\n mutate(sq_resid=.resid^2)|&gt;\n summarise(SS=sum(sq_resid))\n\n# A tibble: 1 × 1\n     SS\n  &lt;dbl&gt;\n1  5.62",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#the-mean-minimizes-textss_textresidual",
    "href": "book_sections/linear_models/mean.html#the-mean-minimizes-textss_textresidual",
    "title": "• 7. Mean",
    "section": "The mean minimizes \\(\\text{SS}_\\text{residual}\\)",
    "text": "The mean minimizes \\(\\text{SS}_\\text{residual}\\)\nYou already know the formula for the mean. But let’s imagine you didn’t. What properties would you want a good summary of the center of the data to have? What criteria would you use to say one summary was the best? We have already discussed this at length, and concluded it depends on the shape of the data etc…\nWhile you should consider the shape of our data when summarizing it, a common criterion of the “best” estimate is the one that minimizes the sum of squared residuals. Figure 3 shows that of all proposed means for the proportion of hybrid seeds set at GC, the arithmetic mean minimizes the sum of squared residuals. This is always the case!\nBy looping over many proposed means (\\(0\\) to \\(0.3\\)), the plot below illustrates that the arithmetic mean minimizes the sum of squared residuals: The sum of squared residuals for a given proposed mean is shown: In color on all three plots (yellow is a large sum of squared residuals, red is intermediate, and purple is low); Geometrically as the size of the square in the center plot; On the y-axis of the right plot.\n\n\n\n\n\n\n\n\nFigure 3: The mean minimizes the sum of squared residuals: The left panel shows the observed data points connected to proposed means (shown as a horizontal line), with colors indicating how much the sum of squared residuals differ from the minimum sum of squared residuals. The center panel visualizes the total squared error geometrically. The right panel shows the sum of squared residuals as a function of proposed mean values, with the current proposed mean highlighted with a point. The proposed value that minimizes the sum of squared residuals equals the arithmetic mean.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/mean.html#optional-extra-learning",
    "href": "book_sections/linear_models/mean.html#optional-extra-learning",
    "title": "• 7. Mean",
    "section": "OPTIONAL EXTRA LEARNING",
    "text": "OPTIONAL EXTRA LEARNING\n\nOptional / advanced reading:\nAbove, I showed that for our case study, the arithmetic mean minimizes the sum of squared residuals, and then I asserted that this is always the case. For those of you who enjoy calculus and want a formal proof—rather than a professor saying “trust me”—I’ve provided a link that shows this more generally.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Mean"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html",
    "href": "book_sections/linear_models/lm_cat_pred.html",
    "title": "• 7. Categorical predictor",
    "section": "",
    "text": "Example: Proportion Hybrid Seeds = f(Petal Color)\nThe previous section showed that we can think about a simple mean as an intercept in a linear model with no explanatory variables. But in the introduction to linear models, I introduced \\(\\hat{y}_i\\) as a conditional mean. This section builds on our understanding of the intercept as a mean, moving toward our goal of understanding \\(\\hat{y}_i\\) as a conditional mean. To do so, we will start with a binary explanatory variable, and then move on to a categorical variable with multiple levels. Because I find this hard to introduce abstractly, we’ll jump right back into the Clarkia hybridization example.\nAfter exploring the mean proportion of hybrid seed as a summary, we are ready to explore variables that could explain variation in the proportion of hybrid seeds. To start, let’s focus on petal color as perhaps white petals do not attract pollinators and consequently make fewer hybrids. In this case the conditional means are:\ngc_rils                                             |&gt;\n  filter(!is.na(petal_color) , !is.na(prop_hybrid)) |&gt;\n  group_by(petal_color)                             |&gt;\n  summarise(mean_prop_hybrid = mean(prop_hybrid))\n\n# A tibble: 2 × 2\n  petal_color mean_prop_hybrid\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 pink                  0.260 \n2 white                 0.0322\nWe can represent these conditional means in an equation by modeling the mean proportion of hybrid seeds for individual \\(i\\), conditional on its petal color:\n\\[\\text{PROP HYBRID SEED}_i = b_0 + b_1 \\times \\text{WHITE}_i + e_i\\]\nWe can think of the equation above as representing two separate cases:\n\\[\\text{PROP HYBRID SEED}_{i|\\text{Pink petal}} = b_0 + e_i\\] \\[\\text{PROP HYBRID SEED}_{i|\\text{White petal}}  = b_0 + b_1 + e_i\\]\nWe know (from above) that\nBelow we see that the lm() function in R can do this for us!",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#example-proportion-hybrid-seeds-fpetal-color",
    "href": "book_sections/linear_models/lm_cat_pred.html#example-proportion-hybrid-seeds-fpetal-color",
    "title": "• 7. Categorical predictor",
    "section": "",
    "text": "\\(b_0\\), the intercept, is the mean proportion hybrid seed set for the “reference level.” In this case the reference level is pink. We know this because PINK is not in the equation.\n\\(b_1\\) is the difference in the mean proportion hybrids set on white and pink-petaled plants.\nWHITE is an indicator variable.\n\nWHITE equals 1 for white-petaled plants.\n\nWHITE equals 0 for pink-petaled plants.\n\n\n\n\n\n\n\\(b_0\\) – the mean proportion of seeds from pink flowers that were hybrids equals 0.260.\n\n\\(\\text{PROP HYBRID SEED}_{i|\\text{White petal}}\\) equals 0.0322.\nSo we can solve for \\(b_1\\) – the difference between the mean proportion of hybrids on white and pink-petaled plants – as \\(\\text{PROP HYBRID SEED}_{i|\\text{White petal}} - b_0\\) (0.0322 - 0.260 = -0.2278).",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-lm",
    "href": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-lm",
    "title": "• 7. Categorical predictor",
    "section": "Building a model with lm()",
    "text": "Building a model with lm()\nWe can build linear models in R with the lm() function: lm(response ~ explanatory, data = data_set). So, the following code models the proportion of hybrid seeds as a function of petal color.\n\nlm(prop_hybrid ~ petal_color, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ petal_color, data = gc_rils)\n\nCoefficients:\n     (Intercept)  petal_colorwhite  \n          0.2598           -0.2277  \n\n\nThe resulting coefficients match the values we calculated by hand, aside from minor rounding differences.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#residuals",
    "href": "book_sections/linear_models/lm_cat_pred.html#residuals",
    "title": "• 7. Categorical predictor",
    "section": "Residuals",
    "text": "Residuals\nAs in the case for the mean, we calculate residuals as the difference between each observed value, \\(y_i\\), and its predicted value, \\(\\hat{y}_i\\). However, in this case, \\(\\hat{y}_i\\), differs for plants with different petal colors. Again hovering over a point in Figure 1 reveals its residual. Also, as before we can use the augment() function to see predictions and residuals.\n\n\nCode\nlibrary(plotly)\nprop_hybrid_plot_color &lt;-  gc_rils                    |&gt;\n  filter(!is.na(prop_hybrid),!is.na(petal_color))     |&gt;\n  mutate(i = 1:n())                                   |&gt;\n  group_by(petal_color)                               |&gt; \n  mutate(y_hat_i = mean(prop_hybrid))                 |&gt;\n  ungroup()                                           |&gt;\n  mutate(y_i     = round(prop_hybrid, digits = 3),\n         y_hat_i = round(y_hat_i, digits = 3),\n         e_i     = round(y_i - y_hat_i, digits = 3))  |&gt;\n  mutate(color2 = ifelse(i == 3, \"3\" , petal_color))|&gt;\n  ggplot(aes(x = i, y = y_i, y_hat_i = y_hat_i, e_i = e_i, color = color2))+\n  geom_point(size = 4, alpha = .6)+\n  geom_hline(data = gc_rils    |&gt; \n               filter(!is.na(prop_hybrid),!is.na(petal_color)) |&gt; \n               group_by(petal_color)|&gt; \n               summarise(prop_hybrid = mean(prop_hybrid)),\n             aes(yintercept = prop_hybrid), linetype = \"dashed\", size = 2)+\n  facet_wrap(~petal_color)+\n  labs(y = \"Proportion hybrid\", title =\"This plot is interactive!! Hover over a point to see its residual\")+\n  theme_dark()+\n  scale_color_manual(values= c(\"darkgreen\",\"pink\",\"white\"))+\n  theme(legend.position = \"none\")\n\nggplotly(prop_hybrid_plot_color)\n\n\n\n\n\n\n\n\nFigure 1: An interactive plot showing the proportion of hybrid seeds for each Clarkia xantiana subspecies parviflora recombinant inbred line (RIL) planted at GC split by petal color. Each point represents a line’s observed proportion hybrid seed, and the dashed line shows the group mean across all lines with pink (left) and white (right) petals. Hovering over a point reveals its residual — the difference between the observed value and the group mean. The green point provides an example datapoint to focus on for understanding.\n\n\n\n\n\nWorked example.\nTake, for example, individual 3 (\\(i = 3\\), green point in the plot above), where 1/4 of its seeds are hybrids:\n\nIts observed value, \\(y_i\\), is the proportion of its seeds that are hybrids, which is 0.25.\n\nIts predicted value, \\(\\hat{y}_i\\), is the proportion of seeds from pink-flowered plants that are hybrids (dashed line), which is 0.260.\n\nIts residual value, \\(e_i = y_i - \\hat{y}_i\\), is the difference between the proportion of its seeds that are hybrids and proportion of seeds from pink-flowered plants that are hybrids, which is \\(0.250 - 0.260 =  - 0.010\\). Scroll over the third data point in Figure 2 to verify this.\n\nNote that this data point had a large residual when we modeled proportion of hybrid seeds as a simple mean, but now has a residual value near zero when we model proportion of hybrid seeds as a function of petal color.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#categorical-predictors-with-more-than-two-levels",
    "href": "book_sections/linear_models/lm_cat_pred.html#categorical-predictors-with-more-than-two-levels",
    "title": "• 7. Categorical predictor",
    "section": "Categorical Predictors with More Than Two Levels",
    "text": "Categorical Predictors with More Than Two Levels\nAbove we considered a binary predictor, but the same modeling approach works for nominal predictors with more than two categories. For example, modeling, proportion of seeds that are hybrid as a function of location would look something like this:\n\\[\\text{PROP HYBRID SEED}_i = f(\\text{location})\\] \\[\\text{PROP HYBRID SEED}_i = b_0 + b_1 \\times \\text{LB}_i + b_2 \\times \\text{SR}_i  + b_3 \\times \\text{US}_i+ e_i\\]\nIn this case,\n\n\\(b_0\\), the intercept, is the mean proportion of hybrid seeds for the “reference level.” In this case the reference level is GC. We know this because GC is not in the equation.\n\n\\(b_1\\) is the difference in the mean proportion hybrids set at LB and at the reference location (GC).\nLB is an indicator variable.\n\nLB equals 1 for plants planted at LB.\n\nLB equals 0 for plants not planted at LB.\n\n\n\\(b_2\\) is the difference in the mean proportion hybrids set at SR and at the reference location (GC).\n\n… and so on, as summarized in the table below.\n\n\n\n\n\n\n\nTerm in Model\nInterpretation\n\n\n\n\nIntercept (\\(b_0\\))\nMean proportion of hybrid seeds at GC (reference group)\n\n\n\\(b_1\\) (locationLB)\nDifference between LB and GC: mean at LB − mean at GC\n\n\n\\(b_2\\) (locationSR)\nDifference between SR and GC: mean at SR − mean at GC\n\n\n\\(b_3\\) (locationUS)\nDifference between US and GC: mean at US − mean at GC\n\n\n\n\n\nPredicted proportion hybrid seeds by location:   - \\(\\hat{y}_{i|\\text{GC}} = b_0\\) - \\(\\hat{y}_{i|\\text{LB}} = b_0 + b_1\\) - \\(\\hat{y}_{i|\\text{SR}} = b_0 + b_2\\) - \\(\\hat{y}_{i|\\text{US}} = b_0 + b_3\\) **",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-multiple-levels-with-lm",
    "href": "book_sections/linear_models/lm_cat_pred.html#building-a-model-with-multiple-levels-with-lm",
    "title": "• 7. Categorical predictor",
    "section": "Building a model with multiple levels with lm()",
    "text": "Building a model with multiple levels with lm()\nThe syntax for building a model for a categorical variable with multiple levels is the same as that for a binary categorical variable: lm(response ~ explanatory, data = data_set). The following code models the proportion of hybrid seeds as a function of planting location:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOptional / advanced: “How does R pick the reference level? and how can you change it?”\nBy default, R uses the alphabetically first level of a categorical variable as the reference group. This isn’t always the best way to think about our data. Later we will work on changing the order of our levels, but if you can’t wait, check out the [forcats](https://forcats.tidyverse.org/) package.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Categorical predictor"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html",
    "href": "book_sections/linear_models/regression.html",
    "title": "• 7. Linear regression",
    "section": "",
    "text": "A review of covariance and correlation\nWe have previously quantified the association between two numeric variables as:\nWhile these measures, and particularly \\(r\\), are essential summaries of association, neither allows us to model a numeric response variable as a function of numeric explanatory variable.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#a-review-of-covariance-and-correlation",
    "href": "book_sections/linear_models/regression.html#a-review-of-covariance-and-correlation",
    "title": "• 7. Linear regression",
    "section": "",
    "text": "A covariance: How much observations of \\(x\\) and \\(y\\) jointly deviate from their means in an individual, quantified as: \\(\\text{cov}_{x,y}= \\frac{n}{n-1}(\\overline{xy}-\\bar{x} \\times\\bar{y}) = \\frac{\\sum (x_i-\\bar{x})\\times(y_i-\\bar{y})}{n-1}\\).\n\nA correlation, \\(r\\): How reliably \\(x\\) and \\(y\\) covary – a standardized measure of the covariance. The correlation is quantified as \\(r_{xy}= \\frac{\\text{cov}_{x,y}}{s_x\\times s_y}\\), where \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#linear-regression-set-up",
    "href": "book_sections/linear_models/regression.html#linear-regression-set-up",
    "title": "• 7. Linear regression",
    "section": "Linear regression set up",
    "text": "Linear regression set up\nSo, how do we find \\(\\hat{y}_i\\)? The answer comes from our standard equation for a line — \\(y = mx + b\\). But statisticians have their own notation:\n\\[\\hat{y}_i = b_0 + b_1 \\times x_i\\] The variables in this model are interpreted as follows:\n\n\\(\\hat{y}_i\\) is the conditional mean of the response variable for an individual with a value of \\(x_i\\). Note this is a guess at what the mean would be if we had many such observations - but we usually have one or fewer observations of y at a specific value of x.\n\n\\(b_0\\) is the intercept – the value of the response variable if we follow it to \\(x = 0\\).\n\n\\(b_1\\) is the slope – the expected change in \\(y\\), for every unit increase in \\(x\\).\n\n\\(x_i\\) is the value of the explanatory variable, \\(x\\) in individual \\(i\\).",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#math-for-linear-regression",
    "href": "book_sections/linear_models/regression.html#math-for-linear-regression",
    "title": "• 7. Linear regression",
    "section": "Math for linear regression",
    "text": "Math for linear regression\nBefore delving into the R of it all, let’s start with some math. To make things concrete, we’ll consider the proportion of hybrid seed as a function of petal area.\n\nMath for slope\nWe find the slope, \\(b_1\\), as the covariance divided by the variance in \\(x\\):\n\\[b_1 = \\frac{cov_{x,y}}{s^2_x}  =  \\frac{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\frac{1}{(n-1)}\\sum(x_i-\\bar{x})^2}=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\\]\n\n\nMath for intercept\nWe find the intercept, \\(b_0\\) (sometimes called \\(a\\)), as\n\\[b_0 = \\bar{y}-b_1\\bar{x}\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nggplot(gc_rils, \n       aes(x = petal_area_mm, \n           y = prop_hybrid))+\n  geom_smooth(method = \"lm\", \n              se = FALSE)+\n  geom_point(size = 6)\n\n\nInterpretation\n\n\n\n\n\n\n\n\n\nFigure 1: A scatterplot showing the relationship between petal area (in mm²) and the proportion of hybrid seeds in parviflora RILs, with a fitted linear regression line.\n\n\n\n\nThe calculations above (displayed in Figure 1) correspond to the following linear model:\n\\[\\text{PROPORTION HYBRID}_i =  -0.208 + 0.00574  \\times \\text{PETAL AREA}_i\\]\n\nThe intercept of -0.208 means that if we followed the line down to a petal area of zero, the math would predict that such a plant would produce a nonsensical negative 20% hybrid seed. This is a mathematically useful construct but is biologically and statistically unjustified (see below). Of course, we can still use this equation (and implausible intercept), to make predictions in the range of our data.\nThe slope of 0.00574 means that every additional \\(mm^2\\) increase in area, we expect about a half percent more hybrid seeds.\n\n\nDo not predict beyond the range of the data.\nPredictions from a linear model are only valid within the range of our observed data. For example, while our model estimates an intercept of –0.208, we certainly don’t expect a microscopic flower to receive fewer than zero pollinator visits. Similarly, we shouldn’t expect a plant with a petal area of \\(300 \\text{ mm}^2\\) to produce more than 150% hybrid seeds.\nThese exaggerated examples highlight an important point: we should not make predictions far outside the range of our data. Even a less extreme prediction — say, estimating that a plant with 200 mm² of petal area will produce 95% hybrid seeds — may not be reliable. Although this number may seem biologically plausible, we have no reason to trust that our model’s predictions hold beyond the range of data we collected.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#residuals",
    "href": "book_sections/linear_models/regression.html#residuals",
    "title": "• 7. Linear regression",
    "section": "Residuals",
    "text": "Residuals\nAs seen previously, predictions of a linear model do not perfectly match the data. Again, the difference between observed, \\(y_i\\), and expected, \\(\\hat{y}_i\\), values is the residual \\(e_i\\). Try to estimate the residual of data point three (green point) in Figure 2.\n\n\nCode\nggplot(gc_rils|&gt; mutate(i = 1:n()), \n       aes(x = petal_area_mm, y = prop_hybrid))+\n    geom_smooth(method = \"lm\", se = FALSE)+\n        geom_point(aes(color = (i==3), size = (i == 3)))+\n        scale_size_manual(values = c(3,6))\n\n\n\n\n\n\n\n\nFigure 2: The same scatterplot showing the relationship between petal area as in Figure 1. The highlighted point in turquoise marks plant 3, used as an example in the residual calculation. This point’s vertical distance from the regression line represents its residual\n\n\n\n\n\n\nCONCEPT CHECK Looking at Figure 2, visually estimate \\(e_3\\) (the residual of point three in green) \n\n\nHow to solve this.\n\nSolution: The green point (\\(i = 3\\)) is at y = 0.25, and x is just a bit more than 50. At that x value, the regression line (showing \\(\\hat{y}\\)) is somewhat below 0.125 (I visually estimate _{ mm}= 0.090$). So 0.250 - 0.090 = 0.160.\n\n\n\nMath to get us closer\n\nMath Approach: As above, let’s visually approximate the petal area of observation 3 as 51 mm. Plugging this visual estimate into our equation (with coefficients from our linear model):\n\\[\\hat{y}_{\\text{petal area} \\approx 50 mm}-0.207834 + 0.005738 * 51 = 0.084\\] \\[e_3 = y_ -  \\hat{y} = 0.250 - 0.084 = 0.166\\]\n\n\n\nThe slope minimizes \\(\\text{SS}_\\text{residuals}\\)\nAs seen for the mean, by looping over many proposed slope(\\(-0.01\\) to \\(0.02\\)) the plot below illustrates that the slope from the formula above minimizes the sum of squared residuals: The sum of squared residuals for a given proposed slope (with intercept plugged in from math) is shown: In color on all three plots (yellow is a large sum of squared residuals, red is intermediate, and purple is low); Geometrically as the size of the square, in the center plot; On the y-axis of the right plot.\n\n\n\n\n\n\n\n\nFigure 3: The slope minimizes the sum of squared residuals: The left panel shows observed data points along with a proposed regression line. Vertical lines connect each point to its predicted value, illustrating residuals. The color of the line corresponds to the total residual sum of squares. The center panel visualizes the total squared error as a square whose area reflects the sum of squared residuals. The right panel plots the sum of squared residuals as a function of the proposed slope, with a moving point highlighting the current slope value. The slope that minimizes the sum of squared residuals defines the best-fitting line.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#using-the-lm-function",
    "href": "book_sections/linear_models/regression.html#using-the-lm-function",
    "title": "• 7. Linear regression",
    "section": "Using the lm() function",
    "text": "Using the lm() function\nAs before, we build a linear model in R as lm(response ~ explanatory). So to predict the proportion of hybrid seed as a function of petal area (in mm) type:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAgain, we can view residuals and other useful things with the augment() function. Let’s go back to the webr section above and pipe the output of lm() to augment()\n\nCONCEPT CHECK: Use the augment function to find the exact residual of data point 3 to three decimal places \n\n\nSolution\n\nPaste the code below into the webR window above to find the answer.\n\nlibrary(broom)\nlm(prop_hybrid ~ petal_area_mm, data = gc_rils) |&gt;\n  augment() |&gt;\n  slice(3)       # You can just go down to the third, but I wanted to show you this trick",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#looking-forward.",
    "href": "book_sections/linear_models/regression.html#looking-forward.",
    "title": "• 7. Linear regression",
    "section": "Looking forward.",
    "text": "Looking forward.\nYou’ve now seen how we can model a linear relationship between two numeric variables, how to calculate the slope and intercept, and how the best-fitting line is the one that minimizes the sum of squared residuals. You’ve also interpreted these model components in a biological context, visualized residuals, and learned how to fit and explore a linear model in R.\n\nIn the next section, we extend linear models to include two explanatory variables.\n\nLater in this book, we’ll introduce concepts of uncertainty and hypothesis testing and model evaluation to deepen our understanding of what these models can (and cannot) tell us, and when to be appropriately skeptical about their predictions.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/regression.html#optional-extra-learning.",
    "href": "book_sections/linear_models/regression.html#optional-extra-learning.",
    "title": "• 7. Linear regression",
    "section": "OPTIONAL EXTRA LEARNING.",
    "text": "OPTIONAL EXTRA LEARNING.\n\n\\(r^2\\) and the proportion variance explained\nSo far, we’ve focused on just one type of sum of squares — the residual sum of squares, \\(\\text{SS}_\\text{residual}\\) (also called \\(\\text{SS}_\\text{error}\\)). As you well know by now:\n\n\\(\\text{SS}_\\text{residual}\\) describes the distance between predictions and observations, and is quantified as the sum of squared differences between observed values, \\(y_i\\), and the conditional means, \\(\\hat{y}_i\\). \\(\\text{SS}_\\text{residual} = \\sum{(e_i)^2} = \\sum{(y_i-\\hat{y}_i)^2}\\).\n\nWe will explore two other types of sums of squares later, but in case you can’t wait:\n\n\\(\\text{SS}_\\text{model}\\) describes the distance between predictions and the grand mean, and is quantified as is the sum of squared differences between conditional means, \\(\\hat{y}_i\\), and the grand mean, \\(\\bar{y}\\). \\(\\text{SS}_\\text{model} = \\sum{(\\hat{y}_i-\\bar{y})^2}\\).\n\\(\\text{SS}_\\text{total}\\) describes the overall variability in the response variable, and is quantified as the sum of squared differences between observed values \\(y_i\\), and the grand mean, \\(\\bar{y}\\). \\(\\text{SS}_\\text{total} = \\sum{(y_i-\\bar{y})^2}\\).\n\nDividing \\(\\text{SS}_\\text{model}\\) by \\(\\text{SS}_\\text{total}\\) describes “the proportion of variance explained” by our model, \\(r^2\\):\n\\[r^2 = \\frac{\\text{SS}_\\text{model}}{\\text{SS}_\\text{total}}\\]\nReassuringly, the square root of \\(r^2\\) equals the absolute value of \\(r\\)\n\nlibrary(broom)\nlm(prop_hybrid ~ petal_area_mm, data = gc_rils) |&gt;\n  augment()|&gt;\n  summarise(mean_y   = mean(prop_hybrid),\n            SS_error = sum(.resid^2),\n            SS_total = sum( (prop_hybrid - mean_y)^2 ),\n            SS_model = sum( (.fitted - mean_y)^2 ),\n            r2       = SS_model /  SS_total,\n            r        = cor(prop_hybrid, petal_area_mm),\n            sqrt_r2  = sqrt(r2))\n\n# A tibble: 1 × 7\n  mean_y SS_error SS_total SS_model    r2     r sqrt_r2\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  0.151     4.95     5.59    0.638 0.114 0.338   0.338",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear regression"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html",
    "href": "book_sections/linear_models/two_predictors.html",
    "title": "• 7. Two predictors",
    "section": "",
    "text": "General Linear models in R\nWe found that both petal area and petal color predict the proportion of hybrid seeds in parviflora RILs. We’ve seen how to model each relationship individually, but what if you want to consider both traits at once? – Does petal size still matter when accounting for petal color? Or do visits ‘explain away’ the apparent association of proportion hybrid seeds with petal size?\nThe “general linear model” allows us to tackle such questions by modeling multiple explanatory variables at once (Figure 1). To do so, we extend our familiar simple linear model to include both kinds of predictors at once.\nIn this case, each predictor gets its own coefficient:\nMathematically, the prediction for an individual becomes:\n\\[\\hat{y}_i = b_0 + b_1 x_{1i} + b_2 x_{2i}\\] To make this more concrete, let’s define:\nWith this biological grounding we can re-cast our equation as\n\\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i = b_0 + b_1   \\times  \\text{PETAL AREA MM}^2 +  b_2 \\times \\text{WHITE}_i\\]\nThe mathematical trick for estimating parameters (e.g. \\(b_0\\), \\(b_1\\), and \\(b_2\\)) in a general linear model exceeds what I care that you learn. So let’s find these in R with the lm() function.\nTo conduct a general linear model in R just add terms to your linear model:\nlm(response ~ explan_1 + explan_2, data = data). For our case:\nlm(prop_hybrid ~ petal_area_mm + petal_color, data = gc_rils)\n\n\nCall:\nlm(formula = prop_hybrid ~ petal_area_mm + petal_color, data = gc_rils)\n\nCoefficients:\n     (Intercept)     petal_area_mm  petal_colorwhite  \n       -0.091439          0.005759         -0.239181",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#general-linear-models-in-r",
    "href": "book_sections/linear_models/two_predictors.html#general-linear-models-in-r",
    "title": "• 7. Two predictors",
    "section": "",
    "text": "Beware of associated predictors (multicollinearity): If pink-petaled parviflora RILs tended to have bigger petals than white ones, it becomes trickier to cleanly separate their effects. The overall model predictions (\\(\\hat{y}_i\\)) can be reliable, but the estimated coefficients (\\(b_1\\), \\(b_2\\)) can become unstable or hard to interpret. For now know that it’s fine if predictors are somewhat related, but when they are strongly correlated, we have to be cautious when interpreting individual coefficients.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#conditional-means-from-general-linear-models",
    "href": "book_sections/linear_models/two_predictors.html#conditional-means-from-general-linear-models",
    "title": "• 7. Two predictors",
    "section": "Conditional means from General Linear Models",
    "text": "Conditional means from General Linear Models\nWith the estimates above, we can return to our general linear model equation to find conditional means: \\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i = b_0 + b_1   \\times  \\text{PETAL AREA MM}^2 +  b_2 \\times \\text{WHITE}_i\\] \\[\\widehat{\\text{PROPORTION OF HYBRID SEED}}_i =  -0.0894 + 0.00576  \\times  \\text{PETAL AREA MM}^2 - 0.244192   \\times \\text{WHITE}_i\\]\n\n\n\n\n\ni\npetal_area_mm\npetal_color\nprop_hybrid\n\n\n\n\n1\n43.9522\nwhite\n0.000\n\n\n2\n55.7863\npink\n0.125\n\n\n3\n51.7031\npink\n0.250\n\n\n4\n57.2810\nwhite\n0.000\n\n\n\n\n\n\nA worked example and challenge questions\nThe table below shows values of the explanatory and response variables for the first four samples in the gc_rils dataset. Individuals \\(i=1\\) and \\(i=3\\) are show in black and red x’s in Figure 2, and I provide R code for calculating the prediction for individual \\(i=1\\) below. In the webR session below:\n\nFind the residual for individual \\(i=1\\).\n\nFind the predicted proportion hybrid seed and the residual for individual \\(i=3\\).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHow to solve this.\n\n\n # prediction for i = 1\n # b0   + b1 * petal area + b2 * white\ny_1 &lt;- -0.0894 + 5.76e-3 * 44 - 0.244 * 1       \nprint(paste(\"Prediction for individual i=1:\",y_1))\n\n[1] \"Prediction for individual i=1: -0.07996\"\n\n# Residual for ind i=1\ne_1 &lt;- 0.00 - y_1 # Residual = observed - predicted\nprint(paste(\"Residual for individual i=1:\",e_1))\n\n[1] \"Residual for individual i=1: 0.07996\"\n\n# prediction for i = 3\n# b0   + b1 * petal area + b2 * white\ny_3 &lt;- -0.0894 + 5.76e-3 * 51.7 #   no b2 because its pink  \nprint(paste(\"Prediction for individual i=3:\",y_3))\n\n[1] \"Prediction for individual i=3: 0.208392\"\n\n# Residual for ind i=1\ne_3 &lt;- 0.25 - y_3 # Residual = observed - predicted\nprint(paste(\"Residual for individual i=3:\",e_3))\n\n[1] \"Residual for individual i=3: 0.0416079999999999\"\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Proportion of hybrid seeds as a function of petal area and petal color in parviflora RILs. This plot essentially replicates Figure 1, with the following caveats: it shows the same slope for each petal color, matching our model assumptions. The plot is interactive. Individuals \\(i=1\\) and \\(i=3\\) are highlighted with black and red Xs, respectively.\n\n\n\n\n\n\nlibrary(broom)\n\n# Minimal code for a figure\n# like that on the left\n\nmy_lm &lt;- lm(prop_hybrid ~ \n              petal_color + \n              petal_area_mm, \n            data = gc_rils)\n\naugmented_data &lt;-my_lm |&gt;\n   augment()\n\naugmented_data |&gt;\n    ggplot(aes(\n      x = petal_area_mm,\n      y = prop_hybrid,\n      color = petal_color)\n      )+\n    geom_point()+\n    geom_smooth(aes(\n        y = .fitted),\n      method = \"lm\",\n      se = FALSE)",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#wrapping-up-two-predictors-one-model",
    "href": "book_sections/linear_models/two_predictors.html#wrapping-up-two-predictors-one-model",
    "title": "• 7. Two predictors",
    "section": "Wrapping Up: Two Predictors, One Model",
    "text": "Wrapping Up: Two Predictors, One Model\nAbove, we extended a simple linear model to include two predictors — one continuous and one categorical. We could, of course, have had two categorical predictors, two numeric predictors, or more than two predictors as well. By adding multiple predictors, we can better account for biological complexity — modeling how several traits simultaneously influence a response. We can even include interactions between variables, as I show in the optional content below.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/two_predictors.html#optional-extra-learning.",
    "href": "book_sections/linear_models/two_predictors.html#optional-extra-learning.",
    "title": "• 7. Two predictors",
    "section": "OPTIONAL EXTRA LEARNING.",
    "text": "OPTIONAL EXTRA LEARNING.\n\nIncluding interactions.\nEarlier, we assumed that the increase in proportion of hybrid seeds with increased petal area was the same for pink and white-flowered plants, but as shown in Figure 1, this might not be true. It looks like petal area is strongly associated with proportion hybrid seed in pink, but not white-flowered plants.\nWe can further extend linear models to include such interactions (i.e. differences in slopes).\n\\[\\hat{y}_i = \\text{INTERCEPT} + \\text{SLOPE} \\times \\text{PETAL AREA} +\\] \\[ b_2 \\times \\text{WHITE}+ b_3 \\times   \\text{WHITE} \\times \\text{PETAL AREA}\\]\nWe can do this in R by adding this interaction explicitly:\nlm(response ~ explan1 + explan2 + explan1:explan2), where : notes the interaction.\n\nlm(prop_hybrid ~ petal_area_mm + \n                 petal_color   + \n                 petal_area_mm:petal_color, \n    data = gc_rils) \n\n\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n-0.3289259\n\n\npetal_area_mm\n0.0095891\n\n\npetal_colorwhite\n0.2326197\n\n\npetal_area_mm:petal_colorwhite\n-0.0075498\n\n\n\n\n\nFrom these estimates, we can model proportion of hybrid seeds for pink and white-petaled flowers as follows:\n\\[\\hat{y}_{i|\\text{PINK}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\]\n\nand\n\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 + 0.00960 \\times \\text{PETAL AREA}\\] \\[+0.237 -0.00766 \\times \\text{PETAL AREA}\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.330 +0.237 +\\text{PETAL AREA} \\times (0.00960 -0.00766)\\]\n\\[\\hat{y}_{i|\\text{WHITE}} = -0.093 +\\text{PETAL AREA} \\times (0.001946)\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Two predictors"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html",
    "href": "book_sections/linear_models/lm_summary.html",
    "title": "• 7. Linear model summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Questions. Glossary. R functions. R packages. More resources.\nLinear models provide a unified framework for estimating the expected value (i.e., the conditional mean) of a numeric response variable as a function of one or more explanatory variables. These models are additive: the expected value is found by summing components of the model — the intercept plus the effect of each variable multiplied by its value. The sum of squared differences between observed values and predicted values describes how closely the data match the model’s predictions. Linear models can be descriptive tools that capture the structure, variation, and relationships in a dataset. In later chapters, we will build on this foundation to evaluate models more critically — assessing how well they fit, how reliable their predictions are, and how to diagnose their limitations.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#chapter-summary",
    "href": "book_sections/linear_models/lm_summary.html#chapter-summary",
    "title": "• 7. Linear model summary",
    "section": "",
    "text": "Chatbot tutor\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#practice-questions",
    "href": "book_sections/linear_models/lm_summary.html#practice-questions",
    "title": "• 7. Linear model summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the ril data, cleaned it some, and have started some of the code!\n\nQ1) What is the key difference between a scientific and a statistical model?\n\n Statistical models make predictions, scientific models do not. Statistical models can be wrong, scientific models cannot. Statistical models describe patterns, scientific models are built on understanding processes. Statistical models make assumptions, scientific models do not. There are no differences between scientific and statistical models.\n\n\nQ2 Consider the R code and output below. The (Intercept) describes:\n\n The mean bill depth of penguins in this dataset. The mean bill depth of all penguins. The mean bill depth of penguins of the “reference level” species. The expected bill depth if all the predictors are set zero.\n\n\nlibrary(palmerpenguins)\nlm(bill_depth_mm~1, penguins)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 1, data = penguins)\n\nCoefficients:\n(Intercept)  \n      17.15  \n\n\nQ3) Without running this code, predict which sex will be the reference level in the model: lm(bill_length_mm ~ sex, penguins)? malefemalewhichever has the smaller meanimpossible to predict\n\n\nQ4 - Q7) Linear models with categorical predictors. The penguins data has data from three species of penguins – Adelie, Chinstrap and Gentoo. To answer the following questions, consider the R code and output below and no other information (that is, do not load these data into R).\n\nlm(bill_length_mm~species, penguins)\n\n\nCall:\nlm(formula = bill_length_mm ~ species, data = penguins)\n\nCoefficients:\n     (Intercept)  speciesChinstrap     speciesGentoo  \n          38.791            10.042             8.713  \n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ4) What is the mean bill length of Adelie penguins in the dataset? 38.79110.04248.833not enough information\nQ5) What is the mean bill length of Chinstrap penguins in the dataset? 38.79110.04248.833not enough information\nQ6) How many mm longer Chinstrap bills as compared to Gentoo bills in this dataset? 10.0428.7131.329not enough information\nQ7) What is the mean bill length of all penguins in this dataset? 38.79119.18245.0426743.92not enough information\n\n\nQ8 - Q13) Mathematics of linear regression. Use the summaries below to conduct a linear regression that models the response variable, bill depth, as a function of the explanatory variable, bill length.\n\n\n\n\n\nmean_depth\nmean_length\ncov_length\nsd_depth\nsd_length\n\n\n\n\n17.59\n46.57\n0.62\n0.78\n3.11\n\n\n\n\n\n\n\n\nmin_depth\nmax_depth\nmin_length\nmax_length\nsample_size\n\n\n\n\n16.4\n19.4\n40.9\n58\n34\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) The correlation between these variables is .\nQ9) The slope in this model is .\nQ10) The intercept in this model is .\nQ11) According to the model, what is the predicted bill depth (in mm) for a penguin with a 50 mm long bill .\nQ12) A penguin with a 20 mm deep and 50 mm long bill will have a residual of  mm.\nQ13) According to the model, what is the predicted bill depth for a penguin with a 500 mm long bill  mm deep bill.\n\n\nHaven’t gotten it yet?\n\nOf course you haven’t!! The correct answer, of course, is that you cannot predict outside the range of our data. See Figure 1!\n\n\n\nQ14 - Q16) More than one explanatory variable. Use the summaries below to conduct a linear regression that models the response variable, bill depth, as a function of the explanatory variable, bill length.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\nlibrary(broom)\n\n\ngentoo_data &lt;- penguins        |&gt;\n  filter(species == \"Gentoo\") \n\nlm(bill_depth_mm ~ bill_length_mm +sex, data = gentoo_data)|&gt;\n  augment() |&gt;\n  ggplot(aes(x=bill_length_mm , y=bill_depth_mm,color = sex))+\n  geom_point(size = 5, alpha = .7)+\n  geom_smooth(method = \"lm\",se = FALSE, linetype = \"dashed\",linewidth = 2)+\n  geom_smooth(aes(y = .fitted), se=FALSE, linewidth = 2)+\n  theme(legend.position = \"top\", \n        axis.title  = element_text(size = 28),\n        axis.text   = element_text(size = 28),\n        legend.text = element_text(size = 28),\n        legend.title = element_text(size = 28))\n\n\n\n\n\n\n\n\nFigure 2: Bill depth as a function of bill length for Gentoo penguins, separated by sex. Solid lines show the predicted values from a multiple regression model including both bill length and sex as predictors. Dashed lines show simple linear fits ignoring other predictors. The plot highlights both the overall trend with bill length and differences in mean depth between males and females.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ14) In Figure 2 there is a male penguin with a bill that is about 56 mm long (the second most extreme right point). Approximate, by eye its residual. \nQ15) Based on Figure 2, which of the following are nearly identical for male and female Gentoo penguins? (Select all that apply.)\n\n Mean bill lengths Mean bill depths Slopes Intercepts This plot shows that nearly nothing is nearly identical between male and female Gentoo penguins\n\n.\nQ16) Use the web R space above to model flipper length as a function of body mass and sex of Chinstrap penguins. The sum of squared residuals is:.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#glossary-of-terms",
    "href": "book_sections/linear_models/lm_summary.html#glossary-of-terms",
    "title": "• 7. Linear model summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n📚 1. Concepts of Modeling\n\nStatistical Model: A mathematical description of patterns in data, often used to summarize, predict, or test hypotheses.\nScientific Model: A conceptual model based on biological understanding, explaining processes in the real world.\n\n\n\n\n🔀 2. Different Predictor Types\n\nCategorical Predictor: A variable with discrete groups. Modeled by differences in intercepts across groups.\nNumeric Predictor: A continuous variable. Modeled by slopes showing expected change in the response per unit change in the predictor.\nIndicator Variable: A numeric coding of a categorical variable (e.g., 0 for “pink,” 1 for “white”).\nReference Level: The baseline category in a categorical predictor against which other groups are compared.\n\n\n\n3. Components of a Linear Model\n\nConditional Mean, \\(\\hat{y}_i\\): The predicted value of a response variable for given explanatory variable values.\n\nGeneral linear model form: \\(\\hat{Y}_i = f(\\text{explanatory variables}_i)\\).\nLinear combination form: \\(\\hat{Y}_i = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\dots + b_k x_{k,i}\\).\n\n\nIntercept (b₀): The expected value of the response when all explanatory variables are zero (sometimes called \\(a\\)).\nSlope (b₁): The expected change \\(\\hat{y}_i\\) with change in the predictor.\n\nFor numeric predictor: The expected change in the response for a one-unit increase in a numeric explanatory variable. \\(b_1 = \\text{cov}_{x,y}/\\sigma^2_x\\).\n\nFor binary predictor: The difference in the meane of non-reference and reference level.\n\n\n\\(x_{1,i}\\) The value of explanatory variable, \\(1\\), in individual \\(i\\).\n\nFor numeric predictors: The value of the explanatory variable.\n\nFor binary predictors: The value of the indicator variable.\n\n\\(x_1\\) equals 0 for the reference group.\n\n\\(x_1\\) equals 1 for the non-reference group.\n\n\n\n\n\n4. Concepts for Linear Models.\n\nObserved value (yᵢ): The actual value of the response variable: \\(y_i = \\hat{y}_i +e_i\\).\n\nResidual (eᵢ): The difference between an observed value and its model-predicted value, \\(e_i = y_i-\\hat{y}_i\\).\n\nResidual Sum of Squares: \\(\\sum e_i^2\\)\nResidual Standard Deviation: A measure of typical residual size — how far off predictions tend to be \\(\\sum e_i^2/(n-1)\\).\n\n\n\n\n🚫 5. Model Limitations\n\nExtrapolation: Making predictions outside the range of observed data — generally unsafe.\nMulticollinearity: When explanatory variables are highly correlated, making it hard to separate their individual effects.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#key-r-functions",
    "href": "book_sections/linear_models/lm_summary.html#key-r-functions",
    "title": "• 7. Linear model summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\n📈 Building Linear Models\n\nlm(): Fits linear models. Syntax: lm(response ~ explanatory, data = dataset).\naugment() ([broom]): Adds predictions and residuals to your dataset for easy exploration.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#r-packages-introduced",
    "href": "book_sections/linear_models/lm_summary.html#r-packages-introduced",
    "title": "• 7. Linear model summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nbroom: Tidies model outputs (like fitted values and residuals) into neat data frames.\nggplot2: Used for visualizing data and model fits.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/linear_models/lm_summary.html#additional-resources",
    "href": "book_sections/linear_models/lm_summary.html#additional-resources",
    "title": "• 7. Linear model summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nWeb resources:\n\nLinear modelling: introduction. From Analysing Data using Linear Models by Stéphanie M. van den Berg.",
    "crumbs": [
      "7. Linear Models",
      "• 7. Linear model summary"
    ]
  },
  {
    "objectID": "book_sections/ordination.html",
    "href": "book_sections/ordination.html",
    "title": "8. Ordination",
    "section": "",
    "text": "Let’s get started with ordination!\nA common set of tools, known as ordination methods, summarize high-dimensional datasets into a few major axes of variation. These summaries can be incredibly informative, revealing key patterns in the data. For example, John Novembre showed that summarizing whole-genome data by its major axes of variation revealed a structure in European genetic variation that closely mirrors the geographic map of Europe. This example highlights the best of ordination methods because it:\nWe will work through the intuition and mechanics of how to conduct a few standard ordination techniques. Our focus will be on building a conceptual understanding and a pragmatic “know-how”, rather than a rigorous mathematical foundation. Along the way, we’ll also wrestle with practical questions. For example “What do we do about missing data?”, “Should we scale our variables?”, and “When are two variables redundant? (and why should I care?)”\nAs usual, we conclude by summarizing the chapter, presenting a chatbot tutor, practice questions, a glossary, a review of R functions and R packages introduced, and provide links to additional resources.",
    "crumbs": [
      "8. Ordination"
    ]
  },
  {
    "objectID": "book_sections/ordination.html#lets-get-started-with-ordination",
    "href": "book_sections/ordination.html#lets-get-started-with-ordination",
    "title": "8. Ordination",
    "section": "",
    "text": "We’ll begin with Principal Component Analysis (PCA) — a linear method that looks for the directions of greatest variance in your data. We will start with a familiar dataset: Clarkia individuals from the GC site, for which we’ve measured anther-stigma distance, petal area, and leaf water content. These traits may relate to different aspects of plant performance or pollination biology — but PCA won’t “know” that. It will simply tell us how plants vary. We split this exploration into three sections: (1) We get started with pca, (2) We look under the hood of PCA to understand a bit o how it works, and (3) We look out for common issues with PCA gone wrong.\nNext we take a quick tour of alternatives to pca, including Multiple Correspondence Analysis for nominal datasets, Factor Analysis of Mixed Data for when a dataset has both continuous and categorical variables, Principal cordinates analysis and non-metric dimensional scaling for nonlinear data, and t-SNE and UMAP for high dimnesional data.\n\n\n\n\n\n\nNovembre, J., Johnson, T., Bryc, K., Kutalik, Z., Boyko, A. R., Auton, A., Indap, A., King, K. S., Bergmann, S., Nelson, M. R., Stephens, M., & Bustamante, C. D. (2008). Genes mirror geography within europe. Nature, 456(7218), 98–101. https://doi.org/10.1038/nature07331",
    "crumbs": [
      "8. Ordination"
    ]
  },
  {
    "objectID": "book_sections/ordination/pca.html",
    "href": "book_sections/ordination/pca.html",
    "title": "• 8. PCA quick start",
    "section": "",
    "text": "PCA in R with prcomp()\nOne reason biologists are so interested in associations between variables is that very little in this world is independent. Take our parviflora RILs, for example. If you recall, these recombinant inbred lines were made to disentangle correlations between traits by mixing up genomes from two populations and allowing for multiple generations of recombination. Although this effort was largely successful – trait correlations persist in our RILs because traits are tightly linked on a chromosome or are physiologically connected. As such, for example, petal area is strongly positively related to anther stigma distance, and negatively correlated with leaf water content.\nPCA summarizes variability into major axes of variation in a sample by finding combinations of traits that explain the most variability in the data, and are independent of each other. PCA first find principal component 1 (PC1) as the linear combination of all traits that explain the most variability in our sample. It goes on to do the same for PC2, and so on. We are left with as many principal components as original traits — but all are independent and the first few principal components explain more of the variance than the last few.\nNow let’s run a quick PCA and then unpack what it’s doing. We run a PCA in R following these steps:",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA quick start"
    ]
  },
  {
    "objectID": "book_sections/ordination/pca.html#pca-in-r-with-prcomp",
    "href": "book_sections/ordination/pca.html#pca-in-r-with-prcomp",
    "title": "• 8. PCA quick start",
    "section": "",
    "text": "PCA requires complete data. If you have missing observations for individuals, you can either 1. Drop these individuals. 2. Impute (a fancy word for informed guessing) missing values with tools like missMDA. 3. Use more advanced PCA methods that can handle missing data directly, such as probabilistic PCA.\n\nFormat the data.\n\nOnly keep numeric (and continuous-ish) variables.\n\nDeal with missing data (see aside on right).\n\nMake sure variables are roughly symmetric (otherwise transform).\n\nRemove redundant variables.\n\nDecide should data be centered and scaled? (The answer is usually yes).\n\nRun PCA with the prcomp() function.\nInterpret PCA results including:.\n\nProportion variance explained,\nTrait loadings, and\n\nPC values (aka scores)\n\nVisualize the results.\nConnect visualization to the underlying biology.\n\n\nFormatting the data\nBefore we format the data let’s have a look at it\n\n\nCode for selecting data for PCA from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4pca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(made_hybrid, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc )\n\n\n\n\n\n\n\n\n\nWe usually want to “center” and “scale” the data (i.e. subtract off the mean and divide by the standard deviation). We discuss why we do (or do not) choose to do this later. For now, know that you can center and scale within prcomp(center = TRUE, scale = TRUE), or before running the PCA with the scale() function.\n\nscaled_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale)                     # Subtract mean and divide by SD\n\n\n\n\n\n\n\n\n\nRun PCA with the prcomp() function.\nThis is the easy part – if your data are centered and scaled, and there are no NA’s just give your data as an argument to the prcomp() function:\n\ngc_ril_pca &lt;- prcomp(scaled_gc_rils_4pca)         # Run the pca\n\n\n\nInterpreting PCA output\nBecause a PCA returns numerous things we want to know about (percent variance explained by each PC, trait loadings onto PCs, individual values on each PC), prcomp() returns something with an awkward structure (known as an S3 object in R). To make this easier to deal with, we will use the tidy() and augment() functions from the broom package introduced earlier.\n\nLooking into proportion variance explained\nIt is too easy to fool ourselves into looking at something very interesting that explains relatively little of the variation in our dataset. Remember that principal components sequentially explain less and less of the overall variability in our dataset. So it is always worth reporting the proportion variance captured by focal principal components. Here we see that PC1 captures roughly 30% of the variability in our data, while PC5 captures about 11% of the overall variability.\n\nlibrary(broom)                    # Load the broom library\ntidy(gc_ril_pca, matrix = \"pcs\")  # Extract percent variance explained from pc object\n\n\n\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n\n1\n1.241\n0.308\n0.308\n\n\n2\n1.137\n0.258\n0.566\n\n\n3\n0.957\n0.183\n0.749\n\n\n4\n0.838\n0.140\n0.890\n\n\n5\n0.742\n0.110\n1.000\n\n\n\n\n\nThere are a few common visualizations of the proportion of variance explained by each PC.\n\nA scree plot (Figure 1 A) shows the amount of variation soaked up by the principal component on the y-axis. Scree plots usually have points for the standard deviation attributable to each PC, and those points are connected by lines.\nA proportion variance explained plot (Figure 1 B) is very similar, but the y-axis is the proportion variance explained by each PC (that is the variance (i.e. std.dev\\(^2\\)) explained by each PC divided by the sum of standard variances), and is shown as a bar plot.\nA cumulative proportion variance explained plot (Figure 1 C) shows the cumulative sum of a proportion variance explained plot and is shown with points and lines. In this case, the value for PC2 equals the sum of the proportions of variance explained by PC1 and PC2.\n\n\n\nCode for making scree plots and PVE plots.\nlibrary(patchwork)\n\npc_summary &lt;- tidy(gc_ril_pca, matrix = \"pcs\")\n\nscree_plot &lt;- ggplot(pc_summary, aes(x = PC, y = std.dev))+\n  geom_point(size = 3)+\n  geom_line(lty= 2)+\n  ggtitle(\"A) Scree plot\")\n\npve_plot &lt;-  ggplot(pc_summary, aes(x = PC, y = percent))+\n  geom_col()+\n  ggtitle(\"B) PVE plot\")\n\ncumulative_pve_plot &lt;- bind_rows( \n  tibble(PC = 0 , std.dev = 0, percent = 0, cumulative = 0),\n  pc_summary)|&gt;\n  ggplot( aes(x = PC, y = cumulative))+\n  geom_point(size = 3)+\n  geom_line(lty= 2)+\n  ggtitle(\"C) Cumulative PVE\")\n\nscree_plot + pve_plot  + cumulative_pve_plot \n\n\n\n\n\n\n\n\nFigure 1: Visualizing percent variance explained by PCs. Principal component analysis (PCA) reduces dimensionality by compressing variance into the first principal components. The plots above show how much variability is explained by each PC. A is a scree plot showing the standard deviation of each principal component, reflecting how much variation each one captures from the original data. B displays a bar plot of the proportion of total variance explained by each component. C shows the cumulative proportion of variance explained, beginning at zero and increasing with each additional PC, reaching 100% by the fifth component.\n\n\n\n\n\n\n\nLooking into trait loading\n\n\n\n\n\nLoading of each trait onto each PC\n\nSo about 30% of the variability is captured by PC1, but what even is PC1? I’m glad you asked this question! PC1 (like all principal components) is a combination of variables each weighted by a certain amount. For PC1, this combination point in the direction of the most variability in the (scaled and centered) data set. In this case, the value of PC1 in individual, \\(i\\), equals:\npetal_area x 0.56 + asd x 0.48 + growth_rate x 0 + stem_dia x 0.43 + lwc x -0.53.\nHere each trait refers to the scaled and centered value in individual \\(i\\). Similarly, “loadings” of traits on each PCA are presented in the table to the right and in Figure 2.\n\nlibrary(broom)                            # load broom package\ntidy(gc_ril_pca, matrix = \"loadings\") |&gt;  # Get trait loadings\n  arrange(PC)                             # Sort the data so we see PC1 first\n\n\n\nCode for visualizing trait loadings onto PCs\nlibrary(forcats)\ngc_ril_pca |&gt;\n    tidy(matrix = \"loadings\") |&gt;\n    mutate(variable = fct_reorder(column, -1*value * as.numeric(PC==1),.fun = max))|&gt;\n    ggplot(aes(x = PC, y = variable, fill= value))+\n    geom_tile(color = \"black\")+\n    scale_fill_gradient2(low = \"firebrick\", mid = \"white\", high = \"steelblue\", midpoint = 0) +\n    geom_text(aes(label = round(value, digits = 3)))+\n  labs(fill = \"Loading\")+\n  scale_x_discrete(expand = c(0,0))+\n  scale_y_discrete(expand = c(0,0))+\n  theme(axis.title.y = element_blank(),\n        plot.background = element_rect(fill =  \"white\",color =\"white\"),\n        panel.background  = element_rect(fill =  \"white\",color =\"white\"),\n        legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"))\n\n\n\n\n\n\n\n\nFigure 2: Trait loadings on each PC axis.\n\n\n\n\n\n\n\nLooking into PC values\nWe can now find each individual’s value on each PC1. To do so, we\n\nTake the scaled and centered values of each trait,\n\nMultiply this by the trait loading on a given PC,\n\nTake the sum.\n\nRinse and repeat for all individuals on all PCs\n\nSo individual 1’s value for PC1 is:\n(0.56 x -1.26) + (0.48 x -1.09) + (0 x 0.54) + (0.43 x -0.65) + (-0.53 x -0.31).\nThis equals -1.33. Rather than repeating this for all individuals and all PCs, we can find these values with the augment() function in the broom package:\n\naugment(gc_ril_pca)\n\n\n\n\n\n\n\n\n\n\nVisualizing PCs\nNow that we know what our PCA means, let’s have a look at our results. Below, I show how to use the autoplot() function in the ggfortify package. I present four plots:\n\nAll give the prcomp() output to the autoplot() function to make a plot.\n\nAll find the proportion variance explained by PCs on the x and y axes, and set the “aspect ratio” as \\(\\text{pve}_\\text{y}/\\text{pve}_\\text{x}\\).\n\nWhy mess with aspect ratio? In PCA, each axis (e.g., PC1 and PC2) explains a different amount of variance, so an honest plot presents axes proportional to their importance. Otherwise, distances and angles in the plot can be misleading. Setting the aspect ratio as the ratio of proportion of variance explained, to prevent such misinterpretation.\n\n\nThe plot in tab two (Trait loadings) generates a classic “pca biplot” by adding trait loadings.\n\nThe plot in tab three (+ categories) uses color to add a categorical variable to the plot. This allows us to see if some categorical variable e.g., species (or in this case, if a flower made at least one hybrid) is associated with some part of PC space.\n\nThe plot in tab four (PC1 v PC5) shows that we can use x and y arguments in autoplot() to show other PCs.\n\n\nPC1 v PC2Trait loadings+ categoriesPC1 v PC5\n\n\n\n\nlibrary(ggfortify) \npc_var_explained &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npercent_pc1 &lt;- pc_var_explained |&gt; filter(PC == 1) |&gt;  pull(percent)\npercent_pc2 &lt;- pc_var_explained |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca, size=4, alpha = .7)+\n  theme(aspect.ratio = percent_pc2 / percent_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc2 &lt;- pve |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca,  size=4, alpha =.7,\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 7)+\n  theme(aspect.ratio = pve_pc2 / pve_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc2 &lt;- pve |&gt; filter(PC == 2) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca,data = gc_rils_4pca, color = 'made_hybrid', \n         size = 5, alpha = .8,\n         loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 5)+\n  scale_color_manual(values = c(\"brown\",\"forestgreen\"))+\n  theme(aspect.ratio = pve_pc2 / pve_pc1, \n        legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggfortify) \npve &lt;- tidy(gc_ril_pca, matrix = \"pcs\") \npve_pc1 &lt;- pve |&gt; filter(PC == 1) |&gt;  pull(percent)\npve_pc5 &lt;- pve |&gt; filter(PC == 5) |&gt;  pull(percent)\n\nautoplot(gc_ril_pca, x = 1, y = 5, size=4, alpha =.7,\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 5)+\n  theme(aspect.ratio = pve_pc5 / pve_pc1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiological Interpretation of PCA plots\nNow that we can create and interpret PCA plots, let’s return to the underlying biological questions they can address. Some common questions to ask are:\n\nAre individuals clustered or spread out?\n\nIn our case individuals are quite spread out. This lets us know that recombination during the creation of our RILs did a reasonable job of dissecting the underlying traits.\n\nClustering would suggest that there is some underlying structure in the data. If clustering was noticeable, we would want to know which traits are driving that structure.\n\nWhich traits influence the main axes? In the biplot, look at the arrows:\n\nWhich traits are most aligned with PC1 or PC2?\n\nAre any traits opposing each other?\n\nDo categories separate in PCA space? In our case we see that nearly all observations in the bottom left of our plot set no hybrid seeds, while the top right (aligned with large petal area) seemed to have a lot of hybrids.\nIs anything interesting happening beyond PC1 and PC2? Check out other PCs (like PC5) to see if subtle patterns or groupings emerge that aren’t visible on the first two axes. But beware, do not overinterpret patterns in PCs that explain very little variability. \nDo you notice a “horseshoe” shape? Sometimes points a PCA plot form a curved or arched pattern. This is a known artifact that often shows up when PCA tries to fit a curved pattern using straight-line axes. This can be misleading – samples at the two ends of the gradient may appear close together even though they’re quite different. When you see a horseshoe, consider transforming key variables or using an alternative ordination approach less sensitive to this issue.\n\n\nNow that we can make and interpret PCA plots, let’s look under the hood and see how they work.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA quick start"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html",
    "href": "book_sections/ordination/pcaII.html",
    "title": "• 8. PCA deeper dive",
    "section": "",
    "text": "How PCA Works\nWe just ran a PCA, looked at plots, and interpreted what the axes meant in terms of traits in our parviflora RILs. We also introduced the core idea: PCA finds new axes — principal components — that are combinations of traits, oriented to explain as much variation as possible. These components are uncorrelated with one another, and together they form a rotated coordinate system that helps us summarize multivariate variation.\nThe description of PCA and interpretation of results presented in the previous chapter are the bare minimum we need to understand a PCA analysis. Here, we will go beyond this bare minimum – dipping into the inner workings of PCA to better understand how it works and what to watch out for. This is essential for critically interpreting PCA results. A challenge here is that there is a bunch of linear algebra under the hood – and while I love linear algebra, I don’t expect many of you to know it. So, we will try to understand PCA without knowing linear algebra.\nWe will look at the two key steps in running a PCA.\nCode for selecting data for PCA from RILs planted at GC\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4pca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\", !is.na(prop_hybrid), ! is.na(mean_visits))|&gt;\n  select(made_hybrid, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc )",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#how-pca-works",
    "href": "book_sections/ordination/pcaII.html#how-pca-works",
    "title": "• 8. PCA deeper dive",
    "section": "",
    "text": "Making a covariance (or correlation) matrix.\n\nFinding Principal Components.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#building-a-covariance-or-correlation-matrix.",
    "href": "book_sections/ordination/pcaII.html#building-a-covariance-or-correlation-matrix.",
    "title": "• 8. PCA deeper dive",
    "section": "Building a covariance (or correlation) matrix.",
    "text": "Building a covariance (or correlation) matrix.\nAfter removing categorical variables from our data, the first step in PCA (if we do this without using prcomp()) is to build a matrix that shows the association between each trait in our data set.\nWe usually want to give each trait the same weight so we either center and scale (for each value of a trait we subtract the mean and divide by the trait’s standard deviation) and then find the covariance between each trait with the cov() function.\n\n\nMaking a covariance matrix from a centered and scaled data set, provides the exact same result as making a correlation matrix from the original data with the cor() function. So sometimes people talk about PCs from the correlation matrix (i.e. centered and scaled data), or the covariance matrix (i.e. centered but not scaled data).\n\n# Scaling the data\nscaled_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale)                     # Subtract mean and divide by SD\n\n# Finding all pairwise covariances on centered and scaled data\ncor_matrix &lt;- cov(scaled_gc_rils_4pca)\n\nA few simple patterns jump out from this correlation matrix:\n\nAll values on the diagonal equal one. That’s because all traits are perfectly correlated with themselves (this is true of all correlation matrices).\nThe matrix is symmetric – values on the bottom triangle are equal to those on the top triangle (this is also true of all correlation matrices).\n\nSome traits are positively correlated with each other – e.g. there is a large correlation between anther stigma distance and petal area – perhaps because they both describe flower size.\nSome traits are negatively correlated with each other– e.g. nearly all other traits are negatively correlated with leaf water content.\n\n\n\nCode for visualizing trait correlations\nlibrary(tibble)\nlibrary(forcats)\ncor_matrix |&gt;\n  data.frame()|&gt;\n  rownames_to_column(var = \"trait_1\") |&gt;\n  pivot_longer(-trait_1, names_to = \"trait_2\", values_to = \"cor\")|&gt;\n  mutate(trait_1 = str_remove(trait_1,\"_mm\"),\n         trait_2 = str_remove(trait_2,\"_mm\"))|&gt;\n  mutate(trait_1 = fct_relevel(trait_1, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         trait_2 = fct_relevel(trait_2, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         cor = round(cor,digits = 3))|&gt;\n  ggplot(aes(x = trait_1, y=trait_2, fill = cor))+\n  geom_tile(color = \"black\")+\n  scale_fill_gradient2(\n    high = \"#0072B2\",    # Blue (colorblind-safe)\n    mid = \"white\",       # Center\n    low = \"#D55E00\",     # Orange (colorblind-safe)\n    midpoint = 0)+\n  geom_text(aes(label = cor),size = 5)+\n  labs(title=\"Trait correlations\")+\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Pairwise correlations among traits in parviflora recombinant inbred lines. Each cell shows the covariance of scaled and centered data (i.e. correlation) between two traits, with color indicating strength and direction of association (blue = positive, orange = negative). Traits include stem diameter, petal area, anther–stigma distance (asd), growth rate, and leaf water content (lwc). The matrix is symmetric, and all diagonal values are 1 by definition.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#finding-principal-components",
    "href": "book_sections/ordination/pcaII.html#finding-principal-components",
    "title": "• 8. PCA deeper dive",
    "section": "Finding Principal Components",
    "text": "Finding Principal Components\nIn PCA, we’re trying to find new axes that summarize the patterns of variation in our dataset. These principal components are chosen to explain as much variance as possible. As we have seen, each principal component is a weighted combination of the original traits. We have also seen that each principal component “explains” a given proportion of the variation in the multivariate data.\nHow does PCA find combinations of trait loading that sequentially explain most of the variance? Traditionally, PCA is done by first calculating a covariance or correlation matrix (as above), then performing eigenanalysis on that matrix to find the loadings and the proportion of variance explained. Eigenanalysis is a bit like “ordinary least squares” (OLS) for finding a best-fit line, with a few key differences:\n\n\nNot quite eigenanalysis Rather than using eigenanalysis, prcomp() and most modern PCA approaches use singular value decomposition (SVD). prcomp() and eigen() both help us find principal components, but they work differently. eigen() is the traditional method: it takes a covariance or correlation matrix and performs eigen decomposition to find the axes of greatest variation. prcomp(), on the other hand, uses singular value decomposition (SVD) directly on the original (centered and optionally scaled) data matrix. This makes it more numerically stable and slightly faster, especially for large datasets. Both methods give nearly identical results if the data is prepared the same way, though the signs of loadings may differ.\n\nEigenanalysis usually deals with more than two variables.\n\nThere’s no split between “explanatory” and “response” variables.\n\nInstead of minimizing the vertical distance between observed and predicted values of y (as in OLS), eigenanalysis finds directions that minimize the sum of squared perpendicular distances — in multidimensional space — from each point to the axis (principal component).\n\nIn R we can conduct an eigenanalysis with the eigen() function.\n\nEigenvalues are the variance explained\nHere the eigen values are the variance attributable to each principal component, and the square root of these values match the standard deviation provided by prcomp() (see below and the previous section):\n\nlibrary(broom)\neigen_analysis &lt;- eigen(cov(scaled_gc_rils_4pca))\npca_analysis   &lt;- prcomp(scaled_gc_rils_4pca)\n\ntidy(pca_analysis, matrix = \"pcs\")|&gt;\n  mutate(eigen_var = eigen_analysis$values ,\n         eigen_sd = sqrt(eigen_var))|&gt;\n  select(-percent, - cumulative)|&gt;\n  relocate(std.dev, .after = eigen_sd)\n\n\n\n\n\n\nPC\neigen_var\neigen_sd\nstd.dev\n\n\n\n\n1\n1.540\n1.241\n1.241\n\n\n2\n1.292\n1.137\n1.137\n\n\n3\n0.915\n0.957\n0.957\n\n\n4\n0.702\n0.838\n0.838\n\n\n5\n0.551\n0.742\n0.742\n\n\n\n\n\n\n\nEigenvectors are (nearly) the trait loadings\n\n\n\n\n\n\n\n\n\nFigure 2: Comparing PCA loadings from prcomp() and eigenvectors from eigen(). Each panel shows one principal component (PC1–PC5). For each trait, the x-axis shows its loading from prcomp(), and the y-axis shows the corresponding value in the eigenvector from eigen(). The dashed lines indicate the near-perfect agreement between the two methods, differing only in sign. This demonstrates that (when everything is working right) prcomp() and eigen() produce equivalent results (up to sign) when the data are properly centered and scaled.\n\n\n\n\nThe trait loadings from eigenanalysis are basically the same as those from prcomp(). Figure 2 shows that, in our case, the trait loadings on each PC are the same, with the caveat that the sign may flips. This doesn’t change our interpretation (because PCs are relative, so sign does not matter), but is worth noting.\n\n\nFind PC values by adding up each trait weighted by its loading\nWe already covered this.. If you like linear algebra, you can think about this as the dot product of the scaled and centered trait matrix and the matrix of trait loadings.\n\nThe problem(s) with missing data in PCA. Missing data causes two problems in PCA.\n\nYou cannot simply make a covariance or correlation matrix with missing data. This can be computationally overcome as follows cor( use = \"pairwise.complete.obs\"), but is only reliable if data are missing at random.\n\nYou cannot find PC values for individuals with missing data. Again there are computer tricks to overcome this, but they must be considered in the context of the data:\n\n\nYou can set missing values to zero. But this brings individuals with a bunch of missing values closer to the center of your PC plot.\n\n\n\nYou can drop individuals with missing data. But again, if patterns of missingness are non-random this can be a problem.\n\n\n\n\nYou can impute (smart guess) the missing values (using packages like missMDA, or mice) or use a PCA approach like emPCA or probabilistic PCA that do this for you. But then you are working on guesses of what your data may be and not what they are.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaII.html#why-we-usually-scale-our-variables",
    "href": "book_sections/ordination/pcaII.html#why-we-usually-scale-our-variables",
    "title": "• 8. PCA deeper dive",
    "section": "Why we usually scale our variables",
    "text": "Why we usually scale our variables\nWe usually center and scale our variables so that our PC gives equal value to all variables. Sometimes we may not want to do this – perhaps we want to take traits with more variability more seriously. While this might be ok if variables are on very similar scales, it can be quite disastrous. Now that we’ve seen how PCA works using scaled data, let’s explore what happens when we skip the scaling step.\nIn our GC RIL data for example, petal area (measured in \\(mm^2\\)) has a much greater variance than any other trait (because the covariance of a variance with itself is the variance). You can see this on the main diagonal of Figure 3 where the variance in petal area is 216, while the second largest value in this matrix is less than two.\n\ncentered_gc_rils_4pca &lt;- gc_rils_4pca |&gt;\n  select(-made_hybrid)|&gt;                # Remove categorical variables\n  mutate_all(scale, scale = FALSE)      # Subtract mean and divide by SD\n\n# Finding all pairwise covariances on centered (but not scaled) data\ncov_matrix &lt;- cov(centered_gc_rils_4pca)\n\n\n\nCode for visualizing trait covariances\nlibrary(tibble)\nlibrary(forcats)\ncov_matrix |&gt;\n  data.frame()|&gt;\n  rownames_to_column(var = \"trait_1\") |&gt;\n  pivot_longer(-trait_1, names_to = \"trait_2\", values_to = \"cov\")|&gt;\n  mutate(trait_1 = str_remove(trait_1,\"_mm\"),\n         trait_2 = str_remove(trait_2,\"_mm\"))|&gt;\n  mutate(trait_1 = fct_relevel(trait_1, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         trait_2 = fct_relevel(trait_2, \"stem_dia\",\"petal_area\",\"asd\",\"growth_rate\",\"lwc\"),\n         cov = round(cov,digits = 3))|&gt;\n  ggplot(aes(x = trait_1, y=trait_2, fill = cov))+\n  geom_tile(color = \"black\")+\n  scale_fill_gradient2(\n    high = \"#0072B2\",    # Blue (colorblind-safe)\n    mid = \"white\",       # Center\n    low = \"#D55E00\",     # Orange (colorblind-safe)\n    midpoint = 0)+\n  geom_text(aes(label = cov),size = 5)+\n  labs(title=\"Trait covariances\")+\n  theme(legend.position = \"bottom\",\n        legend.key.width = unit(2, \"cm\"),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nFigure 3: Covariance matrix of traits in parviflora RILS before scaling. Each cell shows the covariance between a pair of traits, with color indicating magnitude (darker blue = larger covariance). The diagonal shows trait variances. Petal area dominates the matrix with a variance over 215, while all other trait variances are below 2. This disparity illustrates how unscaled PCA can be overwhelmed by traits with larger absolute variation.\n\n\n\n\n\nNow we can see the consequences of not scaling or centering on our PCs:\n\ngc_ril_centered_pca &lt;- prcomp(centered_gc_rils_4pca)\n\nBecase the variance in petal area dominates the covariance matrix, PC1 explains 99.85% of the variability in the data (Table 1) and maps perfectly on to petal area (Table 2).\n\n\n\n# Finding proportion \n# variance explained by PC\ngc_ril_centered_pca   |&gt;\n  tidy(matrix = \"pcs\")\n\n\n\n\n\nTable 1: PVE for unscaled data.\n\n\n\n\n\n\nPC\nstd.dev\npercent\ncumulative\n\n\n\n\n1\n14.6898\n0.9985\n0.9985\n\n\n2\n0.4104\n0.0008\n0.9993\n\n\n3\n0.3511\n0.0006\n0.9999\n\n\n4\n0.1507\n0.0001\n1.0000\n\n\n5\n0.0196\n0.0000\n1.0000\n\n\n\n\n\n\n\n\n\n\n# Finding trait loadings\ngc_ril_centered_pca         |&gt;\n  tidy(matrix = \"loadings\") |&gt; \n  filter(PC==1)\n\n\n\n\n\nTable 2: Trait loadings for unscaled data.\n\n\n\n\n\n\ncolumn\nPC\nvalue\n\n\n\n\npetal_area_mm\n1\n0.9999\n\n\nasd_mm\n1\n0.0076\n\n\ngrowth_rate\n1\n-0.0067\n\n\nstem_dia_mm\n1\n0.0010\n\n\nlwc\n1\n-0.0003\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a sense of how PCA works, lets think harder about what to worry about",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA deeper dive"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html",
    "href": "book_sections/ordination/pcaIII.html",
    "title": "• 8. PCA – Gotchas",
    "section": "",
    "text": "Proceed Cautiously Ahead\nNow that we can run a PCA and know how they work, let’s think hard about how to interpret PCA results. These expand on warnings I made in the PCA quickstart and/or in the previous chapter, but now we can approach them with a bit of sophistication. I first start with a list of things to worry about. You should ask these questions of every PCA you see. Next I look more deeply into the idea that PCA goes after the variance in the data, so the data matter a lot.\nI previously listed some first things to do when you see a PCA. These include:",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html#proceed-cautiously-ahead",
    "href": "book_sections/ordination/pcaIII.html#proceed-cautiously-ahead",
    "title": "• 8. PCA – Gotchas",
    "section": "",
    "text": "Understanding the structure of the data.\n\nConnecting this structure to a biological interpretation (but watch out for artifacts – see below).\nThinking about what could have gone wrong in this PCA (I specifically noted to be wary of a horseshoe shape).\n\n\nPCA evaluation checklist:\n Could structure be an artifact? \nIn large scale datasets (such as -omics, or cases in which many people collected data or data are stratified), some portion of the variance might be caused by artifacts like who collected the data, which run of an instrument was used, what time the data was generated etc. rather than the motivating idea.\n\n\nYour life will be better if you try to randomly spread such potential artifacts randomly across your more exciting biological factors, rather than identifying their potential impact later.\nBefore rushing to interpret results biologically always look into the boring explanations. You can check for these issues by making exploratory data visualizations or evaluating associations between such boring variables (observer, batch, time of day etc) and PCs of interest.\n\n Was the interpretation of 2D shape justified? \nSometimes PCA plots show compelling shapes — curves, clusters, gradients — that are tempting to interpret biologically. But be cautious: these shapes don’t always reflect complex biology. For example, Luca Cavalli-Sforza famously used PCA to study human genetic variation. While foundational, some of his interpretations of the shapes in PC space were unjustified — Novembre & Stephens (2008) found that these patterns can arise from much simpler processes. Similarly, 2D structure can arise from nonlinearity in just one dimension.\nWhen you see an interesting shape in a PCA plot, don’t jump straight to a complex biological story. Ask if it could be generated by a simple process.\n\n Could the way in which missing data were handled impact PCs? \nIf you want to run a PCA with missing data you have some decisions to make. Ideally, no data are missing, but of course this is not always possible – for example in genomic analyses, data are often missing because not all sites are sequenced in all individuals.\n\nAssigning each missing data point to the mean for that trait is a common practice, but if some individuals are missing more data than others they will be brought towards the center of PC space. This can be misleading — it makes individuals with lots of missing data appear artificially close to the center of PC space, even if their true trait values are far from average. This is a known for the types of population genomic analyses I do (Yi & Latch (2022)).\n\nAlternatively imputing the missing data can give us more confidence in the shape of our PCs than we deserve and could mislead us by assigning wrong values to an individual’s traits.\n\nOne simple check is to plot the value of a PC on the y-axis against the proportion of missing data on the x-axis. Any linear or nonlinear trend is worrying.\n\n Did you do something silly?  This is somewhat embarrassing but I almost added id as a variable in this PCA. Of course id should not be numeric, and is meaningless. So if I made this mistake and got super lucky there is just some noise in PC1. But it could be way worse, if id non-randomly assigned, it gets a meaningful weighting in PC space and we get super confused. So be sure you don’t make any such silly mistakes.\n\n Should data (not) be scaled? \nWe discussed this at length in the previous section — we usually scale variables so that differences in scale and variability don’t mislead us. But do we always want to give equal weight to variables with very different amounts of variance?\nTake a PCA based on SNPs: scaling the data means that variation at a locus with a very rare allele is treated as just as important as variation at a highly polymorphic locus. That might not be a good idea. In some cases — like when differences in variance reflect meaningful biological differences, or when variables are on the same scale already — you may want to skip scaling. But be cautious – Lever et al. (2017) suggest that when variables are similar, we should not scale the data.\n\n Are variables redundant? \nPCA is most useful when variables are correlated, since it finds axes that summarize that structure. that capture the structure of correlations in the data. But data should not be redundant.\nFor example the full Clarkia dataset contains measurements of both petal area and petal circumference. Such variables are very similar, so putting them both into a PCA is essentially double-counting flower size. This redundancy means that a PCA will allocate more variance to the shared dimension, exaggerating its importance. Consider reducing highly correlated traits to one value (e.g. their mean) or removing the less interpretable variable. How correlated is too correlated? There is no hard and fast answer, but I suggest considering collapsing or removing variables when the absolute value of their correlation, \\(|r|\\), exceeds 0.9.\n\n What data went into the PCA and how even was sampling?  PCA finds combinations of traits that best capture the variance in the data. Thus, is quite sensitive to the sampling scheme. I expand on this below!",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcaIII.html#pca-depends-on-what-you-put-into-it",
    "href": "book_sections/ordination/pcaIII.html#pca-depends-on-what-you-put-into-it",
    "title": "• 8. PCA – Gotchas",
    "section": "PCA depends on what you put into it",
    "text": "PCA depends on what you put into it\nWhen John Novembre showed that a PCA of genetic variation of Europeans looked a lot like a map of Europe, he knew what he was doing. He made a few specific decisions in his sampling and analysis, including – making sure that all sampled individuals had all four great grandparents from the same location, and that sampling was even across Europe. Had John sampled mostly Spaniards with a few other individuals spread around from across Europe, PCs would not make a map of Europe - but rather a somewhat distorted map of Spain. This is all to say that the details of sampling impact results of a principal component analysis.\nJust as sampling matters, so does the choice of what variables go into the PCA. John used many genetic markers from across the genome. A PCA based on the ABO locus would find clusters of blood groups, while a PCA based on morphology would look different in some other way.\nFigure 1 uses data from a natural hybrid zone between Clarkia xantiana subspecies to illustrate the importance of sampling effort. Panels A-C show a PC-plot based off of three floral traits – petal area, anther stigma distance, and the time between pollen release and ovule receptivity – when different numbers of parviflora and xantiana are sampled. The top left panel (Figure 1 A) shows that with 45 samples of each subspecies from site SAW, PC1 captures 84.6% of the variance in the data. The three “xantiana” samples with PC1 values near zero may be first generation hybrids. Figure 1 shows that the variance attributable to PC1 decreases when sampling 45 parviflora and 3 xantiana, and Figure 1 C shows that this decreases further when sampling 3 parviflora and 45 xantiana. Figure 1 D shows the average proportion variance explained by PC1 across different sampling efforts. This illustrates the broader point: PCA doesn’t tell us about populations, but about variation in the data we choose and how we sample.\n\n\n\n\n\n\n\n\nFigure 1: Sampling effort alters the results of PCA. Panels A–C show principal component plots based on three floral traits in Clarkia xantiana (petal area, anther–stigma distance, and timing of pollen release vs. ovule receptivity). Data are from the “Sawmill Road” hybrid zone, with samples from both subspecies: parviflora is represented by red “P”s and xantiana by blue “X”s. Each panel presents a different sampling configuration: Panel A includes 45 individuals of each subspecies, Panel B includes 45 parviflora and 3 xantiana, and Panel C includes 3 parviflora and 45 xantiana. The proportion of total variance explained by PC1 decreases as the sampling becomes more uneven. Panel D summarizes this trend across combinations of sample sizes ranging from 1 to 45 for each subspecies. The color scale represents the average proportion of variance explained by PC1 across ten subsampling efforts for each parameter combination. Red letters A, B, and C correspond to Panels A–C.\n\n\n\n\n\n\n\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component analysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nNovembre, J., & Stephens, M. (2008). Interpreting principal component analyses of spatial population genetic variation. Nature Genetics, 40(5), 646–649. https://doi.org/10.1038/ng.139\n\n\nYi, X., & Latch, E. K. (2022). Nonrandom missing data can bias principal component analysis inference of population genetic structure. Mol Ecol Resour, 22(2), 602–611. https://doi.org/10.1111/1755-0998.13498",
    "crumbs": [
      "8. Ordination",
      "• 8. PCA -- Gotchas"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html",
    "href": "book_sections/ordination/pcalternatives.html",
    "title": "• 8. PCAlternatives",
    "section": "",
    "text": "When data aren’t (all) continuous\nThe PCA framework is enormously popular because it provides a straightforward approach for working with multidimensional data. This is becoming even more relevant as the “big data” or “omic” era makes the collection of massive datasets commonplace. However, despite the incredible popularity of principal component analysis, it is not always the right tool for the job. PCA works by combining all traits into new “summary traits” - linear combinations of the originals, each with its own set of weightings. This means it assumes that the main patterns in the data can be captured by adding up traits in the right proportions. But this assumption doesn’t always hold. When data are nonlinear, categorical, or behave strangely (as is often the case in ecological datasets), PCA can give misleading results.\nIn this section, we introduce alternatives to PCA. Each alternative aims to solve a specific limitation of PCA. But before diving too deep into this section I want to warn you that this is both non-exhaustive (there are even more PCA-like approaches), and fairly superficial (I only briefly discuss these techniques). My goal here is to give you enough information to know what these methods do, when and why they are used, how they differ from PCA (and each other), and references / links so you can learn more.\nPCA assumes that data are continuous and have nice distributions. Although PCA can still be run when these assumptiosn are not met (John Novembre’s map of Europe was generated from 0/1/2 data – noting the number of ‘alternative’ alleles at a locus), there are PCA -like approaches made for categorical data and for a mix of data types.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#when-data-arent-all-continuous",
    "href": "book_sections/ordination/pcalternatives.html#when-data-arent-all-continuous",
    "title": "• 8. PCAlternatives",
    "section": "",
    "text": "Multiple Correspondence Analysis (MCA)\nMCA is appropriate when all your variables are categorical, especially nominal variables with multiple categories. MCA applies a method similar to PCA to uncover patterns of association among the categorical variables and reduce dimensionality.\nYou can run an MCA using the mca() function from the MASS package in R, or for more in-depth analyses and nicer figures you may want to use the MCA function in the factoMineR package. You interpret the output much like a PCA. That is, each axis summarizes shared patterns across the variables, and the biplot shows which samples and categories tend to cluster together. Nice and clear start! Here are a few suggested edits to tighten grammar, clarify the meaning, and better match the tone and structure from earlier in your book:\nThe code below and the resulting figure (Figure 1) show a worked example using the parviflora RIL dataset. The MCA analysis includes three categorical traits: visited (whether the plant received any visits), petal_color, and petal_area (binned as small / medium / large). Points are colored by whether each plant made at least one hybrid. Dim 1 (which is like PC1) separates plants with pink petals and visits from those with white petals and no visits — and aligns closely with whether or not the plant made a hybrid.\n\n\nCode for selecting data for MCA from RILs planted at GC\nlibrary(forcats)\nril_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_rils.csv\"\nril_data &lt;- readr::read_csv(ril_link) |&gt;\n  dplyr::mutate(growth_rate = case_when(growth_rate ==\"1.8O\" ~ \"1.80\",\n                                          .default = growth_rate),  \n                growth_rate = as.numeric(growth_rate),\n                made_hybrid = prop_hybrid &gt; 0)\n\ngc_rils_4_mca  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\")|&gt;\n  mutate(visited = ifelse(mean_visits&gt;0,\"visited\",\"not visited\"), \n         made_hybrid = ifelse(made_hybrid,\"made hybrid\",\"no hybrid\"),\n         petal_area_qual = cut(petal_area_mm,c(0,53.5,66.3,Inf)))  |&gt;\n  mutate(petal_area_qual = fct_recode(petal_area_qual, \n                                       small_petal = \"(0,53.5]\", \n                                       medium_petal = \"(53.5,66.3]\",\n                                       large_petal = \"(66.3,Inf]\"))|&gt;\n  dplyr::select(made_hybrid,  visited , petal_color, petal_area_qual)\n\n\n\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(forcats)\nlibrary(patchwork)\nmca_result &lt;- gc_rils_4_mca |&gt; \n  mutate_all(as.factor)|&gt;                      # The MCA function needs variables to be factors\n  MCA(quali.sup = \"made_hybrid\",graph = FALSE) # quali.sup  means ignore this column (made hybrid)\n                                               # we want this to look at our MCA, but not to go into MCA\n# Visualize individuals (samples)\nmca_plot &lt;- fviz_mca_ind( mca_result,  habillage = \"made_hybrid\", \n    repel = TRUE,col.var = \"darkgrey\" ,palette = \"Dark2\")+\n  theme(legend.position = \"bottom\", aspect.ratio = 25/37)\nmca_loadings&lt;- fviz_mca_var(mca_result, repel = TRUE,col.quali.sup = \"black\")\nmca_plot + mca_loadings\n\n\n\n\n\n\n\n\n\nFigure 1: Multiple Correspondence Analysis (MCA) of categorical traits in parviflora RILS. Left panel: Individuals are plotted in MCA space based on Dim 1 (37% variance explained) and Dim 2 (25%). Each point represents a RIL, colored by whether it made hybrid seed (made_hybrid) or not. Lines connect individual labels to the data point they are assocaited with. Right panel: Positions of each variable category in the same MCA space. Categories closer together (e.g., pink and visited) tend to co-occur. Dim 1 separates lines with small petals, no visits, and no hybrids (left) from those with larger petals and more visits (right).\n\n\n\n\n\n\n\nFactor Analysis of Mixed Data (FAMD)\nSo, PCA works for continuous variables and MCA works for categorical variables, but what if your dataset has both? Don’t despair, Factorial Analysis of Mixed Data (FAMD) allows us to analyze datasets that include both quantitative and categorical variables. FAMD is a blend of PCA and MDA giving appropriate weights to each. Running this on our parviflora RILs at GC with categorical variables visited and petal_color, and continuous variables petal_area_mm, asd_mm, growth_rate, stem_dia_mm, and lwc, we see that large values in Dim 1 are associated with making hybrid seed ( Figure 2 ).\nFAMD scales and balances the contributions of each variable type so that no one type dominates. You interpret the output like PCA: samples that cluster together in the reduced space tend to share trait combinations — whether continuous, categorical, or both. You can run FAMD with the FAMD() function in the FactoMineR package and make plots with functions in factoextra package.\n\n\nCode for selecting data for FAMD from RILs planted at GC\ngc_rils_4_famd  &lt;- ril_data |&gt;\n  na.omit()|&gt;\n  filter(location == \"GC\")|&gt;\n  mutate(visited = ifelse(mean_visits&gt;0,\"visited\",\"not visited\"), \n         made_hybrid = ifelse(made_hybrid,\"made hybrid\",\"no hybrid\"))|&gt;  \n  select(made_hybrid, visited, petal_color, petal_area_mm, asd_mm, growth_rate,  stem_dia_mm, lwc)\n\n\n\nlibrary(FactoMineR);  library(factoextra);  library(forcats); library(patchwork)\nfamd_result &lt;- FAMD(gc_rils_4_famd, sup.var = \"made_hybrid\" ,graph = FALSE) # Make FAMD\n\n# Visualize results\nfamd_plot &lt;- fviz_famd_ind( famd_result,  habillage = \"made_hybrid\", \n    repel = TRUE,palette = \"Dark2\",col.quali.var = \"white\")   +     # Plot inds\n  theme(legend.position = \"bottom\", aspect.ratio = 21.1/25.9)         \nfamd_scree_plot &lt;- fviz_screeplot(famd_result)\nfamd_quant_loadings&lt;- fviz_famd_var(famd_result, \"quanti.var\",repel = TRUE) # Plot loadings of quantitative traits\nfamd_qual_loadings&lt;- fviz_famd_var(famd_result, \"quali.var\",repel = TRUE,col.quali.sup = \"black\") # Plot loadings of qualitative traits\n\n# Combine plots\n(famd_plot + famd_quant_loadings) / (famd_scree_plot+ famd_qual_loadings) + \n  plot_layout(heights = c(3, 2))\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#pcalternatives_pcoaANDnmds",
    "href": "book_sections/ordination/pcalternatives.html#pcalternatives_pcoaANDnmds",
    "title": "• 8. PCAlternatives",
    "section": "Distance-based methods for nonlinear numeric data: PCoA and NMDS",
    "text": "Distance-based methods for nonlinear numeric data: PCoA and NMDS\nIn many cases, data have complex distributions that might do weird things to PCA. For example, Brooke has collected data not just on the number of pollinator visits, but on how many of each species visited each plant. Similarly, studies of the microbiome or environmental microbiology, often describing the relative frequency of different microbes in various environments.\n\n\n\nEuclidean distance: The straight-line distance between two points. Great when your variables are continuous and on the same scale (e.g., plant height and leaf area). Results of a PCoA based on euclidean distance are often nearly identical to a PCA.\nHamming distance: Counts how many features differ. Often used for binary strings (e.g., how many loci differ in genotype).\nBray–Curtis dissimilarity: Compares counts, like how many visits each plant got from each pollinator or how many read counts of each so-called “observed taxonomic unit” (OTU). Bray–Curtis dissimilarity is sensitive to both presence and abundance.\nJaccard distance: Looks at shared vs. unique elements. Good for presence–absence data (e.g., which microbes are present in each soil sample).\n\nOne approach to deal with such cases is to develop a “distance matrix” – an \\(N \\times N\\) table that shows how different each pair of samples is from one another. There is no universal way to define a distance – the right distance depends on your data (see options on our right). We then summarize this \\(N \\times N\\) distance matrix into a lower-dimensional summary of the data.Two common approaches – PCoA and NMDS do this in slightly different ways.\n\nPrincipal Coordinate Analysis (PCoA)\nPrincipal Coordinate Analysis (PCoA) runs an analysis similar to PCA on any distance matrix you give it. Specifically it takes the distance matrix and “double-centers” the data (A math trick that allows us to treat a distance matrix like traits for a PCA) and then runs an eigenanalysis.\n\nRunning a PCoA: The most common way to run a PCoA in R is to use the vegdist() function in the vegan package to make a distance matrix, and then provide this distance matrix to the cmdscale() function. Follow this link from Bakker (2024) for a bit more information.\nInterpreting PCoA results: The output of a PCoA is nearly identical to PCA, but the axes are principal coordinates, and we do not consider trait “loading” because we do not consider traits.\n\n\n\nNon-metric Multidimensional Scaling (NMDS)\n\n\n\n\n\n\n\n\n\nFigure 3: NMDS plot of microbial community composition from environmental biofilm samples. Points represent biofilm samples collected across three sites (D1, D3, D6) and treatment types (fluid, inert control, mineral), plus an ambient control. The plot is based on Bray–Curtis dissimilarity of OTU (operational taxonomic unit) counts from environmental sequencing. Groupings indicate that microbial community composition differs across both location and treatment. Adapted from a blogpost by Caitlan Casar.\n\n\n\n\nWhen data are so messy that any actual distance is difficult to interpret, we turn to NMDS. Like PCoA NMDS starts with a distance matrix, but then instead of using eigenanlysis, NMDS uses a complex algorithm to preserve the rank similarity of pairs of samples in a pre-specified (usually two or three) number of dimensions. The NMDS approach is useful because it can handle messy data, but using this approach means we lose the concept of “variance explained”, and interpreting results as distances between samples.\n\nRunning a NMDS: Like PCoA, we begin an NMDS analysis by using the vegdist() function to make a distance matrix. We then use vegan’s and then provide this distance matrix to the metaMDS() function. Follow this link from Bakker (2024) for a bit more information.\nInterpreting NMDS results: Rather than reporting the percent variance explained, NMDS calculates stress – a summary of how well the NMDS analysis summarizes the data. Stress values less than 0.1 mean that NMDS is a reasonable summary, while values greater than 0.2 mean that NMDS does not do a good job of summarizing our data. We can also visualize data points in the MDS1 and MDS2 space (as in Figure 3) to understand the similarity between samples.",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/pcalternatives.html#pcalternatives_tsneANDumap",
    "href": "book_sections/ordination/pcalternatives.html#pcalternatives_tsneANDumap",
    "title": "• 8. PCAlternatives",
    "section": "Capturing fine structure in high-dimensional space with t-SNE and UMAP",
    "text": "Capturing fine structure in high-dimensional space with t-SNE and UMAP\nAs datasets get larger, more heterogeneous, and more complex, there’s a growing need for fast ways to reveal structure in high-dimensional data. For example, single-cell RNA sequencing actually measures gene expression in thousands of individual cells—one cell at a time—while keeping data from each cell separate. Making sense of this high-dimensional data is challenging, and so we need techniques that allow for effective visualization. Two popular tools—t-SNE and UMAP—have become widely used for this purpose, and are now applied in many fields beyond single-cell analysis. These approaches can identify clusters in the data which might correspond to different cell types or states.\n\nt-SNE is a technique for visualizing high-dimensional data by preserving local similarities - it tries to make sure that points that are close in the original space stay close in the plot. This means t-SNE is great at finding clusters, but the distances between clusters are mieaningless. That is, while points within a cluster are typically similar, two clusters being far apart (or close together) doesn’t necessarily mean anything. This nice explanation of how to carefully conduct and evaluate a t-SNE analysis is worth checking out if you want to learn more. But t-SNE is rarely used now that we have UMAP.\nUMAP has largely taken over from t-SNE in many areas because it’s much faster and usually does a better job of preserving both local and some global structure. Like t-SNE, UMAP helps us find and visualize clusters. But unlike t-SNE, clusters that are closer together in UMAP space are often more similar, though the distances between them still don’t have a strict, interpretable meaning (as they would in PCA or PCoA). Read this fantastic UMAP tutorial, if you plan on running a UMAP, and look into this (slightly) more technical explanation by the people who invented this approach (Healy & McInnes (2024)).\n\nRunning UMAP in R: The most commonly used R package for UMAP is uwot, which is also used behind the scenes by the popular single-cell analysis package Seurat.\n\nThe umap2() function in uwot is preferred—it includes better defaults than the older umap() function.\nParameter choices matter, so it’s worth experimenting with options like n_neighbors (which affects how much local structure is preserved) and min_dist (which controls how tightly points are packed together in the plot).\n\n\n\n\n\n\n\n\nAn example of single-cell RNA-seq data from peripheral blood mononuclear cells visualized by PCA, t-SNE, and UMAP are shown below, taken from a blogpost by Matthew N. Bernstein.\n\n\n\n\n\nThe beautiful plots made by UMAP do not always truly highlight important biology. Read this warning in Chari (2023) before over-interpreting your results.\nMore broadly, with complex and non-transparent methods like t-SNE and UMAP it’s always worth looking for additional lines of evidence to ensure that we are not being misled.\n\nHere’s a short, clean “Common Misinterpretations” box you can drop into your PCA Alternatives section. It’s in your tight, student-facing, biologically grounded style, and meant to fit naturally after method descriptions or before any review questions.\n\nJust because you’ve reduced your multidimensional data into two dimensions doesn’t mean everything in the plot has a clear biological meaning. A few common pitfalls:\n\nOverinterpreting t-SNE and UMAP plots These methods are built for visualization, not interpretation. The axes have no meaning. The global structure (e.g., distances between faraway clusters) often doesn’t reflect biological distance at all — only the local neighborhood structure is trustworthy.\nReading direction into NMDS axes NMDS axes can flip, rotate, or stretch between runs. The relative arrangement of samples matters, not the absolute axis values or orientations. Always look at patterns, not coordinates.\nAssuming PCA alternatives give you “components” like PCA Only some methods (e.g., FAMD, MCA) return interpretable “axes” with loadings like PCA. For others, like NMDS and t-SNE, you don’t get a clear mapping from traits to axes.\nInterpreting distance as Euclidean when it’s not If your method used Jaccard or Bray–Curtis distances, don’t interpret sample spacing as if it came from straight-line distances in trait space.\n\nIn short: don’t read more into the plots than the method can give you. Use them to explore, not to conclude.\n\n\n\n\n\nBakker, J. D. (2024). Applied multivariate statistics in R. Pressbooks.\n\n\nChari, L., Tara AND Pachter. (2023). The specious art of single-cell genomics. PLOS Computational Biology, 19(8), 1–20. https://doi.org/10.1371/journal.pcbi.1011288\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1), 82. https://doi.org/10.1038/s43586-024-00363-x",
    "crumbs": [
      "8. Ordination",
      "• 8. PCAlternatives"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html",
    "href": "book_sections/ordination/ordination_summary.html",
    "title": "• 8. Ordination summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nModern biological datasets often have more traits than we can look at or make sense of directly. Ordination approaches help us see structure in these large and complex datasets. These techniques reduce dimensionality: they summarize patterns across many variables into just a few new ones that soak up as much variation in the data as possible. When data are numeric and well-behaved, we typically use PCA. But when variables are categorical, mixed, or messy (as is common in ecological or genomic data), PCA can mislead. In these cases, alternatives like MCA, FAMD, PCoA, NMDS, t-SNE, and UMAP (among others) may be more appropriate. Some work with distances, others with probabilities. Some produce axes you can interpret; others are just for visualizing structure. Whatever method you use, make sure you understand its assumptions, limitations, and how to interpret its output - such approaches are no better than the scientist using them.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summarySummary",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summarySummary",
    "title": "• 8. Ordination summary",
    "section": "",
    "text": "Chatbot tutor\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryPractice_questions",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryPractice_questions",
    "title": "• 8. Ordination summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions! By using the R environment you can work without leaving this “book”. To help you jump right into thinking and analysis, I have loaded the necessary data, cleaned it some, and have started some of the code!\n\nWarm up\nQ1) For which of these cases is a PCA most appropriate?\n\n You want to measure how strongly two continuous variables are correlated. You want to reduce a dataset with many continuous variables to a few variables that capture most of the variation. You have messy data with missing values and a mix of continuous and categorical variables. You’re hoping to discover natural clusters in your data.\n\n\n\nClick here for explanation\n\n\nIf you chose option 1, you’re not totally off base — PCA does involve correlations between variables. But if all you want is to measure the strength or direction of association between two continuous variables, a correlation coefficient (like Pearson’s r) is simpler, more interpretable, and more appropriate.\nIf you chose option 2, congrats 🥳, This is the core purpose of PCA — reducing dimensionality by summarizing structure in multivariate continuous data.\nIf you chose option 3, you’re describing a tricky (but common) dataset. Unfortunately, PCA isn’t great at handling messy or mixed data. It requires complete, numeric data — missing values break it, and categorical variables need special handling.\nIf you chose option 4, that’s a common motivation and frequent misinterpretation. Sure, PCA might reveal clusters in the data, but that’s not what PCA is for. If clustering is your goal, use clustering methods (e.g. k-means, hierarchical clustering, which we will get to later – i hope) instead.\n\n\n\n\nExample Plot 1\nConsider the plot below - a PCA of the iris data set, as you answer questiosn two through six.\n\n\n\n\n\n\n\n\n\n\nQ2) Which of the plots above is most appropriate?\n\n A: It stretches PC2, making subtle vertical differences easier to see. B: It keeps the axes roughly square, which feels visually balanced and avoids exaggeration. C: It sets the axes proportional to the percent variance explained, making distances and angles in the plot geometrically meaningful. There is no 'best' because plot choice depends on audience and goals.\n\nQ3) Based on the plot(s) above which species appears to have the largest petals (i.e., petal length and width)?\n\n setosa versicolor virginica 🤷 PCA doesn’t tell us anything about trait values.\n\nQ4) Based on the plot(s) above, how would you interpret PC2?\n\n PC2 reflects petal area: higher values mean larger petals PC2 reflects petal area: higher values mean smaller petals PC2 reflects sepal area: higher values mean larger sepals PC2 reflects sepal area: higher values mean smaller sepals There is no clear biological interpretation of PC2 from this plot We should never attempt to interpret PCs biologically PC2 should be ignored because it explains so little of the variance.\n\nQ5 The correlation between petal width and petal area is 0.96. It is not unclear what to do in such cases, but one of these options iis not justifiable. Which one?\n\n Don't change anything. These are different traits! Drop one of the petal traits (it doesn't really matter which one). If you drop one petal trait, be sure to drop a sepal trait Drop both petal length and width, and replace them with petal area.\n\n.\nBONUS: What would you do?\n\n\n\n\nQ6 Write a brief paragraph summarizing this plot.\n\n\n\n\n\n\nExample Plot 2\nI made a PC from wild Clarkia (not RILs) collected at the Sawmill road hybrid zone. Note the data are centered and scaled. Use this webR workspace to answer question seven (below).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ7) The plot above appears to show three clusters, but there is a problem (or two). What is th biggest problem?\n\n PCA is not meant for finding clusters PC2 only explains 19% of the variance so we should not pay much attention to it The middle cluster is largely made up of missing data Some variables are categorical so we should have run and FAMD analysis Something went into our PC that shouldn't have\n\n\n\nHint\n\nLook into trait loading, either by typing tidy(saw_pca, matrix = \"loadings\") or by adding loadings = TRUE,  loadings.label = TRUE to autoplot().\n\n\n\nExplanation\n\nYou can see that PC2 is basically id – a variable it is not anything we ever should not consider as biologically interesting.\n\n\n\n\nExample Plot 3\nUse the data below to answer questions eight and nine.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ8) What went wrong in this plot (Note, choose the worst mistake for this dataset)\n\n Nothing - it's great! PC1 splits the data into two groups! Axes are not proportional to PVE. Something went wrong. with PC1 - I should figure it out. The data are not centered and scaled.\n\nQ9) Fix all the mistakes above (not just the biggest one) and remake this plot. What percent of variability in the data is attributable to PC2 \n\n\nHint 1\n\nLook into trait loading, either by typing piping PCA output to tidy( matrix = \"loadings\") or by adding loadings = TRUE,  loadings.label = TRUE to autoplot().\n\n\n\nHint 2\n\nLook into at the variable associated with PC1. Remove the most filter() for all data except the mostextreme value.\n\n\n\nHint 3\n\nMake a pca, be sure to center and scale your axes!\n\n\n\n\nPCAlternatives\n\nQ10) In which of the following ordination methods is it most appropriate to interpret the distance between points in the plot as representing similarity between samples?\n\n NMDS, because it’s designed for messy ecological data. t-SNE, because it preserves local structure. PCA and PCoA — but only if axes are scaled proportional to variance explained. All of them — that’s what ordination plots are for!\n\n\n\nExplanation\n\nPCA (Principal Component Analysis) and PCoA (Principal Coordinate Analysis) use Euclidean geometry, so distances between points are interpretable (although PCoA distanses are in units of distance given by the distance matrix) — if you scale the axes to reflect the variance explained. If you don’t, you can distort distance and angle relationships.\nNMDS doesn’t preserve absolute distances — only the rank order of pairwise distances (who’s closest to whom). The same goes for t-SNE and UMAP, which emphasize local neighborhood structure but distort global distances. So while clusters might look compelling, distances between them often lie.\n\nYou’ve got a huge dataset from museum specimens: 4 continuous variables (e.g., bill length, wing chord, body mass, tarsus length)\n2 ordered categorical variables (e.g., molt stage from 1–5, plumage brightness from dull to vibrant)\nSeveral missing values in most rows\nYou want to visualize broad patterns and see if anything jumps out. Which approach is the least bad?\n\n PCA - just ignore missing values and let prcomp() do its thing. FAMD - if you impute missing data first, it can handle mixed types and give interpretable dimensions. UMAP - it will figure out the structure automatically. NMDS - it ignores variable types and missing values, so it must be fine.\n\n\n\nExplanation\n\nNone of these is perfect, but FAMD is probably your best bet if you carefully impute missing values (e.g., using missMDA::imputeFAMD()). PCA drops all incomplete rows; NMDS can’t handle mixed variable types or NA; UMAP will happily embed noise and call it structure.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryGlossary_of_terms",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryGlossary_of_terms",
    "title": "• 8. Ordination summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n📚 1. Concepts of Ordination\n\nOrdination A general term for methods that reduce complex, high-dimensional data into fewer dimensions to help us find patterns or structure. Useful for summarizing variation, visualizing groups, or detecting gradients in data.\nDimensionality Reduction The process of collapsing many variables into a smaller number of “components” or “axes” that still explain most of the relevant variation.\nDistance Matrix A table showing how different each pair of observations is, based on some measure (e.g., Euclidean, Bray-Curtis). The starting point for many non-linear ordination methods.\nScores The new coordinates of each sample on the reduced axes (like PC1, PC2, or NMDS1, NMDS2). These are what we usually plot.\nLoadings The contribution of each original variable to a principal component. They tell us what the new axes “mean” in terms of the original traits.\nProportion of Variance Explained (PVE) How much of the total variation in the data is captured by each axis (e.g., PC1, PC2). Higher values suggest the axis captures an important pattern.\n\n\n\n🔢 2. PCA and Matrix-Based Methods.\n\nPrincipal Component Analysis (PCA) A technique that finds new axes (principal components) that capture as much variation as possible in your numeric data. Based on eigenanalysis of the covariance or correlation matrix.\nEigenvalues Numbers that describe how much variance is associated with each principal component.\nEigenvectors The directions (in trait space) along which the data vary the most — the basis of your new axes (PC1, PC2…).\nCentering and Scaling Centering subtracts the mean of each variable; scaling divides by the standard deviation. This standardizes variables to make them comparable before running PCA.\nSingular Value Decomposition A matrix factorization technique that expresses a matrix as the product of three matrices: U, D, and Vᵗ. PCA is often computed using SVD of the centered (and sometimes scaled) data matrix.\n\n\n\n🎲 3. Categorical and Mixed Data\n\nMultiple Correspondence Analysis (MCA) A PCA-like method for datasets made up of categorical variables (e.g., presence/absence, categories). Often used for survey or genetic marker data.\nFactor Analysis of Mixed Data (FAMD) Combines ideas from PCA and MCA to handle datasets with both continuous and categorical variables.\nMultiple Factor Analysis (MFA) Used when your data come in blocks (e.g., gene expression + morphology + climate). It balances across blocks to find shared structure.\n\n\n\n🧭 4. Distance-Based Ordination.\nPrincipal Coordinates Analysis (PCoA) finds axes that best preserve distances between samples, based on any distance matrix (e.g., Bray-Curtis, UniFrac, Jaccard). Results resemble PCA but from a different starting point.\nNon-metric Multidimensional Scaling (NMDS) A distance-based method that tries to preserve the rank order of distances. Great for ecology or microbiome data. Axes have no fixed meaning — they just help you visualize structure.\n\n\n🌐 5. Nonlinear Embedding Methods.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding) A nonlinear method that preserves local structure (similarity between nearby points) but often distorts global structure. Good for finding clusters but not for understanding axes.\n**UMAP (Uniform Manifold Approximation and Projection)* A newer nonlinear method like t-SNE, but faster and often better at preserving both local and global structure. Often used for visualizing high-dimensional data like RNA-seq or image features.",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryNew_r_functions",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryNew_r_functions",
    "title": "• 8. Ordination summary",
    "section": "🛠️ Key R Functions",
    "text": "🛠️ Key R Functions\n\n\n🔢 Principal Component Analysis (PCA)\n\nprcomp(): Performs PCA for you. Rememmber to always type center = TRUE, and to usually type scale = TRUE, (unless you think scaling is inappropriate.\neigen(): Find eigenvectors and eigenvalues of a matrix. Pair this with cov() or cor() if you want to run a PCA without prcomp().\n\naugment() In the broom package: Adds PCA scores to the original data for plotting or further analysis.\ntidy() In the broom package: Makes a tidy tibble for key outputs the prcomp output (The specific outpput depends on the matrix = option).\n\ntidy(, matrix = \"pcs\"): Makes a tibble showing the variance attributable to each PC.\n\ntidy(, matrix = \"loadings\"): Makes a tibble describing the loading of each trait onto each PC.\n\ntidy(, matrix = \"scores\"): Makes a tibble showing the value of each sample on each PC. This is simple a long format version of the output of augment().\n\nautoplot(): In the ggfortify package. Plots PCA results with groupings, or loadings arrows.\ntheme(aspect.ratio = PVE_PC2/PVE_PC1): This is a specific use of ggplot2’s theme() function, which we will discuss more later. Be sure to add this to all of your PC (or PC-like) plots to help make sure readers do not misinterpret your results. Find PVE_PC1 and PVE_PC2 from your eigenvalues in you prcomp() output (or more easily tidy(, matrix = \"pcs\"):).\n\n\n\n\n🔎 Categorical & Mixed Data Ordination\n\nMCA(): In the FactoMineR package performs Multiple Correspondence Analysis for categorical variables.\nFAMD(): In the FactoMineR package performs a Factor Analysis of Mixed Data (categorical + continuous variables).\nfviz_... In the factoextra package are helper functions to make plots from MCA and FAMD output.\n\n\n\n\n🧭 Distance-based methods.\n\nvegdist(): In the vegan package makes a distance matrix for many common distance measures.\ncmdscale() Allows for classical multidimensional scaling on a distance matrix (as in PCoA).\nmetaMDS() In the vegan package performs NMDS on a distance matrix.\n\n\n\n\n🌐 UMAP & Visualization\n\numap2() In the uwot packageerforms Uniform Manifold Approximation and Projection (UMAP).",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/ordination/ordination_summary.html#ordination_summaryR_packages_introduced",
    "href": "book_sections/ordination/ordination_summary.html#ordination_summaryR_packages_introduced",
    "title": "• 8. Ordination summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\n\nbroom: Tidies output from PCA (tidy(), augment()) into easy-to-use data frames.\nggfortify: Simplifies plotting of PCA and other multivariate objects using autoplot().\nFactoMineR: Provides functions for multivariate analyses like MCA, FAMD, and MFA.\nfactoextra: Makes it easy to visualize outputs from FactoMineR analyses (fviz_mca_ind(), fviz_famd_var(), etc.).\nvegan: Used to compute distance matrices (vegdist()), run PCoA (cmdscale()) and NMDS (metaMDS()).\nuwot: Performs UMAP for dimensionality reduction of high-dimensional data.\n\n\n\nAdditional resources\n\nWeb resources:\n\nIntroduction to ordination – By our coding club.\nPrincipal component analysis – Lever et al. (2017).\n\nBe careful with your principal components – Björklund (2019).\n\nA gentle introduction to principal component analysis using tea-pots, dinosaurs, and pizza – Saccenti (2024).\n\nUniform manifold approximation and projection – Healy & McInnes (2024).\nSeeing data as t-SNE and UMAP do – Marx (2024).\n\nUnderstanding UMAP.\nHow to Use t-SNE Effectively by Wattenberg et al. (2016).\n\nVideos:\n\nStatQuest: Principal Component Analysis (PCA), Step-by-Step from StatQuest.\nHow to create a biplot using vegan and ggplot2 (From Riffomonas project).\nUsing the vegan R package to generate ecological distances (From Riffomonas project).\nRunning non-metric multidimensional scaling (NMDS) in R with vegan and ggplot2 (From Riffomonas project).\nPerforming principal coordinate analysis (PCoA) in R and visualizing with ggplot2 (From Riffomonas project).\n\n\n\n\n\n\nBjörklund, M. (2019). Be careful with your principal components. Evolution, 73(10), 2151–2158. https://doi.org/https://doi.org/10.1111/evo.13835\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and projection. Nature Reviews Methods Primers, 4(1), 82. https://doi.org/10.1038/s43586-024-00363-x\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component analysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nMarx, V. (2024). Seeing data as t-SNE and UMAP do. Nature Methods, 21(6), 930–933. https://doi.org/10.1038/s41592-024-02301-x\n\n\nSaccenti, E. (2024). A gentle introduction to principal component analysis using tea-pots, dinosaurs, and pizza. Teaching Statistics, 46(1), 38–52. https://doi.org/https://doi.org/10.1111/test.12363\n\n\nWattenberg, M., Viégas, F., & Johnson, I. (2016). How to use t-SNE effectively. Distill. https://doi.org/10.23915/distill.00002",
    "crumbs": [
      "8. Ordination",
      "• 8. Ordination summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html",
    "href": "book_sections/data_viz.html",
    "title": "9. Better Figures",
    "section": "",
    "text": "Why Make a Plot?\nTo make these abstract ideas concrete, lets return to our parviflora RILs. A quick glance suggests that the probability of setting hybrid seed goes up as leaf water content goes down. While this observation is true, it is not the real story – the real story is that parviflora RILs with smaller petals make fewer hybrids. The initial observation is a red herring that arises because of the negative assocaition between leaf water content and petal area.\nIt’s nearly impossible to look at raw numbers in a dataset and come away with a holistic understanding. Communicating results by listing numbers is inefficient and overwhelming. While summaries of single variables, associations between variables can efficiently convey certain aspects of your data, they often hide important details. On their own, summary statistics can mislead, overlook critical patterns, and fail to provide readers with an intuitive way to evaluate your claims.\nA good plot is more than a condensed and efficient presentation of the data. Rather, making a plot is literally our opportunity to shape how the reader sees the data, and is therefore a critical medium by which we tell our scientific story. Because a plot is a crafted story with a purpose, we must think about this as we build our plots. Ask yourself:\nGraphs exist to communicate clear points. Together, a set of plots should form a cohesive narrative. When creating an explanatory plot, ask yourself:\nAs we work through this checklist, consider that at the extremes there are two types of plots.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#why-make-a-plot",
    "href": "book_sections/data_viz.html#why-make-a-plot",
    "title": "9. Better Figures",
    "section": "",
    "text": "What’s the key story that the data are telling?\n\nWhat point am I trying to make?\nHow does this plot support that story?\n\nCan someone skeptical follow and verify it?\n\nHow can a plot be improved to clearly communicate this message?\n\nHow does this point fit into the larger story I want to tell?\n\n\n\nExploratory plots are plots we make to make sense of the story in the data.\nExplanatory plots tell this story to a broader audience.\n\n\nWhy Make Exploratory Plots?\n\nThe first principle is that you must not fool yourself – and you are the easiest person to fool.\n\n— Attributed to physicist Richard Feynman\n\n\nBefore telling a story, we must know the story we aim to tell. This includes understanding the overall message we aim to convey, recognizing the extent to which the data support this message, and not missing key elements of the data. As an example of the importance of looking at our data, lets look into datasaurus a more elaborate version of Anscombe’s quartet.\n\nlibrary(datasauRus); library(ggplot2); library(dplyr)\nsummary_stats &lt;- datasaurus_dozen                      |&gt; \n  group_by(dataset)                                    |&gt; \n  summarize(mean_x =  mean(x),   mean_y = mean(y), \n            stdev_x = sd(x),     stdev_y = sd(y), \n            cor_xy = cor(x, y)) \n\n\n\n\n\n\n\n\nThe plots below highlight the importance of looking at your data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHist of xHist of ySlopesScatterplot\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAfter exploring these data, answer the following questions:\n\nWhich dataset has no y values between 45 and 55? \nWhich shape is made by one of the datasets in datasaurus? A turtleA starA heart\n\nAs see looking at histograms of x and y reveals some differences between the datasets, but examining a scatterplot is truly revealing!\n\n\nWhy make explanatory plots?\nAn explanatory plot effectively communicates results while giving skeptical readers the chance to evaluate our claims. Plots are such a critical tool in scientific communication that, in many lab meetings, papers are often discussed by focusing on the figures. Crafting a good explanatory plot is much like craft a good story. As you watch the video below, consider how the components of telling a good story can be mapped onto the idea of making a good plot.\n\n\n\n\nWatch this video on what makes a good story, and consider how this applies to storytelling with data visualization.\n\n\n\nExample of Telling a Story With Plots:\nIn basketball, most shots are worth two points, while distant shots beyond the three-point line are worth three points. Around 2008, the NBA began embracing analytics, and analysts discovered that three-point shots provide more value than most two-point shots. As a result, teams shifted their strategy to prioritize three-pointers or high-percentage two-point shots close to the basket (podcast for the curious).\n1 A compares shot selection before and after the rise of analytics in the NBA. It demonstrates that before this shift, teams had no obvious trends in shot selection, while afterward, most teams focused on three-pointers and close-range shots. 1 B shows the dramatic rise in three-point attempts from 2006 to the present, providing historical context. Together, they tell the story of the NBA’s analytics revolution.\n\n\n\n\n\n\n\n\nFigure 1: Figure A is modified from images on Instagram @llewellyn_jean. Figure B is modified from an article on espn.com.\n\n\n\n\n\n\nis not perfect. For example, the team names are too small to read. But fixing this would be unnecessary—the team names don’t significantly contribute to the story we’re telling.\n\nAfter you create your plot, take a moment to reflect. How well does your figure make its intended point? How could it detract from your message? Then brainstorm ways to improve your plot to more clearly and honestly convey your point.\n\n\nThe Process\nComputational tools like ggplot2 are great for making good plots, but remember they are tools to help you, not constrain you. Many experts (and the internet) suggest that before jumping into ggplot, you should first:\n\n\n\nMy approach to figure-making in #ggplot ALWAYS begins with sketching out what I want the final product to look like. It feels a bit analog but helps me determine which #geom or #theme I need, what arrangement will look best, & what illustrations/images will spice it up. #rstats pic.twitter.com/GUjeEgqZxj— Shasta E. Webb, PhD (@webbshasta) May 22, 2020\n\n\n\nSketch your desired plot to conceptualize it.\nBe cautious of defaults and common plots, as they might not always serve your needs.\n\nCreating a good plot is an iterative process. You will likely go back and forth between pencil-and-paper sketches and ggplot until you reach a design you’re happy with.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#lets-jump-into-key-concepts-in-data-viz",
    "href": "book_sections/data_viz.html#lets-jump-into-key-concepts-in-data-viz",
    "title": "9. Better Figures",
    "section": "Let’s jump into key concepts in data viz!",
    "text": "Let’s jump into key concepts in data viz!\nWhile bad plots can be bad in various ways, all good plots share certain characteristics. Specifically, all good plots are: Honest, Transparent, Clear, Accessible, and avoid distractions. This chapter provide specific practices you can use to make such good plots, and then introduces the idea of understanding your audience and mode of presentation, and how to write about plots. As always, I conclude with a chapter summary.\nBut before moving along, let’s take a break from Clarkia and focus on the distribution of student-to-teacher ratios across continents. We will use Figure 2 to exemplify how we can make good plots.\n\n\n\n\n\n\n\n\nFigure 2: It is hard to make a plot this bad.\n\n\n\n\n\nNote that improving a figure is an iterative process, so we slowly get better, and may take some wrong turns along the way. Figure 3 shows one path throght this process.\n\n\n\n\n\n\n\n\nFigure 3: Making a plot is an iterative process. gif taken from Cedric Scherer’s blogpost on The evolution of a ggplot.",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz.html#looking-ahead",
    "href": "book_sections/data_viz.html#looking-ahead",
    "title": "9. Better Figures",
    "section": "Looking ahead",
    "text": "Looking ahead\nThere are two distinct challenges to making a good plot goal:\n\nUnderstanding the ideas behind making a good plot (this chapter).\n\nThe technical process of creating the plot in R (next chapter).\n\nThese are separate tasks. If you focus too early on the mechanics of R, you risk creating poorly designed visuals. Once you have a clear vision of the plot, implementing it in R becomes more straightforward (especially with the help of generative AI tools like ChatGPT, Claude, Gemini etc..)\n\n\n\n\nGould, P. (1981). Letting the data speak for themselves. Annals of the Association of American Geographers, 71(2), 166–176. https://doi.org/https://doi.org/10.1111/j.1467-8306.1981.tb01346.x",
    "crumbs": [
      "9. Better Figures"
    ]
  },
  {
    "objectID": "book_sections/data_viz/audience&delivery.html",
    "href": "book_sections/data_viz/audience&delivery.html",
    "title": "[• 9. Audience & Format ]{#audience&delivery .quarto-section-identifier}",
    "section": "",
    "text": "The Audience and the Format\nLabeling something as a “good plot” or a “bad plot” is overly simplistic. Sure, there are features that make a plot “better” or “worse”, but plots do not have an independent existence. Plots are presented to specific audiences in specific contexts. Considering your audience and the presentation format is key to making a truly effective visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Audience & Format"
    ]
  },
  {
    "objectID": "book_sections/data_viz/audience&delivery.html#the-audience-and-the-format",
    "href": "book_sections/data_viz/audience&delivery.html#the-audience-and-the-format",
    "title": "[• 9. Audience & Format ]{#audience&delivery .quarto-section-identifier}",
    "section": "",
    "text": "The Audience\n\n\n\n\n\n\n\n\n\nFigure 1: An audience at a sceintific meeting (The 20204 EU Drones Conference to be precise). Photo posted to wikimedia commons and shared under Creative Commons Attribution 2.0 Generic License by the Belgian Presidency of the Council of the EU 2024.\n\n\n\n\nWe tell stories to an audience, not a wall. Just as you would communicate differently with a friend versus a colleague, your plot should be tailored to its intended audience. When designing a figure, ask yourself: (1) Who is this for? (2) What background knowledge or context do they bring (3) What do they need from this? (4) What do you want from them – Feedback? Funding? Action? Respect etc…?\n“Archetypes” of a potential audience for your plot include:\n\n🫵 You and Your Team: You are the first person to look at your plot. You want to know that you are not fooling yourself. You are deeply familiar with the data, methods and many peculiarities of the study.\n\nFor this audience: You may want to color points by technical covariates (e.g. when the reading was taken, who took it? which machine was used? etc) and use tools like plotly to understand outliers so that you don’t waste weeks digging into the wrong story.\n\n🧠 The Expert Critical Reader: This is the scientific audience for academic articles. Often a peer, a reviewer, or even a competitor. The expert wants precision, detail, honesty and clarity. The expert wants to quickly know what you did and what it means, but may be quite critical of your results so would like to be able to evaluate the evidence for themselves.\n\nFor this audience: You will likely want to guide their interpretation but while showing all the data and empowering them to verify your conclusions.\n\n🤷 The Curious Novice: Not everyone encountering your work will be an expert. You might communicate your work to students just beginning to master the subject, or biologists from a related subdiscipline. This audience too, would like to be critical, but often needs additional context, background, and structure to make the most of a plot.\n\nFor this audience: Your main job here is as a teacher. You should explain your plots very clearly, and their points should be obvious.\n\n📣 The Public or Non-Expert: You might present your work to a broad audience (e.g. TEDx style), to a lawyer, a jury, a politician etc. These people are likely interested in your work (they’re here aren’t they?), and do want to be able to evaluate its legitimacy, but may not have statistical or biological expertise.\n\nFor this audience: Your main job here is as a convincer. As the expert, you aim to help this audience see things your way. Make your plots clear and easily digestible. You should largely refer to published studies for legitimacy, but be prepared to handle questions from engaged participants.\n\n\n\n\nTailoring Presentations to Their Format\n\n\n\n\n\n\n\n\n\n\nFigure 2: A snippet from Van Halen’s 1982 tour rider detailing the ‘Munchies’ requirements, accessed from snopes.com.\n\n\n\n\n\n\nIn one notable incident, officials at what is now Colorado State University Pueblo refused to honor the request, leading the band to go on a rampage that involved throwing food all over a dining area as well as “unmentionable” acts in a nearby restroom. However, even more damage was caused to the basketball floor in the gymnasium due to the weight of the stage brought in.\n\n— Wikipedia\n\n\nScience is communicated in many formats. Here’s how to tailor your plots to different mediums:\n\n\n📘 Books / Papers: These plots are static and need to be self-contained. Readers should be able to understand the plot and its main point without relying on the accompanying text. You don’t know how people will read your paper.\n\n\nPlots as “Brown M&Ms”\nThe American hard rock band Van Halen had a strange issue in their “rider” - they insisted on having a bowl of M&Ms but no brown M&Ms Figure 2. While they did have a very rock bad diva response to this being broke (see the quote o the right), this request was more than a rock-diva thing. As explained by vocalist David Lee Roth this request was a way to ensure that the venue had closely read the requirements for the show to ensure safety (as aluded to at the end of the Wikipedia quote).\nIn the same way, scientists use visual cues to make rapid judgments about the quality and trustworthiness of research. A sloppy, unpolished plot acts like a brown M&M in the candy bowl; it’s a red flag that signals a potential lack of care that could extend to the data analysis itself (evenif the analysis is fine). A polished, professional plot does the opposite: it signals that the author is careful and trustworthy. It tells the reader they are in good hands.\n\n\n\n🎤 Public Talks: In presentations, you control the flow of information. A successful plot in a scientific talk:\n\nBuilds up figures slide by slide, to encourage audience understanding and engagement.\nUses large text, big points, and wide lines so that it is legible from the back of the room.\nFavors results and interpretation over details.\n\n\n\n\n\n\n\n\n🖼️ Posters:. Because your poster will be presented alongside tens or hundreds of others in a big room (Figure 3 A), your poster and its figures should be designed to draw viewers in and encourage them to engage. Posters are arguably the most difficult format in science communication, so here are some tips:\n\nMake it stand out: You’re often in a room with tens to hundreds of posters, and people are circulating casually, often with a coffee or drink in hand. Catch their eye. There is a time and place for charjunk and that time and place is a poster session.\n\nUse as little text as possible: Once you’ve removed unnecessary text, go back and remove even more.\nMake all graphical elements big: Text, points, labels—all of it.\n\n\n\n\n\n\n\n\n\n\nFigure 3: (A) A typical scientific poster session—crowded, fast-moving, and visually competitive. (B) A redesigned scatterplot of Iris data intended for a poster, using large points, direct species labels, and photographs of the flowers to make the plot visually engaging and immediately interpretable.\n\n\n\n\n\n\n\n💻 Digital Formats: Online formats allow for interactive elements, GIFs, and animations. These can help readers explore your data themselves, making the story more intuitive and engaging, but don’t overdo it. You should focus on helping readers engage with the plot to absorb the key results, not distract them from those results. As you can tell from my efforts in this book, this is my favorite context.\n\n\n\n\n\n\n\n\n\nFigure 4: Interactive iris plot with trendlines and direct labels. This interactive scatterplot shows Sepal Length versus Petal Length for three Iris species. Each species is plotted in a distinct color and shape, with accompanying trendlines and direct labels. Designed for digital formats, this figure allows for hover-based exploration, emphasizing user engagement and interpretability.\n\n\n\n\n\nSummary of the impact of medium on plot development.\n\n\n\n\n\n\n\n\nFormat\nKey Feature\nPrioritize\n\n\n\n\nPaper\nStatic, standalone\nClarity, context in figure\n\n\nTalk\nControlled pacing\nSimplicity, build-up, big elements\n\n\nPoster\nCompetitive and noisy setting\nVisual impact, minimal text\n\n\nDigital\nDynamic & user-driven\nInteractivity, clarity\n\n\n\n\n\n\n\nWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%\n\n— Donald Knuth\n\n\n\n\nPremature optimization is the root of all evil\nAlthough Donald Knuth was talking about optimizing computer programs, the same logic applies to figures. In research or publication contexts, refining the plot can be worth the time - but only after the scientific story is sound. Perfecting a plot before it is ready to be presented can be a massive time sink. I therefore recommend the following workflow to make great plots without wasting too much time:\n\nMake quick, rough plots to start exploring the stories in your data.\nMake additional plots and summaries to see if you fooled yourself and/or if an artifact can better explain your data than your hypothesis.\nMake a good “base plot” that clearly and honestly shows the point and circulate this to friends collaborators etc.\nSave the code for this plot and don’t lose it.\n🛑STOP🛑 working on the plot for a while. Do everything else to make your science good. Resist the urge to keep tweaking at this stage - your time is better spent elsewhere for now.\nOnce you are ready to present the work take steps towards further refining the plot following the concepts outlined in the rest of this chapter. Now is the time to make this figure shine!!! Do most of this in R - but feel free to spruce a quick thing up some other way (e.g. powerpoint / illustrator etc…) if it’s fast one-off.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Audience & Format"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html",
    "href": "book_sections/data_viz/honest_plots.html",
    "title": "• 9. Honest plots",
    "section": "",
    "text": "Good Plots Are Honest\nPlots should clearly convey points without misleading or distorting the truth. A misleading exploratory plot can lead to confusion and wasted time, while a misleading explanatory plot can erode the reader’s trust. Honesty in plots builds credibility and helps ensure that both you and your audience stay on track.\nImportantly, honest people with good intentions can still create misleading plots - often due to default settings in R or other visualization tools. So, simply not intending to deceive isn’t enough. After we make a plot, we should take a step back and examine it - better yet, show it to naive peers to see what conclusions they draw from it. This practice can ensure that our plots are not unintentionally misleading.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html#good-plots-are-honest",
    "href": "book_sections/data_viz/honest_plots.html#good-plots-are-honest",
    "title": "• 9. Honest plots",
    "section": "",
    "text": "(Dis-)Honest Axes\n\n\n\n\n\n(Dis-)Honest Y-Axes\nDishonestY: truncating the axis. When people see a filled area in a plot, they naturally interpret it in terms of proportions. This can be a problem when the baseline of a filled bar is hidden or cropped, making modest differences look dramatic. Compare Figure 1, Figure 2 and Figure 3 in the three tabs below to see how a truncated y-axis can mislead a reader.\n\nFig. 1Fig. 2Fig. 3\n\n\n\n\n\n\n\n\n\n\nFigure 1: Misleading plot: Truncated y-axis with no visible axis labels. The differences in student–teacher ratios appear exaggerated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Still misleading: Adding axis labels to a truncated y-axis does not prevent our plot from misleading readers. The visual perception continues to overpower the text.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Honest plot: The y-axis starts at zero, providing an accurate visual impression of the true differences in student–teacher ratios.\n\n\n\n\n\n\n\n\n\n\nIf you don’t see any plots here, click on any of the tabs. Then browse through all of them by changing between tabs.\nFrom these plots we can see that:\n\nFigure 1 leads a reader to believe that student-to-teacher ratios in Africa are four times higher than in Asia, even though the actual difference is closer to two-fold.\nFigure 2 illustrates that adding y-labels does little to fix this, as our eyes are still driven to the magnitude of difference in bars, not the few letters that try to override this visual message.\nFigure 3 solves this by honestly displaying the full y-axis. \n\n\nNot all y-axes need to start at zero:\nTruncating the y-axis is most misleading for filled plots, but it’s not always necessary to start at zero.\n\nScatterplots: These don’t typically trick the eye the way bar plots do, so it’s less important to start the y-axis at zero. If you want to emphasize absolute differences, show the data as points and worry less about truncating the y-axis.\n\nNon-zero baselines: For variables like temperature, starting the y-axis at zero may be arbitrary.\n\n\n\nDishonestY: sneaky scales\nWe have previously discussed how common data transformations that can aid in modeling and visualizing our data. For example, log-transformation can be helpful for variables that grow exponentially. When such transformations are necessary be sure to clearly communicate the transformed scale. For example, if you use a log scale, but your readers don’t notice it, they will misinterpret the plot – a straight line on a log-log plot suggests a power-law relationship, while a straight line on a semi-log plot suggests exponential growth. A straight line on a log scale means exponential growth, not a steady increase.\nIf you’re displaying your data on a log scale, be loud about it: label the axes clearly. I particularly like the annotation_logticks() function to in the ggplot2 package to communicate the scale (as in Figure 4 B)\n\n\nCode for making a logscale two-panel figure and adding logticks.\nlibrary(patchwork)\n\na&lt;- ggplot(msleep, aes(bodywt, brainwt, label = name)) +\n    geom_point(na.rm = TRUE) +\n    scale_x_log10(\n        breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n        labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n    ) +\n    scale_y_log10(\n        breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n        labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n    )  \n\n\nb &lt;- a + \n  annotation_logticks() + \n  labs(title = \"Mammalian brain weight as a function of body weight\", \n       subtitle = \"Both axes are plotted on a log10 scale.\")\n\na &lt;- a + labs(title = \"Mammalian brain weight as a function of body weight\",\n              subtitle = \" \")\n\na+b\n\n\n\n\n\n\n\n\nFigure 4: Mammalian brain vs. body weight on a log scale. A the data on log-transformed axes with labeled axes. B adds log tick marks with annotation_logticks(), making the scale more visually explicit.\n\n\n\n\n\n\nDishonestY: broken axes\nSometimes extreme values on the y-axis make it hard to see meaningful variability for lower values. In some cases this situation is so extreme that the data cannot be plotted on a continuous without the bulk of the data being squished flat. One solution is a broken axis – a literal break to show the two different ranges of the data. I hesitate to recommend such an approach because it so often misleads the reader by distorting the relative distances. If you must use a “broken axis” be very explicit that you are doing so.\nA simple break as shown in Figure 5 A is insufficient. Rather something more extreme, like a large break marked by bright lines (e.g. Figure 5 B) is needed to ensure that the reader does not process the data without considering the axis break.\n\n\n\n\n\n\n\n\nFigure 5: Broken y-axes can be misleading if not clearly marked. Panel A includes a subtle y-axis break but risks misleading viewers. Panel B makes the break impossible to miss by using dashed lines and extra spacing. Always be loud and clear if you use a broken axis. Read more here.\n\n\n\n\n\n\nDishonestY: Unfair comparisons\nFewer babies are born in the U.S. on Sundays than any other day — with Saturdays close behind. When I first heard this I was amazed, but then I realized it makes sense – many births are scheduled C-sections, or induced in some other way, and doctors would prefer to take weekends off. But there is also a seasonality to births which cannot be explained by doctor’s schedules. Let’s look at this plot of the number of babies born each month of 2023 in Canada. While doing so, pay careful attention to the axes – as you will find that truncation isn’t the only way a y-axis can mislead\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRather than me telling you what’s wrong here I want you to figure out what’s wrong and fix it. But you won’t be on your own, this chatbot tutor is here to help!\n\n\n\nDishonestY: Y’s meaning depends on X: Sometimes values on the y-axis have different meanings for different values on the x-axis. For example, inflation tends to push prices up over time, and more people are born—or die—in larger populations. In these cases, showing raw values can mislead. It’s better to standardize the y-axis so it has a consistent meaning across x (like deaths per 1,000 people or inflation-adjusted cost), or to give viewers some point of reference so they can make fair comparisons.\n\n\n(Dis-)Honest X-Axes\nIt’s not just the y-axis that can mislead, there is plenty of opportunity for the x-axis to mislead as well. Common ways that the x-axis can mislead include:\n\nOrder that runs counter to expectations: Say we were plotting survival of Clarkia in four different temperatures – “Freezing”, “Cold”, “Warm”, and “Hot”. We would expect the x-axis to be in that order, but unless you tell it otherwise, R puts categorical variables in alphabetical order (i.e. “Cold”, “Freezing”, “Hot”, “Warm”). This will likely lead to patterns that surprise and confuse readers, as we explore in Figure 6.\nArbitrary spacing: Sometimes our categories suggest an order but not equal intervals — like “Control”, “Low”, “Medium”, and “Super High”. Plotting them on a linear x-axis makes the steps look evenly spaced, even if the treatment jump from “Medium” to “Super High” is much larger than from “Low” to “Medium”. This can trick readers into seeing a much sharper trend than is really there (e.g. Figure 7).\nInsufficient context: Seasonal ups and downs can be misused to make claims about long-term trends. For example, employment often drops in January as seasonal jobs disappear. But that doesn’t mean we should be reading headlines every year like “The year is off to a bad start.” That’s why it’s standard to present seasonally adjusted unemployment rates. Similar issues show up all over the biological world too - if you don’t consider seasonal or cyclical patterns, it’s easy to mislead or be misled.\n\nWith these ideas in mind, lets look at two plots (Figure 6 and Figure 7) showing trends in the temperature in Minnesota across the year:\n\nFig. 4Fig. 5\n\n\n\n\n\n\n\n\n\n\nFigure 6: (Dis-) Honest x-axis ordering: Average monthly high temperatures in Minnesota.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: (Dis-) Honest x-axis spacing: This plot uses only a subset of months (November, December, January, February, March, July) with a gap between March and July.\n\n\n\n\n\n\n\n\n\n\nIf you don’t see any plots here, click on any of the tabs. Then browse through all of them by changing between tabs.\nFrom these plots we can see that:\n\nBy having an order that runs counter to expectations, Figure 6 leads a reader to believe that temperature swings up and down many times dramatically across the year. This is because months are in alphabetical instead of sequential order.\n\nFix this by changing the order of categories (but this is harder than it sounds, we will work on that in the next chapter).\n\nBecause of the arbitrary spacing in Figure 7’s x-axis it looks like a sudden jump in July, but the jump is artificial - we’re just missing spring months.\n\nFix this by leaving a space on the x-axis where those categories should be (but make sure the missingness is not mistaken for zero).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Airline passengers over time: This plot shows monthly passenger counts in the US from 1949 to 1960, revealing strong seasonal trends and long-term growth. The final months of 1960 (highlighted in red) appear as a decline only when removed from this full context.\n\n\n\n\n\nBecause it does not provide sufficient context Figure 8 misleads readers into thinking that the airline industry was crashing in late 1960.\n\nFix this by providing sufficient context – i.e. the year-over-year data in Figure 9 shows that Figure 8 is a predictable seasonal decline, not a not a sustained decline.\n\n\n\n\n\n\n\n\n\n\nFigure 9: Seasonal fluctuations in US air travel (1949-1960).\n\n\n\n\n\n\n\n\nHonest Bin Sizes\n\n\n\n\nThe video above explains how bin sizes can mislead. This issue comes up when using histograms to explore the shape of a distribution. The problem is that bin size is a smoothing decision, and smoothing decisions always involve trade-offs:\n\n\nSmoothing decisions aren’t unique to histograms - similar issues arise when choosing a kernel bandwidth in a density plot, set a bin width for a bar chart, or even a zoom in on a map.\n\nToo few bins oversmooths the data—you might miss real structure like bimodality or skew.\n\nToo many bins adds visual noise—random variation starts to look like meaningful bumps and wiggles.\n\nTo get a better sense for this, play with the bin number in the interactive salmon body size example above. Watch how the story changes as you try all values from 3 to 10, then try a few larger values. As you explore, ask yourself: Which bin size gives a clear picture without hiding or exaggerating the structure in the data? while recognizing there not always a single “right” answer.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| column: page-right\n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(munsell)\nlibrary(bslib)\nlibrary(readr)\n\n# Define UI for app that draws a histogram ----\n\nui &lt;- fluidPage(\n  titlePanel(\"Bin size can mislead!\"),\n  \n  fluidRow(\n    column(4),  # Empty column for spacing\n    column(4,\n           numericInput(\"bins\", \"Number of bins\", value = 3,\n                        min = 2, max = 200, step = 1,width = \"22%\")\n    ),\n    column(4)   # Empty column for spacing\n  ),\n  \n  fluidRow(\n    column(12,\n      plotOutput(\"plot\", width = \"100%\", height = \"430px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n      salmon &lt;- read.csv('https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/salmon_body_size.csv')\n  output$plot &lt;- renderPlot({\n    library(ggplot2)\n\n    # Create ggplot histogram\n    ggplot(salmon, aes(x = mass_kg)) +\n      geom_histogram(bins = as.numeric(input$bins)+2, \n                     fill = \"salmon\", \n                     color = \"black\", \n                     alpha = 0.7) +\n      labs(title = \"Distribution of Body Mass in Salmon\",\n           x = \"Body Mass (kg)\",\n           y = \"Count\") +\n      scale_x_continuous(limits = c(0.9,3.6))\n  }, res = 150)\n}\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nAfter exploring this for a while try these short questions:\nQ1) Which bin number makes the reader think this distribution is unimodal and right-skewed? .\n\n\n\n\nQ1 Explanation\n\nWith 3 bins, the histogram oversmooths the data and there is no dip between peaks, but it’s not clear if the data are symmetric or skewed.\nThe histogram still oversmooths the data with 4 bins. But in this case the data appears more obviously right-skewed.\n\nQ2) Which is the smallest bin number that allows the reader to see that the data are clearly bimodal? .\n\n\n\n\nQ2 Explanation\n\nI accepted either six or seven. At six it seems like something is likely going on (counts seem to increase as x increases), and once you get to seven two obvious modes emerge.\n\nQ3) Which statement best describes the tradeoff in choosing the number of bins in a histogram? More bins always give more accurate results.Fewer bins always prevent overfitting.Fewer bins emphasize general patterns; more bins reveal detail.The number of bins doesn’t matter if the color is consistent.\n\nWhat about density plots?\nConcerns about bin size apply to smoothing in density plots. In ggplot, you can control the smoothing of a density plot using the adjust argument in the geom_density() function.\n\n\nAnother plotting option.\nIf no bin size seems to work well, you can display the cumulative frequency distribution using stat_ecdf(). This method avoids the binning issue altogether. The y-axis shows the proportion of data with values less than x, and bimodality is revealed by the two steep slopes in the plot. However, these plots can be harder for inexperienced readers to interpret.\n\n\n\n(Dis-)Honest color choice\nEven color can mislead. If colors imply an order (e.g., light to dark) but the categories don’t follow a logical sequence, viewers may misinterpret the pattern. Figure 10 shows temperature across America, but “warm” has a darker red than “Very hot”. This could mislead readers into thinking North Carolina is hotter than Texas. This is made even worse because “Warm” comes after “Very hot” in the legend. Fix this by making sure the order of colors (and color keys) make some sense and follow the reader’s expectations.\n\n\n\n\n\n\n\n\nFigure 10: U.S. states grouped by temperature category—Freezing, Cold, Mild, Warm, and Very Hot. Unfortunately, the colors do not follow a logical temperature progression - “Warm” is shown in a dark maroon and “Very Hot” is bright red, while “Freezing” and “Cold” are flipped on the blue scale. This non-intuitive color order (darker doesn’t always mean more extreme) makes the map unnecessarily confusing. What makes this even worse is that the color key also goes in this confusing order.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/honest_plots.html#section-summary",
    "href": "book_sections/data_viz/honest_plots.html#section-summary",
    "title": "• 9. Honest plots",
    "section": "Section summary",
    "text": "Section summary\nEven honest people can make dishonest plots. Truncated y-axes, weird or uneven x-axes, misleading bin sizes, and unstandardized values can all distort what your audience sees—especially when viewed quickly or from a distance. A good plot doesn’t just show the data; it helps readers reach the right conclusion without extra mental gymnastics. After you make a plot, show it to someone—fast, far away, or with minimal labels—and ask what they think it says. If they walk away with a different takeaway than you intended, your plot needs work.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Honest plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/transparent_plots.html",
    "href": "book_sections/data_viz/transparent_plots.html",
    "title": "• 9. Transparent plots",
    "section": "",
    "text": "Good plots are transparent\nHere’s a motivating scenario and learning goals section that fits your tone and structure:\nTransparency is a great way to communicate honestly and encourage active engagement. Transparently presenting our data empowers our readers to evaluate our claims, critique them, test them for themselves, and even uncover new insights in our data. As such, showing the data is a critical step toward building trust with a skeptical audience and invites them to engage with the data themselves.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Transparent plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/transparent_plots.html#good-plots-are-transparent",
    "href": "book_sections/data_viz/transparent_plots.html#good-plots-are-transparent",
    "title": "• 9. Transparent plots",
    "section": "",
    "text": "Showing Your Data\nAs we saw in the datasauRus example (revisited in Figure 1), relying on summary statistics can obscure important patterns. Similarly, plots that only show summaries (e.g., barplots of means) fail to provide the full picture. Whenever possible, show all of your data.\n\n\n\n\n\n\n\n\nFigure 2: Always show your data! (A): A cartoon by allison_horst illustrating how summary statistics (e.g. means +/- error bars) can obscure interesting structure in the data. The raw data on the right reveal a bimodal distribution hidden by the simple summary. This figure is reformatted this for space here is the original. (B) The Meme-style reaction images of Nicholas Cage and Pedro Pascal from “The Unbearable Weight of Massive Talent” shows that a raw data accompanied by data summaries is generally preferred to a simple summary because the prior shows the data.\n\n\n\n\n\n\n\nBarplots aren’t inherently bad. While barplots should not be used to report means, they are effective for presenting proportions or count data.\n\nWhen I say: “Show your data”,\nI mean: “Show your F***NG data! All of it. Not a mean, not a trendline. SHOW ALL OF YOUR DATA!”\n\n\nIs it ever OK to not show all the data?\nDespite my emphatic cursing above, there are rare cases where showing all your data is actually less honest than showing a summary. This usually happens when overplotting becomes a problem - when there’s so much data, or so many points stacked at a single value, that showing every data point hides the overall pattern instead of revealing it. Below, we’ll walk through examples where showing the full data might be misleading and what to do instead.\n\n\n\nTransparency Avoids Overplotting\nSometimes, showing all your data can actually obscure patterns—a problem known as overplotting. Overplotting occurs when data points in a plot overlap or cluster so densely that it’s difficult or impossible to discern individual values or patterns in the data. This typically happens when you have a large number of data points, or when the range of data values is narrow, causing points to pile on top of each other. Overplotting can obscure the underlying distribution, relationships, or trends, making it hard to interpret the data accurately.\nFigure 3 shows several techniques (like jittering, using transparency, etc.), and alternative plots (e.g., density plots, box plots, or sina plots) that we can use to reveal patterns that would otherwise be hidden.\n\n\n\n\n\n\n\n\nFigure 3: Sometimes showing all the data hides patterns. (a) shows overplotting, where data points overlap and obscure the distribution of values. (b–i) demonstrate solutions for overplotting. The sina plot (f) is one of my favorites because it shows both the shape of the data and individual data points. After installing and loading the ggforce package, you can use geom_sina() to create a sina plot. Data from Beall (2006). Download the data here.\n\n\n\n\n\n\n\nTransparency Links Data, Code, and Results\nThe most transparent data are fully reproducible. Readers should be able to download your code and data, replicate your analysis, and understand the dataset well enough to perform their own analysis. As discussed in our previous sections on reproducible science, this level of transparency is becoming the standard in scientific research.\n\n\n\n\nBeall, C. M. (2006). Andean, Tibetan, and Ethiopian patterns of adaptation to high-altitude hypoxia. Integrative and Comparative Biology, 46(1), 18–24. https://doi.org/10.1093/icb/icj004",
    "crumbs": [
      "9. Better Figures",
      "• 9. Transparent plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/clear_plots.html",
    "href": "book_sections/data_viz/clear_plots.html",
    "title": "• 9. Clear plots",
    "section": "",
    "text": "Good Figures Are Clear\nGood plots are clear, with messages that stand out. To achieve clarity in a plot, we need to make labels informative and readable, minimize cognitive burden, make the point obvious, and avoid distractions.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Clear plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/clear_plots.html#good-figures-are-clear",
    "href": "book_sections/data_viz/clear_plots.html#good-figures-are-clear",
    "title": "• 9. Clear plots",
    "section": "",
    "text": "Have Informative and Readable Labels This should go without saying, but make sure that your labels can be read and that readers know what these labels mean. Sometimes labels are unclear because authors simply don’t look at the figures, other times they use shorthand that might be clear to experts or the team working on the project, but not to outsiders or non-experts.\nMinimize cognitive burden Two of my favorite books are Crime and Punishment and 100 Years of Solitude. While they’re great stories, I remember struggling when trying to track relationships between characters or remember that Raskolnikov and Rodya are the same person. Scientific communication is not \\(19^{th}\\) century Russian literature – as science communicators we strive to be consistent and to minimize how much your reader has to keep in their mind.\nMake points obvious A scientific figure should tell a story, but it shouldn’t be a mystery or a puzzle. Science is complicated enough, and people who read science are often busy - so the message of a plot should be clear. Readers should use their brains making sense of the implications of scientific findings, not figuring out what the finding is.\nAvoid distractions Readers should focus on your story, not on unnecessary visuals or effects.\n\n\nClear Plots Highlight Patterns\nGood plots do more than “show the data.” They help your reader see what matters, fast. That means intentionally designing plots to make important patterns and comparisons obvious - especially the ones that are central to your scientific story.\n\nBring Out Key Comparisons\nBecause data cannot speak, we can’t just “present the data.” Good plots are designed to help readers see and evaluate the patterns central to the story you’re telling. Just like in storytelling, emphasizing the wrong details can distract and mislead. This is important becase the same dataset can tell very different stories depending on how it is plotted. This often means guiding your reader’s eye with color, layout, or label placement.\nFigure 1 shows one such example:\n\nBecause most people at a given time do not have COVID-19 including the “non-covid” cases makes it hard to see any difference between the placebo and the mRNA vaccine (Figure 1 A-B).\nBy focusing on the proportion of severe vs. mild COVID cases in the placebo and vaccine group, Figure 1 C shows that vaccinated folks have proportionally fewer cases of extreme COVID among the vaccinated. But it does not provide information about the total number of infections.\nFigure 1 D-F show both the reduced number of cases and the lower proportion of severe COVID among vaccinated participants.\n\n\n\n\n\n\n\n\n\nFigure 1: Same data, different message: Various plots of the Moderna vaccine trial data tell different stories. (a) and (b) imply the vaccine isn’t effective by highlighting that most participants didn’t develop COVID. (c) compares the severity of cases by treatment but hides the vaccine’s effect on infection risk. (d) and (e) emphasize the severity of cases, while (f) highlights vaccine efficacy but makes it harder to compare severity. (f) is my favorite despite this.\n\n\n\n\n\n\n\nConsider How People Process Images\nWhen creating a plot, consider not just the data but how readers will interpret it. You don’t need to be a graphic designer to make a good plot. Try asking a friend what they see in your plot—can they easily spot the pattern or comparison you want to highlight?\nLet’s try this by looking at Figure 2 (you be the friend), which like Figure 1 shows different presentations of the same data. As you examine these plots consider which one allows you to best estimate the difference in X and Y.\n\n\n\n\n\n\n\n\n\nFigure 2: Facilitate comparisons – Which plot makes it easiest to compare X and Y? Image from slide 28 of this presentation by Karl Broman.\n\n\n\n\n\n\n\n\n\nClear Plots Use Informative and Readable Labels\nBeyond comparisons, clarity often breaks down when readers can’t read axis labels. Often labels for categorical variables are quite long, potentially running over each other and making them difficult to read. Poor plots like Figure 3 A, pollute the literature, and arise when authors don’t check how their plot labels render. Figure 3 b-d show alternatives:\n\nb Abbreviates to prevent labels from jumbling into each other. This definitely helps, but now the reader must think for a bit to connect the label to the actual variable.\n\nc Rotates the labels 90 degrees. This too is helpfull, but we rarely read at this angle.\n\nd Flips the axes. The last option is my favorite because it is the most natural to read.\n\n\n\n\n\n\n\n\n\nFigure 3: Different ways to handle long category labels. Plot A shows overlapping text that is difficult to read. Plot B uses abbreviations, C rotates the labels 90 degrees, and D flips the plot to use the y-axis for labels. Plot D is the clearest and most readable, making comparisons easy.\n\n\n\n\n\n\n\nClear Plots Are Consistent\nVisual consistency across figures can support storytelling, or undermine it when neglected. In a project with multiple plots, visual consistency helps readers follow your story. Unfortunately, R doesn’t remember color assignments unless you explicitly set them. This becomes a problem when you subset data - for example, plotting just two groups from an earlier plot that included six.\nIn Figure 4 a, South America and Asia appear in new default colors (blue and red), breaking the visual connection to earlier plots (e.g. Figure 3) where they were purple and brown. This disrupts the reader’s expectations. You can fix this by explicitly assigning colors, as shown in b, or go further and order the legend to match the plot layout, as in c.\n\n\n\n\n\n\n\n\nFigure 4: Maintaining consistency in color and legend order helps readers make comparisons across figures. (a) shows a mismatch in colors due to default reassignments. (b) fixes the color mapping to match earlier plots. (c) improves further by reordering the legend to match the visual flow.\n\n\n\n\n\n\nMaintaining consistency in color mapping, as shown in Figure @ref(fig-consistent)c, and sorting labels sensibly makes plots even easier to process.\n\n\n\nClear Plots Use Direct Labeling (When Helpful)\n\n\n\n\n\n\n\n\n\nFigure 5: Density plot showing student_ratio values for two regions (South America and Asia), with direct labels placed on the respective curves instead of using a separate legend. This use of direct labeling reduces the cognitive burden.\n\n\n\n\nFigure 5 improves on Figure Figure 4 C by using direct labeling instead of a legend. Readers no longer need to scan over to the legend, decode which color is which, and then return to the data to interpret what they’re seeing. Instead, the category labels are placed right on the plot, where the data are. This reduces cognitive load, and brings the readers’ attention where it belongs – the data.\n\n\nClear Plots Order Categories Sensibly\nHow you order categorical variables on an axis can dramatically affect a plot’s readability. Be deliberate about how you order categories on the x-axis (when categorical). Here are three guiding principles to help you:\n\nIf the data are ordinal (e.g., months of the year), place them in their natural order.\nFor nominal categories order by a meaningful statistic (e.g. decreasing mean or median) to make trends or extremes stand out.\n\nFor many low-frequency categories, consider grouping them into “Other” and placing it last.\n\nFigure 6 compares the default alphabetical order of continents (panel A) with the same plot reordered by the mean student–teacher ratio (panel B). The second version reveals the pattern more clearly and makes it easier to identify the lowest- and highest-ratio regions.\n\n\n\n\n\n\n\n\nFigure 6: Ordering categories by value makes patterns more interpretable. Panel A uses alphabetical order, while Panel B sorts by the mean student–teacher ratio, making differences easier to spot.\n\n\n\n\n\n\nBy default, R arranges categories alphabetically. This is rarely what you want. The forcats package gives you tools to arrange categorical variables more sensibly:\n\nfct_relevel() allows you to manually reorder factor levels in a specific order. This is great for ordinal variables like “high”, “medium”, “low”.\nfct_reorder() allows you to reorder factor levels based on a summary of another variable for when you want nominal categorical variables ordered by means, medians, etc. (as in Figure 6 B).\nfct_lump_*() allows you to group rare categories in “other”.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Clear plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/avoid_distractions.html",
    "href": "book_sections/data_viz/avoid_distractions.html",
    "title": "• 9. Avoid Distractions",
    "section": "",
    "text": "Good Figures Avoid Distractions\nFigure 1: Just because you can make a fancy plot doesn’t mean you should. A reminder from Jeff Goldblum as Dr. Ian Malcolm in Jurassic Park.\nBuckminster Fuller aimed to be “invisible,” letting his ideas, not his appearance, speak. This principle, which underlies the name of my favorite podcast on design, 99% Invisible. The same principle applies to figures A good figure calls attention to patterns in the data, not to itself.\nJust because you found an R package to make a Sankey diagram or a 3D bar chart doesn’t mean you should use it. Good data viz starts with a question and ends with a design that answers it, not the other way around. Before (or more realistically, halfway through) making an overly complex plot think, WWIMS (“What Would Ian Malcolm Say?” Figure 1), and ask yourself:\nBelow I illustrate this principle for common issues in data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Avoid Distractions"
    ]
  },
  {
    "objectID": "book_sections/data_viz/avoid_distractions.html#good-figures-avoid-distractions",
    "href": "book_sections/data_viz/avoid_distractions.html#good-figures-avoid-distractions",
    "title": "• 9. Avoid Distractions",
    "section": "",
    "text": "What is the viewer supposed to learn?\nIs this visual emphasizing the right relationships?\n\nIs there a simpler or more direct alternative?\n\n\n\nWhat the duck?\n\n\n\n\n\n\n\n\n\nFigure 2: The Big Duck was built in 1931 by duck farmer Martin Maurer and used as a shop to sell ducks, dairy, and duck eggs. This building inspired the architectural term later borrowed by Edward Tufte to critique overly stylized, decorative elements in data visualizations. In this metaphor, a “duck” prioritizes form over function. Image posted to Wikimedia commons by Mike Peel and shared under a CC-BY-SA-4.0 license.\n\n\n\n\n\nWhen a graphic is taken over by decorative forms or computer debris, when the data measures and structures become Design Elements, when the overall design purveys Graphical Style rather than quantitative information, then the graphic may be called a duck in honor of the duck-form store, “Big Duck.” For this building the whole structure is itself decoration, just as in the duck data graphic.\n\n— Edward Tufte\n\n\nTufte (1983) coined the term ‘duck’ to describe figures that showcase cleverness rather than data. An extreme example is the banana genome paper, where a banana drawing obscures the Venn diagram’s meaning (Figure 3). The image attempts to show gene families shared across three plant genomes, but fails because superimposing this over a cartoon banana is too distracting.\n\nResist the temptation to create flashy but ineffective visuals.\n\nRemember: visuals should prioritize clarity over aesthetics.\n\n\n\n\n\n\n\n\n\nFigure 3: Perhaps the ultimate data viz duck in comparative genomics. This plot is bananas. Figure 4 of the banana genome paper (D’Hont et al., 2012).\n\n\n\n\n\n\n\nDon’t use 3D or animation unnecessarily\n\n\n\n\n\n\n\n\n\nFigure 4: This rotating 3D pie chart demonstrates that an overcomplicated chart that looks flashy can be a problem.\n\n\n\n\n3D and animation are only helpful for specific purposes, like showing protein structures or time-lapse data. Unless the data our the audience calls for it, resist the urge to use them otherwise. More often than not, these flashy formats distract from your message and confuse your reader. They make your plot harder to read, harder to interpret, and much easier to ignore(as demonstrated in Figure 4).\n\n\nAvoid “glass slippers”\nA “glass slipper” (Figure 5) is when a visualization designed for one purpose is misapplied elsewhere, leading to confusion. Keep your visual tools fit for purpose. See this fun video from Calling Bullshit if you like.\n\n\n\n\nLately I've been getting all my best bullshit from promoted tweets. Here from @NexthinkNews, a classic \"glass slipper\" visualization (https://t.co/09curqq0tU), in which data is shoehorned into a highly specialized and entirely inappropriate format. pic.twitter.com/aYGxBRkHPG— Calling Bullshit (@callin_bull) March 14, 2019\n\n\n\nFigure 5: A tweet from the Calling Bullshit highlights a ‘glass slipper’. This ‘Periodic Table of IT Ops Tools’ mimics the structure of the chemical periodic table but does not reflect any real periodicity or organizational logic in the data it presents.\n\n\n\n\n\nThe examples above are cases of what Tufte called chartjunk – visual elaborations that are not needed to understand the information in the plot. Such additions can distract the viewer, increase interpretation time, or even mislead.\n\n\n\nIn defense of (occasional) chartjunk\nWe will soon consider the importance of considering audience when making plots. Such consideration can reveal that there are circumstances in which rules of data visualization are to be broken. Although much derided, chartjunk has numerous benefits:\n\nChartjunk can increase long-term memorability of the chart.\nChartjunk, in the form of semantically meaningful icons, can increase accessibility of charts for people with Intellectual and Developmental Disabilities.\n\nSo if you’re working on a serious plot for a scientific publication, avoid chartjunk. But if you’re aiming to get someone to come to your poster or remember your talk in a day full of seminars, you may find that limited and tasteful “chartjunk” is useful.\n\nWho is Tufte? I usually try to avoid centering famous individuals in science or statistics, because these fields are — in reality — massive collaborative efforts. But it’s worth knowing about Edward Tufte, not because he’s a singular genius, but because many widely cited “rules” of data visualization trace back to his book The Visual Display of Quantitative Information (Tufte (1983)).\nThis work shaped how people think about clutter, ink, and visual integrity and is the Bible of data visualization. But Tufte is a smart person with strong opinions, not a god. So we are allowed to disagree with him. I, for example, think there are times when a little chartjunk can actually help (see above).\n\n\n\n\n\nD’Hont, A., Denoeud, F., Aury, J.-M., Baurens, F.-C., Carreel, F., Garsmeur, O., Noel, B., Bocs, S., Droc, G., Rouard, M., Da Silva, C., Jabbari, K., Cardi, C., Poulain, J., Souquet, M., Labadie, K., Jourda, C., Lengellé, J., Rodier-Goud, M., … Wincker, P. (2012). The banana (musa acuminata) genome and the evolution of monocotyledonous plants. Nature, 488(7410), 213–217. https://doi.org/10.1038/nature11241\n\n\nTufte, E. R. (1983). The visual display of quantitative information (p. 197). pub-gp.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Avoid Distractions"
    ]
  },
  {
    "objectID": "book_sections/data_viz/accessible_plots.html",
    "href": "book_sections/data_viz/accessible_plots.html",
    "title": "• 9. Accessible Plots",
    "section": "",
    "text": "Good Figures Are Accessible\nFigure 1: See Section 508 of Amendment to the Rehabilitation Act.\nMaking figures accessible for all tends to make them better for everyone. Consider the diversity of people who may view your figure—this could include readers with color blindness, low vision, those who rely on screen readers, or even those who print your figure in black and white. A good figure should be interpretable by all of these individuals.\nWe have already highlighted several good practices. For example, describing the results of a figure in words can make it accessible to blind or visually impaired readers, while direct labeling can make the content clearer to readers with color vision deficiencies. These examples illustrate the benefits of universal design - they make figures better for all audiences, regardless of specific needs. ACCESIBILITY HELPS EVERYONE!",
    "crumbs": [
      "9. Better Figures",
      "• 9. Accessible Plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/accessible_plots.html#good-figures-are-accessible",
    "href": "book_sections/data_viz/accessible_plots.html#good-figures-are-accessible",
    "title": "• 9. Accessible Plots",
    "section": "",
    "text": "Color\n\n\n\n\n\n\n\n\n\nFigure 2: Ishihara colorblindness test plate: people with red-green colorblindness may not see the number 74.\n\n\n\n\nChoosing effective colors is a challenge. Ensure that your color choices are easy to distinguish, particularly if printed in grayscale or viewed by colorblind individuals. Many R tools can help with this, including the colorspace package. Toensure that your plots are accessible, I recommend:\n\nTesting your figures through a color vision deficiency emulator (like the one embedded below) to see how your plots appear to readers with color vision deficiencies.\n\nPrinting your plots in black and white, to see how it looks in that format.\n\nAnd, using redundant coding and direct labeling to increase accessibility.\n\n\nTry this color vision deficiency emulator either below or at http://hclwizard.org/cvdemulator/ to see how other people might see your plot.\n\n\n\n\n\n\nRedundant coding - such as mapping shape, line type, or pattern in addition to color for the same variable provides readers with multiple ways to differentiate categories. This helps because - although color is a great way to differentiate variables, many readers have color vision deficiencies or may print your work in black and white.\n\n\n\nSize\nEnsure that all elements in your figure, including text, axis labels, and legends, are large enough to be easily read by people with poor eyesight. Always err on the side of larger text. Small text not only diminishes accessibility but can also make figures look cluttered and unclear.\n\n\n\n\n\n\n\n\nFigure 3: Bigger text is easier to read. Image from Advanced Data Science\n\n\n\n\n\n\nTest the size of elements in your figures by viewing them at reduced sizes or printing them. If the labels and details are still readable, they’re likely large enough.\n\n\n\nAlt Text for Figures\nWhen creating figures for digital use (e.g., websites, PDFs, or presentations), it’s important to include descriptive alt text for individuals who rely on screen readers. Alt text provides a textual description of the figure, ensuring that people who cannot see the image can still understand its content.\nGood alt text should describe the key information the figure conveys without unnecessary detail. It’s not enough to simply say “Figure showing data”; you need to explain what the reader should take away from the visual representation.\n\n\n\nAccessibility Checklist\nUse this to quickly review your figures before sharing them with the world:\n\nReadable text: Can all text (titles, axis labels, legends) be read at a glance—even when printed or viewed small?\nAlt text (if digital): Does your figure include a short but clear description for readers using screen readers?\nColor works for everyone:\n\nHave you tested your plot in grayscale or using a color vision deficiency simulator?\nIf color alone might not be distinguishable, have you used direct labeling (e.g., labels placed on the data) and/or redundant coding (e.g., shape, line type, pattern)?\n\nDirect labeling: When appropriate, are group labels placed directly on the plot (rather than in a separate legend)?\nRedundant coding: If color is doing work, are you also using shape, line type, or other cues to reinforce group differences?",
    "crumbs": [
      "9. Better Figures",
      "• 9. Accessible Plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/writing_about_figures.html",
    "href": "book_sections/data_viz/writing_about_figures.html",
    "title": "• 9. Writing about plots",
    "section": "",
    "text": "Writing About Figures\nGood figures should be clear enough to allow readers to interpret them on their own. Ideally, a reader should be able to examine your figure and draw a reasonable conclusion. But figures do not exist on their own, they exist in the in some broader context – a paper, a poster, a book, a presentation, etc… So, although good figures should stand alone, we can enhance the reader’s understanding by guiding their interpretation and emphasizing key takeaways.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Writing about plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/writing_about_figures.html#writing-about-figures",
    "href": "book_sections/data_viz/writing_about_figures.html#writing-about-figures",
    "title": "• 9. Writing about plots",
    "section": "",
    "text": "People read papers differently.\nSome readers barely glance at figures. Others skim the figures first, then dip into the text. Some go back and forth. Because we can’t control how people engage with our work, we need to make sure that both our figures and our writing about them are clear, complementary, and accessible to all readers.\n\n\nWriting Figure Captions\nAlthough well-designed figures should be interpretable without a figure captions, a good caption adds value by surfacing key takeaways and helpful context. A good caption does not merely restate the figure but rather presents additional context, support and background for the point clearly conveyed visually.\nFigure 1 shows an example of a bad (a) and good (b) figure and legend combination.\n\nFigure 1 A fails because it treats the legend as a crutch. The reader has to consult it just to figure out what the axes, groups, and colors mean - basic information that a figure should make obvious. This shortcoming forces readers to hold too much in memory, shifting their focus from interpreting the results to decoding the design.\nFigure 1 B succeeds because it adds helpful detail to a figure that is interpretable without a caption. Once you’ve taken in the visual message, the caption deepens your understanding by adding nuance and context.\n\n\n\n\n\n\n\n\n\nFigure 1: An example of bad (A), and good (B) figure / caption combinations. Readers should be able to understand a well-designed plot without reading the caption, this is not possible in A, but pretty obvious in B. A good caption adds context—not basic decoding.\n\n\n\n\n\n\n\nWriting “Alt text”\nAlt text should help readers who can’t see your figure still grasp its message. Think of alt text like narrating your figure to someone over the phone: what would you say so they could understand it? Below is an example of bad and good alt text for Figure 1 B:\n\n\n\n\n\n\n\n😞Bad alt text for Figure 1 B😞\n☺️Good alt text for Figure 1 B☺️\n\n\n\n\n“Stacked bar plot showing the the COVID vaccine works.”\n“Stacked bar plot showing COVID cases by severity for placebo and mRNA-1273 groups. The placebo group has many more cases, including severe ones (dark blue), while the vaccinated group has only a few mild cases (light blue).”\n\n\n\n\n\nWriting About Figures in Text\nWriting up results is one of the most important parts of doing science - if your work is not clearly communicated, it’s unlikely to make a lasting impact. When describing results, be explicit about what in a figure supports your conclusion. Rather than writing “Figure X shows that Group A grows faster,” aim for something more specific, like: “The steeper increase of Y in Group A than in Group B (Figure X) suggests that…” This style helps the reader connect your interpretation directly to visual evidence.\nWhen reading (or writing) text that discusses a figure, first look at the figure and think about its message. Then, consider:\n\nWhat features of the figure support the claims in the paper?\n\nAre there parts of the figure that challenge or complicate the interpretation in the paper?\n\nBelow is a comparison of weak vs. effective writing about the same figure. The second version goes beyond describing the visual and emphasizes the figure’s statistical and biological meaning.\n\n\n\n\n\n\n\n😞 A bad write-up Figure 1 B 😞\n☺️ A better write-up for Figure 1 B ☺️\n\n\n\n\n“Figure 1 B compares COVID cases and severity of these cases for treatments and controls.”\n“Figure 1 B shows a significant difference in COVID case incidence between the placebo group and those vaccinated with Moderna’s mRNA-1273 vaccine. Of the 15,000 individuals in the placebo group, 185 contracted COVID, while only 11 of the 15,000 vaccinated individuals did. Additionally, none of the vaccinated participants who became infected developed severe COVID, whereas 30 of the 185 infected placebo recipients had severe cases (compare the dark blue bar above the control group and its absence above the mRNA-1273 group).”",
    "crumbs": [
      "9. Better Figures",
      "• 9. Writing about plots"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html",
    "href": "book_sections/data_viz/dataviz_summary.html",
    "title": "• 9. Dataviz Summary",
    "section": "",
    "text": "Chapter Summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\n“Tall Infographics” cartoon from xkcd. Rollover text says: “Big data” does not just mean increasing font size. Explanation here.\nAn effective visualization allows you to rapidly communicate your key findings to your audience. The best visualizations are honest, transparent, clear, and accessible. They highlight important patterns while minimizing confusion or distraction, and they are thoughtfully tailored to both the audience and the format. Great figures avoid misleading elements and use design choices (e.g. captions, color, and labels) to guide interpretation.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_chapter-summary",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_chapter-summary",
    "title": "• 9. Dataviz Summary",
    "section": "",
    "text": "Chatbot tutor\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_practice-questions",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_practice-questions",
    "title": "• 9. Dataviz Summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nTry these questions!\n\n\n\n\n\n\n\n\nFigure 1: Hemoglobin levels of people native to different countries.\n\n\n\n\n\n\nQ1) Which of the plots in Figure 1 keep all information even if printed in black and white? a & ba & ca & db & cb & dc & d\nQ2) Which of the plots in Figure 1 is still somewhat useful but loses some information? abcd\n\n\n\n\n\n\n\n\n\nFigure 2: Canabalistic dads.\n\n\n\n\n\n\nQ3) Which plot in Figure 2 is better? ABIt depends…Don’t judge plots\nQ4) Your chose your answer, above, because the better plot Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssibleI told you we dont judge plots, jeez\n\n\n\n\n\n\n\n\n\nFigure 3: Canabalistic dads, revisited.\n\n\n\n\n\n\nQ5) Which feature of Figure 3 is better than Figure 2? Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssible\nQ6) Which feature of Figure 2 is better than Figure 3? Shows all the dataMakes patterns easy to seePresents data honestlyDraws graphics clearlyIs accesssible\n\n\n\n\n\n\n\n\n\nFigure 4: Canabalistic dads, revisited (again).\n\n\n\n\n\n\nQ7) What is the biggest problem with Figure 4? It does not show all the dataIt does not make patterns easy to seeIt does not display patterns honestlyIt does not draw graphics clearly\n\n\nExplanation\n\nThe y-axis is labeled \"count,\" so unlike Figure 2 and Figure 3, this plot shows only the number of cannibalism cases—not the proportion. As a result, a reader might misinterpret Figure 4 and incorrectly conclude that broods with only one father are the most susceptible to cannibalism.\n\nQ8) Which of the figure above do you like the best and why?\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Data viz test from an online advertisement.\n\n\n\n\n\n\nQ9) I stole Figure 5 from a company selling a data vizclass. Examine their plot and find at least three bad data viz practices. Then say which one you think its the worst and why.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_glossary-of-terms",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_glossary-of-terms",
    "title": "• 9. Dataviz Summary",
    "section": "📊 Glossary of Terms",
    "text": "📊 Glossary of Terms\n\n\n🏷 1. ️ Figure Elements & Interpretation\n\nLegend: A guide that explains the meaning of colors, symbols, or line types in a plot. Helpful when symbols are ambiguous, but often unnecessary when direct labeling is used.\nCaption: Text beneath a figure that highlights the main point and guides the reader’s interpretation. A good caption doesn’t just restate what’s shown—it helps make sense of it.\nDirect Labeling: Placing labels directly on or near data elements (e.g., lines, points, bars), so viewers don’t have to cross-reference with a legend. Especially useful in talks and posters.\nRedundant Coding: Encoding the same variable multiple ways (e.g., using both color and shape for species). Can increase accessibility but should be used carefully to avoid clutter.\n\n\n\n♿ 2. Accessibility & Universal Design\n\nAlt Text: A textual description of a figure, written for people who cannot see it. Good alt text conveys the message of the figure, not just its parts.\nAccessibility: Designing figures so they can be understood by people with diverse abilities (e.g., colorblindness, low vision, screen reader users). Often overlaps with universal design.\nColorblindness: A common visual condition that affects how people perceive color. Plots should use color palettes and redundancy (e.g., line types) to remain interpretable without relying on color alone.\nUniversal Design: The principle of creating products and experiences—like data visualizations—that work well for as many people as possible, regardless of ability.\n\n\n\n💥 3. Visual Clarity & Distraction\n\nOverplotting: When data points are so densely packed they obscure patterns or hide important features. Common with large datasets; solutions include transparency, jittering, or summarizing.\nChartjunk: Any visual element in a plot that doesn’t help convey the data—like heavy gridlines, excessive shading, or unnecessary 3D effects. Coined by Edward Tufte.\nData Viz “Duck”: A graphic with unnecessary visual decoration (named after a duck-shaped building in Long Island). A plot that prioritizes aesthetics or novelty over clarity.\nCognitive Burden: The mental effort required to interpret a figure. Good visualizations reduce cognitive burden by being clear, consistent, and well-structured.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_key-r-functions",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_key-r-functions",
    "title": "• 9. Dataviz Summary",
    "section": "Key R Functions",
    "text": "Key R Functions\n\nThis section did not focus on R, but rather concepts for data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#r-packages-introduced",
    "href": "book_sections/data_viz/dataviz_summary.html#r-packages-introduced",
    "title": "• 9. Dataviz Summary",
    "section": "R Packages Introduced",
    "text": "R Packages Introduced\n\nThis section did not focus on R, but rather concepts for data visualization.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/data_viz/dataviz_summary.html#datviz_summary_r-packages-introduced",
    "href": "book_sections/data_viz/dataviz_summary.html#datviz_summary_r-packages-introduced",
    "title": "• 9. Dataviz Summary",
    "section": "Additional resources",
    "text": "Additional resources\n\nOther web resources:\n\nFundamentals of Data Visualization (Wilke (2019)): A free online book about best practices in data viz. Also available in physical form.\n\nStorytelling with Data: The work of Cole Nussbaumer Knaflic. This website links to her books, a usefull chart guide and more (see links to her most relevant podcast episodes below).\n\nThoughts on poster presentations: NPR article on a brief trend in minimal poster design. A critique of this ide in Forbes.\nAxes of evil: How to lie with graphs: This short blogpost goes over some classic dishonest graphs.\n\nVideos:\n\nThe Art of Data Visualization | Off Book | PBS Digital Studios.\nCalling Bullshit: Misleading axes, Manipulating bin size Data viz ducks and Duck hunting, Glass slippers, The Principle of Proportional Ink.\n\nCorrelation and Causation: “Correlations are often used to make claims about causation. Be careful about the direction in which causality goes. For example: do food stamps cause poverty?”\n\nWhat are Correlations? :“Jevin providers an informal introduction to linear correlations.”\n\nSpurious Correlations?: “We look at Tyler Vigen’s silly examples of quantities appear to be correlated over time), and note that scientific studies may accidentally pick up on similarly meaningless relationships.”\n\nCorrelation Exercise” “When is correlation all you need, and causation is beside the point? Can you figure out which way causality goes for each of several correlations?”\nCommon Causes: “We explain how common causes can generate correlations between otherwise unrelated variables, and look at the correlational evidence that storks bring babies. We look at the need to think about multiple contributing causes. The fallacy of post hoc propter ergo hoc: the mistaken belief that if two events happen sequentially, the first must have caused the second.”\n\nManipulative Experiments: “We look at how manipulative experiments can be used to work out the direction of causation in correlated variables, and sum up the questions one should ask when presented with a correlation.\n\n\nPodcasts:\n\nStorytelling with data: Here are some episodes that I think best complement this chapter #4 it depends…., #4 it depends…., #8 the many myths of data visualization, #10 right place, right graph, #17 which graph should I use?, #43 misleading graphs, #64 Beginner mistakes in data viz.\n\nSocial:\n\nGraph Crimes.\n\n\n\n\n\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media.",
    "crumbs": [
      "9. Better Figures",
      "• 9. Dataviz Summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html",
    "href": "book_sections/betteR_plots.html",
    "title": "10. Better Figures in R",
    "section": "",
    "text": "Creating Engaging, Attractive Plots in R\nYou’ve already been introduced to the basics of ggplot and explored the key elements that make for effective figures. But at this point, you might be feeling a bit frustrated. You know how to generate plots in R, and you understand what makes a plot good, yet creating polished, impactful visuals in R still seems challenging. My advice is threefold:",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#creating-engaging-attractive-plots-in-r",
    "href": "book_sections/betteR_plots.html#creating-engaging-attractive-plots-in-r",
    "title": "10. Better Figures in R",
    "section": "",
    "text": "Start with the right plot for your data. For most exploratory plots—which will make up about 95% of what you create—this is key. Often, a well-chosen plot paired with a couple of quick R tricks can make your visuals clear and informative.\nThis chapter is here to guide you through the rest. For more advanced plotting and customization, take a look at these excellent resources: The R Graphics Cookbook (Chang, 2020), ggplot2: Elegant Graphics for Data Analysis (Wickham, 2016), Data Visualization: A Practical Introduction (Healy, 2018), and Modern Data Visualization with R (Kabacoff, 2024).\n\n\n\n\n\n\n\nThe right (gg)plot for your data (click to expand)\n\n\n\n\n\n\n\n\nData Type\nSuggested Plot(s)\nLikely geom\nWhat It Shows\n\n\n\n\nOne numeric variable\nHistogram, Density plot\ngeom_histogram, geom_density\nDistribution shape, spread, skew, outliers\n\n\nOne categorical + one numeric variable\nBoxplot, Violin plot, Sina plot\ngeom_boxplot, geom_violin, geom_sina\nGroup comparisons, spread, outliers\n\n\nTwo numeric variables\nScatterplot\ngeom_point + geom_smooth\nTrends, clusters, correlation, outliers\n\n\nCategorical counts or proportions\nBar plot, Stacked bar plot, Mosaic plot\ngeom_bar, geom_col, geom_mosaic\nFrequencies, relative proportions\n\n\nTime series (numeric over time)\nLine plot\ngeom_line\nTrends over time",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#review-what-makes-a-good-plot",
    "href": "book_sections/betteR_plots.html#review-what-makes-a-good-plot",
    "title": "10. Better Figures in R",
    "section": "Review: What Makes a Good Plot",
    "text": "Review: What Makes a Good Plot\n\nGood plots tell the story of the data.\n\nGood plots are tailored to the audience and the method of presentation.\n\nGood plots are Honest, Transparent, Clear, and Accessible.\n\nWe’ll explore these concepts in this chapter, with a particular focus on creating honest, transparent, and clear plots, as this is where R offers the most opportunities for customization.",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#review-avoiding-data-viz-time-sinks",
    "href": "book_sections/betteR_plots.html#review-avoiding-data-viz-time-sinks",
    "title": "10. Better Figures in R",
    "section": "Review: Avoiding data viz time sinks",
    "text": "Review: Avoiding data viz time sinks\n\nMaking and critiquing plots is one of my favorite parts of science — I absolutely love it! However, I know it can be a major time sink, and we want to avoid that. We already discussed this once.\n\nHere are my tips for preventing yourself from getting bogged down by every figure:\n\nKnow your goal: Determine whether you’re creating an exploratory or explanatory figure. Don’t waste time perfecting an exploratory plot — it is meant for quick insights, not for publication.\nStandardize your process: Develop a few go-to themes and color schemes that you use frequently. Save and reuse these templates so you can produce attractive plots without customizing each one from scratch.\nMaster the basics: Get comfortable with the most common tasks you’ll perform in ggplot2. Keep the ggplot cheat sheet handy, and bookmark additional resources that suit your workflow.\nPremature optimization is the root of all evil: Save detailed customizations (e.g., annotations, special formatting) for last. This way, you can focus on the essential elements of the plot first without getting bogged down in complex code prematurely.\n\nGet help: Reach out to friends, use Google, consult books, or turn to Generative AI and other resources to solve problems quickly. Remember, the more specific your question, the better the help you’ll receive!",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots.html#whats-ahead-and-how-to-thrive",
    "href": "book_sections/betteR_plots.html#whats-ahead-and-how-to-thrive",
    "title": "10. Better Figures in R",
    "section": "What’s ahead and how to thrive",
    "text": "What’s ahead and how to thrive\nIn this chapter, we’ll tackle the challenge of creating great plots by breaking it down into three key parts.\n\nFirst, in Tools for BetteR Plots, we’ll build a problem-solving toolkit. You’ll learn how to find answers, use help files, and leverage modern resources to help you make the plots you want to make.\nNext, in Making cleaR Plots, we’ll put that toolkit to use in a detailed, step-by-step ‘makeover’ of a messy plot, transforming it into a clear, publication-ready figure.\nFinally, in Plots for the Medium, we’ll learn how to adapt our visualizations for specific contexts like scientific papers, talks, and posters, because a great plot is always tailored to its audience.\n\nAss usual, we conclude with a chapter summary.\n\n\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for visualizing data. https://r-graphics.org/\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org",
    "crumbs": [
      "10. Better Figures in R"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/plotting_tools.html",
    "href": "book_sections/betteR_plots/plotting_tools.html",
    "title": "• 10. Tools for BetteR plots",
    "section": "",
    "text": "Motivating Scenario:\nYou’ve successfully created a basic ggplot, but now you want to make it look good. You know exactly what you want: the legend moved to the bottom, larger axis labels, a custom color palette, etc. but can’t figure out how to do it. Rather than spending hours in pain you ask: Which resources can I leverage to make this plot nice, and how can I use them?\nLearning Goals: By the end of this subchapter, you should be able to:\n\nAdopt a problem-solving mindset for coding by:\n\nFocusing on strategies for finding answers rather than knowing everything.\nIdentifying the part of your code you need.\n\nNavigate and use key resources to find solutions by:\n\nQuickly finding relevant examples in books and blogs.\nReading R’s built-in help() files.\n\nLeverage modern tools to accelerate your workflow by:\n\nUsing graphical tools like the ggThemeAssist RStudio add-in to generate theme code automatically.\nKnowing how and when to effectively google for help.\nUsing Generative AI (e.g., ChatGPT) responsibly to explain concepts, debug code, and suggest solutions.\n\nKnow how and when to ask people for help by preparing a clear, concise question with a minimal reproducible example.\n\n\n\n\nHow to get good at making nice figures\nMaking nice figures can be super fun, and getting good at this is a great skill… but it’s also somewhat “encyclopedic” – more about memorization and knowing your options than creativity. So I think there are two mindsets we can take toward learning how to get good at making plots:\n\n\nThis section is relevant for all coding in all languages This is simply the first time in the book I found it relevant.\n\n🤮 Rote memorization of the “encyclopedic” stuff is boring and frustrating. No one wants to memorize weird ggplot things, and it feels shitty when you don’t know a specific trick.\n🫶 Building a problem-solving toolkit while making nice plots is super empowering.\n\nI therefore focus on the latter strategy – rather than spending time memorizing all the ways to bend ggplot to your will, there are a bunch of resources (below) that help us learn by doing (or at least learn as we are doing). To me, this is the best way to learn.\n\nA key to getting help is figuring out what you actually need help with. Regardless of which tool you use (AI, a friend, Google, or a book) the first step is getting clear on what part you can do, and what part you’re stuck on. That separation makes it way easier to ask a good question, get a useful answer, and move forward.\n\n\n\nNo need to memorize anything: We just need to get good at using the available tools and knowing when to use which. The even better news is that by using these tools regularly and effectively, you’ll actually get better at making good plots on your own!\n\n\nBooks & Blogs\nIn my view, a good book or blogpost is the best way to learn (This is why I am writing this book, after all, and why I include additional resources in each chapter). Authors intentionally and patiently walk through the details of how to do something, and provide concepts and context to understand how and why it works. You don’t need to read any of these cover-to-cover. Think of them more like cookbooks or survival guides: flip to the bit you need (or use the search feature), get the idea, and move on. As noted above, a key is knowing what you need!\nA problem with a book is that it may not have exactly the thing you need right now, and might it not get to the point quickly. In theory if you master the material in a book you are likely to be able to do more complex stuff, but we don’t always have time for that. Sometimes we want the answer fast!\n\nFor more advanced plotting and customization, take a look at these excellent resources:.\n\nThe R Graphics Cookbook (Chang, 2020).\n\nggplot2: Elegant Graphics for Data Analysis (Wickham, 2016).\n\nData Visualization: A Practical Introduction (Healy, 2018).\nModern Data Visualization with R (Kabacoff, 2024).\n\n\n\n\n\nHelpfiles\nThe help() function in R can provide fast information on how to use a specific function. For me helpfiles are incredibly useful, but they take some expertise to use effectively:\n\nFirst you must know the function you need help with. This isn’t always easy, as if you knew the function you might not need help. If you don’t know the function you need, or you can’t make sense of the helpfile, try google!\nSecond, even if you know the function you need help with, helpfiles can be hard to read. Reading helpfiles is actually a skill. See this nice resource to walk you through using a helpfile.\n\nI suggest skimming the helpfile and pay the most attention to\n\nDescription: What the function does.\n\nUsage: How to run the function.\n\nArguments: What you give the function.\n\nExamples: Some examples of using the function.\n\n\nBetween a help file and ChatGPT lies a sweet spot: custom RAGs – language models trained on specific documentation. For example, the top right of https://ggplot2.tidyverse.org has a button labeled “Ask AI”. Clicking it brings you to a language model trained specifically on the help files for dplyr, ggplot2, and tidyr. It gives you answers grounded in the actual docs - without the pain of learning how to read them.\n\n\n\n\n\nThe ggplot website has an embedded RAG trained on dplyr, tidyr, and ggplot2 documentation.\n\n\n\n\nOr paste the help() output into your favorite LLM and ask it to help you understand how to read the helpfile and use the function.\n\n\n\nGoogle and Stackoverflow\nThere is a lot of information on the internet, and Google is your friend. If you don’t know how to do something, try googling it. Often Google searches lead to answers on stackoverflow a question and answer website for computer science.\n\n\nDon’t ask questions on stackoverflow Instead find answers there. Stackoverflow is not a particularly friendly place, they are not gentle with noobs, and get upset when a question you asked could be answered by anything written in the history of stackoverflow.\nGoogling is trickier than it sounds. Like making sense of help, knowing the appropriate search term, separating helpful from unhelpful answers and identifying where the useful information in a webpage is are all skills. These skills require practice and basic knowledge of R. Therefore you will see these tools become more valuable as you get more confident with R.\n\n\n\n\n\n\nGoogle the error message (click to expand)\n\n\n\n\n\nSometimes R doesn’t do what you want and spits out an error message\n\nggplot(aes(x = pull(iris,Sepal.Length), \n           y = pull(iris,Petal.Length))) +\n     geom_point()\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`, not a\n  &lt;uneval&gt; object.\nℹ Did you accidentally pass `aes()` to the `data` argument?\n\n\n\n\n\n\n\nGoogling the error message can help find the answer!\n\n\n\n\n\n\n\n\n\nGUIs\nThe ggThemeAssist package provides a graphical user interface (GUI) that allows you to point and click your way to the desired figure. It then generates the corresponding R code for you (see Figure 1). I learned everything I know about the theme() function (a way to change font size, plot color etc…) from ggThemeAssist.\n\n\nAs of this writing ggplot changed the syntax for placing legends in a plot, so ggThemeAssist’s advice is wrong in that instance.\nTo use ggThemeAssist:\n\nInstall and load the package.\nCreate a ggplot in your R script.\nSelect ggplot Theme Assistant from the add-ins drop-down menu.\nA GUI will appear. Point and click your way through the options, and the corresponding R code will be inserted into your script.\n\n\n\n\n\n\n\n\n\nFigure 1: An example of how to use the ggThemeAssist package from the ggThemeAssist website. Do yourself a favor and use this package.\n\n\n\n\n\n\n\nPeople\nFriends, peers, mentors etc are the most valuable and useful help you can ask for - they often know where you’re coming from, and have had experiences similar to you. The problem is that people have limited bandwidth, limited patience, and don’t know everything. As in all cases in which we seek help, the clearer we can state our goal, and the more we can isolate our challenge the more useful the help we get will be.\n\n\nGenerative AI\n\n\nI wrote this in June 2025 GenAI is rapidly evolving, and ?might? get better/change by the time you read this. We currently don’t know the future impact of LLMs and coding / stats / jobs etc… LLMs are trained on a huge amount of public code and documentation But training doesn’t always reflect the most up-to-date info. In my experience LLMs are particularly bad with obscure R packages. In these cases LLMs often guess plausible but incorrect syntax.\nWhen a friend isn’t available, there’s generative AI (e.g., Claude, ChatGPT, and the like). Unlike friends, GenAI tools have infinite patience and access to way more information. I encourage you to use them to improve your figures when necessary - but I also feel compelled to offer a few warnings and bits of guidance.\n\nGenAI is most useful when you already know what you’re doing.\nI’m pretty good at coding in R. When I ask ChatGPT or Claude for help, it’s amazing. It shows me useful ways to solve problems. When the answer isn’t quite right (and it often isn’t), I can fix it or build from it.\nBy contrast, I know next to nothing about JavaScript. If I ask ChatGPT a JavaScript question and the answer is perfect, I’m in luck. But if it’s even a little off, I end up in a 90-minute debugging session with an LLM—and I’m no closer than when I started.\n\nDon’t trust generative AI to be correct These things are bullshitters. They don’t understand thing. They are written to please us. I could go on about the limitations of LLM’s - but I love them for making plots because it is so easy to see if they worked (does the plot look how I wanted it too) or not.\n\nKnow when to quit (or at least change prompts)\nIf you are going in circles with your favorite LLM about how to make your plot how you want and it’s just not working, take a step back. Think. Is this something GenAI cannot solve? Do I need to change my prompt? Do I need to think differently about the problem? etc.. Don’t waste hours here.\n\nAsking ChatGPT a question and copy-pasting the code it gives you might work. But if you want to actually learn—and avoid painful bugs—try this:\n\nRead the code.\nRead the explanation it gives you.\nRun the code in R and see what happens. PAY ATTENTION. Is this what you wanted?\n\nIf the code doesn’t work as expected, tweak the code to have it do something slightly different. This helps make sure you understand what it’s doing.\n\nIf the code doesn’t work as expected, go back to ChatGPT, books, or Google. Iterate until you understand what’s going on.\n\n\n\nGenAI is really good with error messages When we Googled the error message above, the best answer was the automatic one from generative AI. Googling error messages can work, but often this sends us in the wrong direction because the error message does not light up the right keywords.\n\n\nDo not share any data with a generative AI tool (e.g., ChatGPT, Claude) that you wouldn’t be comfortable posting publicly. Once your data is submitted to an LLM, it’s not uniquely yours anymore. If you’re working with sensitive or private data, do not paste them into a standard chatbot.\n\nUse fake data with the same structure.\n\nUse a built-in R dataset (like penguins, iris, or mtcars) with a similar shape.\n\n\n\n\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for visualizing data. https://r-graphics.org/\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Tools for BetteR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/cleaR_plots.html",
    "href": "book_sections/betteR_plots/cleaR_plots.html",
    "title": "• 10. Making cleaR plots",
    "section": "",
    "text": "Making Clear Plots in R\nIn the previous chapter we discussed that clear plots (1) Have Informative and Readable Labels (2) Minimize cognitive burden, (3) Make points obvious, and (4) Avoid distractions. In this subsection, we focus on how to accomplish these goals in ggplot.\nTo do so, we initially focus on a truly heinous plot, which aims to compare petal area across field sites and subspecies. We can see that Figure 1 is basically unreadable:\nSo, we give it a “makeover” to turn it into a solid explanatory plot.\nLoading and formatting hybrid zone data\nlibrary(stringr)\nhz_pheno_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_hz_phenotypes.csv\"\n\nhz_phenos &lt;- read_csv(hz_pheno_link) |&gt;\n  filter(replicate == \"N\")           |&gt;\n  select(site, ssp =subspecies, prot = avg_protandry, herk = avg_herkogamy, area = avg_petal_area, lat, lon) |&gt;\n  mutate(site_ssp = paste(site, ssp),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\\\\?\",replacement = \" uncertain\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\",replacement = \" xantiana\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" P\",replacement = \" parviflora\"))\nggplot(hz_phenos, aes(x = site_ssp, y = area)) +\n  geom_jitter(width = 1, height =1)\n\n\n\n\n\n\n\nFigure 1: Our starting plot. The x-axis labels are unreadable, and the legend labels are unclear, data points are all over the place.\nAfter improving this plot and considering alternatives, we conclude by introducing a few other data sets to cover additional topics in how to go from a solid exploratory plot to a good explanatory plot!",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Making cleaR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/cleaR_plots.html#making-clear-plots-in-r",
    "href": "book_sections/betteR_plots/cleaR_plots.html#making-clear-plots-in-r",
    "title": "• 10. Making cleaR plots",
    "section": "",
    "text": "We can’t tell which data point is associated with which category.\nThe x-axis labels bump into each other, so we can’t read them anyway.\n\nHow are there negative values for area?\n\nThe meaning of area site_ssp, ssp, P, X, and X? are unclear.\n\nIt’s hard to follow patterns (but there are some bigger things and lower things)!\n\n\n\n\n\n\nEnsuring Labels Are Readable and Informative\n\nStep 1: Making Labels Readable by Flipping Coordinates\nThe first problem to solve is the overlapping text. There are two possible solutions:\n\nFlipping the x and y axes is my favorite solution because so the long labels have room to breathe on the y-axis (Panel: Switch x & y).\n\nRotating the labels on the x-axis is also acceptable, but can be a pain in the neck (Panel: Rotate X label).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSwitch x & yRotate x labels\n\n\nTo learn how to swap x and y axes, let’s start with the code from Figure 1 here.\n\nFirst run the code to make sure it works.\n\nIt should look like Figure 1.\n\nThen switch x and y and see what’s changed.\n\nIt should look like Figure 2.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s the answer if you can’t figure it out.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp)) +\n  geom_jitter(width = 1, height =1)\n\n\n\n\n\n\n\nFigure 2: Our starting plot - now with flipped axes. The legend labels are unclear, data points are all over the place, but now we can read the categories, so that’s something.\n\n\n\n\n\n\n\n\nHere is the alternative solution in which we can rotate the x-axis labels, which we accomplish through the theme function:\n\nggplot(hz_phenos, aes(x =  site_ssp, y = area)) +\n  geom_jitter(width = 1, height =1)+\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 3: Our starting plot - now with rotated x-labels. The legend labels are unclear, data points are all over the place, but now we can read the categories, so that’s something.\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Making Labels Informative by Changing Labels\nSpreadsheets and datasets often use shorthand for column names or categories. Such shorthand can make data analysis more efficient, but makes figures unclear to an outside audience. We could maybe guess that area referred to petal area, and that site_ssp meant the combination of site and species, but that’s not fully clear.\nReplace &lt;ADD A GOOD X LABEL HERE&gt; and &lt;ADD A GOOD Y LABEL HERE&gt; in the labs() function of the code below to make a clearly labelled figure (See my answer in Figure 4).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nYaniv’s code for clearer labels.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", y = \"Site and subspecies combination\")\n\n\n\n\n\n\n\nFigure 4: Our starting plot - now with flipped axesand better labels. The legend labels are unclear, data points are all over the place, but now we can read the categories and know what X and Y mean, so that’s something.\n\n\n\n\n\n\n\n\nStep 3: Picking Colors to Make Labels Informative\nAlthough the Y axis (now) should provide enough information to understand the plot, associating color with a variable can make patterns stick out.\nFigure 5 (in Panel: Default colors) does this by mapping subspecies onto color.\nFigure 6 (in Panel: Color choice + better labels + choose order) takes further control by picking colors ourselves or using a fun and informative color palette.\n\nDefault colorsColor choice + better labels + choose order\n\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")\n\n\n\n\n\n\n\nFigure 5: This plot improves on previous figures by using color to show which data point came from which subspecies.\n\n\n\n\n\n\n\nHere we have taken control of defaults, using scale_color_manual() to rename the categories within the legend.\n\nvalues = c(...) sets the colors for the categories.\n\nbreaks = c(\"X?\", \"X\", \"P\") specifies the original shorthand values from the data and sets the order they should appear in the legend.\n\nlabels = c(\"uncertain\", \"xantiana\", \"parviflora\") provides the new, descriptive labels that correspond to the items listed in breaks.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 1, height =1)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 6: This plot improves on previous figures by using color to show which data point came from which subspecies. Colors are chosen intentionally and default category names are replaced with legible names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing Your ggplot2 Colors\n\n\n\n\n\nThere are many “color palettes” available in R to add some fun to you figures. Check out these options, but be sure to check for accessibility (the colorblindcheck package can help).\n\nRColorBrewer: This option comes with ggplot2. Use scale_fill_brewer() or scale_color_brewer() for a wide range of well-designed sequential, qualitative, and diverging palettes.\nviridis: The most commonly used palette for scientific plots is also built into ggplot2. Its palettes are perceptually uniform and friendly to viewers with color vision deficiency. Use scale_color_viridis_d() (for discrete data) or scale_color_viridis_c() (for continuous data). Change color to fill as necessary.\nThemed & Fun Palettes: Add personality to your plots with packages like wesanderson, or the artistically-inspired MetBrewer. These typically provide a vector of colors to use with scale_color_manual(). See this link for an extensive list of options.\nThe colorspace package: This package is great for creating your own high-quality, color-blind safe custom palettes (based on perceptually-uniform color models).\n\n\n\n\n\n\n\nMaking Patterns Clear\nWe’ve come a long way from Figure 1 – Figure 6 is much improved, and we can now see the xantiana likely has larger petals than parviflora. But it’s still hard to make much sense of these data. Let’s further clarify this plot.\n\nStep 4: Choosing the Appropriate jitter\nA huge problem with this plot are that data points are spread all over the place, because we used the geom_jitter() function. At times jittering points is a good way to prevent over-plotting - but it can be a problem when jittered points change our data or make patterns unclear. In our case jittering introduces both issues:\n\nBecause of the large jitter height, data points aren’t lined up with their category.\n\nBecause of the large jitter width, data points are wrong (notice the negative values for petal area.)\n\nThere are two solutions:\n1. Use geom_point(): I always use geom_point when x, and y are continuous variables. In such cases using jitter actually changes our data, and should be avoided.\n2. Choose appropriate jitter sizes: When an axis is categorical, jittering points along the axis makes sense, but\n\nBe sure that points don’t run across categories (jitter should be small) for the categorical variable.\nBe sure that points aren’t jittered for the axis with the continuous variable.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  geom_jitter(width = 0, height =.25, size=3, slpha = .7)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 7: This plot improves on previous figures by using color to show which data point came from which subspecies. Colors are chosen intentionally and default category names are replaced with legible names. We can now see the true petal area, and unambiguously determine which category a datpoint came from (while avoiding overplotting)\n\n\n\n\n\n\n\nStep 5: Showing Data Summaries\nWe are really getting there! The previous plot shows the raw data clearly, but it’s still hard to precisely estimate the mean petal area for each group or see the uncertainty in that estimate. Summary statistics can guide the reader’s eye and make the main patterns more obvious.\nThe stat_summary() function computes summaries for us and add them to our plot. We’ll explore two common approaches:\n\nAdding bars to show the mean (Panel: Adding a bar).\n\nAdding points and error bars to show the mean and its uncertainty.(Panel: Adding errorbars).\n\n\nAdding a barAdding Errorbars\n\n\nBars allow for effective and rapid estimation of group means, and differences among groups. But adding bars to a plot without care can cover up our raw data. Three tricks to avoid this are:\n\nAdd the stat_summary() layer before geom_jitter(). to ensures the raw data points are plotted on top of the bars.\nMaking bars semi-transparent (via the alpha argument).\n\nMaking the bars a different color than the data points (e.g. fill = \"black\").\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  stat_summary(geom = \"bar\",alpha = .1)+\n  geom_jitter(width = 0, height =.25, size=3, alpha = .7)+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 8: This plot improves on previous figures by adding a bar going from zero to each sample’s mean.\n\n\n\n\n\n\n\nAn alternative to bars is to show the mean and its uncertainty with a point and error bars. Here, we use stat_summary() again, we need to make some additional choices:\n\nWhat the bars should show I usually choose 95% Confidence intervals (more on that in a later chapter) withfun.data = \"mean_cl_normal\".\n\nNOTE: Standard errors,standard deviations, 95% confidence intervals and the like all different, and can be shown with bars. So you must communicate what the bars represent. I usually do this in the figure legend.\n\n\nHow to display the uncertainty I usually choose error bars geom = \"errorbar\" of modest width (width = 0.25), but geom = pointrange can work too.\n\n\nggplot(hz_phenos, aes(x = area, y = site_ssp, color = ssp)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = 0, y=.35))+\n  labs(x = \"Petal area (mm^2)\", \n       y = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))\n\n\n\n\n\n\n\nFigure 9: This plot improves on previous figures by showing both means and 95% confidence intervals for each category.\n\n\n\n\n\n\n\n\n\n\n\nFacilitate Key Comparisons\nWe have previously seen that the way we arrange our data can highlight key comparisons and make trends obvious.\n\nStep 6: Arrange Categories In A Sensible Order\nBy default, R orders categorical variables alphabetically, which is rarely the most insightful arrangement. To make patterns stand out, you should order categories based on a meaningful value. Two such meaningful values are:\n\nThe order of categories If categories are ordinal show them in their natural order. (e.g. Months should go in order). Some things aren’t exactly ordinal but they may have an order that makes trends clear – for example our Clarkia field sites go (roughly) from south to north, so that order makes sense.\n\nThe order of values If categories cannot be sensibly arranged by something about them, it often helps to arrange them by a summary statistic, like the mean or median of the numeric response variable you are plotting. This makes patterns easiest to spot.\n\nWe can achieve either of these aims with functions in the forcats package. This pdf explains all the functions in the package, but most often I use:\n\n\nNOTE There is no connection between the order categories appear in a tibble and the order they are displayed in a plot. Changing the order of factors in a tibble will not change the way they are displayed in the tibble, and reordering observations in a tibble (e.g. with arrange()) will not change their order in a plot.\nLet’s give this a shot in our Clarkia hybrid zone dataset.\n\nFirst, let’s reorder “by hand” with fct_relevel().\nThen, let’s reorder by some value with fct_reorder().\n\nFinally, let’s reorder first by subspecies, and then by latitude with fct_reorder2().\n\n\nOrder “by hand”Order by a variableOrder by two things\n\n\nWe can use fct_relevel() to reorder categories “by hand.”\nBelow, I place \"S22 uncertain\" last (i.e. at the top). I do this by listing all variables in the order I want them. But if you just want to move one variable (as in this case), we can alternatively use the after argument:\n\nTo place it first \"MYVAR\", after = 0\nTo place it last \"MYVAR\", after = Inf\n\nChallenge: Change the code to place \"S22 uncertain\" first (i.e. at the bottom as in Figure 10).\nNote: Due to space considerations, this plot does not include all the best practices from above. Feel free to add them!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s how to put ‘S22 uncertain’ first\n\nTo place S22 uncertain first, use fct_relevel(site_ssp, \"S22 uncertain\", after = 0)\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp placing S22 uncertain first\nhz_phenos &lt;- hz_phenos |&gt;\n    mutate(site_ssp = fct_relevel(site_ssp, \"S22 uncertain\", after = 0))\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area, \n                          y = site_ssp, \n                          color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 10: A plot showing site and subspecies combinations with S22 uncertain last.\n\n\n\n\n\n\n\n\nWe can use fct_reorder() to reorder categories by the area of some variable. Below, I include the code to reoder from smallest to largest petal area. To get better with this approach, try the following challenges:\n\nReorder from biggest to smallest petal area by including .desc = TRUE in fct_reorder().\n\nSolution in Figure 11.\n\n\nReorder from smallest to biggest longitude (lon). .\n\nSolution in Figure 12.\n\n\nNote: Due to space considerations, this plot does not include all the best practices from above. Feel free to add them!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHere’s how to order by Descending Petal Area\n\nTo reorder the categories from the largest mean petal area to the smallest, we use fct_reorder() and set the .desc = TRUE argument. This flips the default ascending order.\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by area, in descending order\nhz_phenos &lt;- hz_phenos |&gt;\n  filter(!is.na(area))|&gt;\n  mutate(site_ssp = fct_reorder(site_ssp, area, .desc = TRUE,.na_rm = TRUE))\n\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area, \n                          y = site_ssp, \n                          color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 11: A plot showing site and subspecies combinations ordered by mean petal area, from largest (bottom) to smallest (top).\n\n\n\n\n\n\n\n\nHere’s how to order from smallest to biggest longitude\n\nTo reorder by longitude, let’s put that variable in!\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by area, in descending order\nhz_phenos &lt;- hz_phenos |&gt;\n  mutate(site_ssp = fct_reorder(site_ssp, lon))\n\n# Plot the reordered data\nggplot(hz_phenos, aes(x = area,\n                      y = site_ssp, \n                      color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) +\n  labs(y = \"Site & Subspecies (Ordered by Area)\", x = \"Petal Area\")\n\n\n\n\n\n\n\nFigure 12: A plot showing site and subspecies combinations ordered by mean longitude, from smallest (bottom) to largest (top).\n\n\n\n\n\n\n\n\nWe can order by more than one thing with fct_reorder2(). Below I order, first by longitude and then by subspecies, but strangely to do so, we type ssp first and then lon.\nChallenge: Change the code order first by subspecies and then by longitude..\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nHere’s how to order by subspecies and then longitude\n\nTo reorder by subspecies and the longitude, try fct_reorder2(site_ssp, lon, ssp).\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\n\n# Reorder site_ssp by subspecies and then by longitude.\nhz_phenos  &lt;- hz_phenos |&gt;\n  mutate(site_ssp = fct_reorder2(site_ssp, lon, ssp))\n\n# PLOT **Don't change this**  \nggplot(hz_phenos, aes(x = area, \n                      y = site_ssp, \n                      color = ssp)) +\n  stat_summary(fun = \"mean\", \n               geom = \"bar\", \n               alpha = 0.2) +\n  geom_jitter(width = 0.0, height = 0.1, \n              size = 3, alpha = 0.7) \n\n\n\n\n\n\n\nFigure 13: A plot showing site and subspecies combinations ordered by subspecies and the mean longitude.\n\n\n\n\n\n\n\n\n\nSummary Improving a Plot\nWe’ve come a long way from that first “heinous” plot! Let’s take a moment to appreciate the journey. We started with a plot that was confusing and basically unreadable. Step-by-step, we identified problems and applied targeted fixes:\n\nWe made labels readable by flipping the axes.\n\nWe made them informative by replacing shorthand with clear names.\n\nWe controlled the jitter to present the data’s position honestly.\n\nWe added summary bars and error bars to guide the reader’s eye to the key patterns.\n\nWe reordered the categories to make the comparison between groups clear and intuitive.\n\nThe big takeaway is that making a great explanatory plot is an iterative process. You don’t have to get it perfect on the first try. The key is to critically look at your plot, identify what’s confusing or unclear, and then use the tools at your disposal to fix it. Our final plot isn’t just “prettier”, it’s more honest, more informative, and a clearer story.\n\n\n\nBonus: Explore Alternative Visualizations\nIt’s always worthwhile to consider alternative visualizations of the same dataset to see which best reveals the key patterns in the data. I usually do this earlier in the figure-making process, but better late than never!\nHere, let’s use “small multiples” - a series of small plots that use the same scales and axes to explore two additional approaches to gaining insight from these data. In my view both of these represent improvements over their analogues in the previous plots because the facets separate the data to clearly highlight specific comparisons of interest.\nOPTION 1 Facet by site\nThe plot below “facets” data by site. I really like Figure 14 because it allows us to visually compare the petal area of different subspecies when they are found at the same site. This makes it easy to see that the difference in petal area between subspecies is largest at “Site 22” and smallest at “Site 6”.\n\nggplot(hz_phenos, aes(x = ssp, y = area, color = ssp)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(y = \"Petal area (mm^2)\", \n       x = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  facet_wrap(~site, nrow = 1, labeller = \"label_both\")+\n  scale_color_manual(values = c(\"yellow\", \"red3\", \"cornflowerblue\"),\n                     breaks = c(\"X?\", \"X\", \"P\"), \n                     labels = c(\"uncertain\", \"xantiana\", \"parviflora\"))+\n  theme(axis.text = element_text(size = 12), \n        axis.title = element_text(size = 12),\n        strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\nFigure 14: A faceted plot showing the petal area of each subspecies, broken down by site. Each panel represents a different field site, allowing for a direct comparison of subspecies within that site. This highlights the differences in petal area between subspecies across sites.\n\n\n\n\n\nOPTION 2 Facet by subspecies\nThe plot below “facets” data by subspecies. I really like Figure 15 because it allows us to visually compare how the petal area for a given subspecies changes across sites. This makes it easy to see that, for example, parviflora plants have their largest petals at Site 6, while xantiana plants have their largest at Site 22 and smallest at Site 6.\n\nggplot(hz_phenos, aes(x = site, y = area, color = site)) +\n  stat_summary(fun = \"mean\", geom = \"bar\", alpha = 0.2) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", \n               color = \"black\", width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(y = \"Petal area (mm^2)\", \n       x = \"Site and subspecies combination\", \n       color = \"subspecies\")+\n  facet_wrap(~ssp, nrow = 1, labeller = \"label_both\")+\n  theme(axis.text = element_text(size = 12), \n        axis.title = element_text(size = 12),\n        strip.text = element_text(size = 12),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigure 15: A faceted plot comparing petal area across sites, with each panel dedicated to a single subspecies. This view makes it easy to assess how the petal area of a specific subspecies changes from one geographic site to another.\n\n\n\n\n\n\n\n\nBONUS: Direct labeling\nSometimes, a legend can feel like a detour for your reader’s eyes. Forcing them to look back and forth between the data and the key adds cognitive load. A great alternative is direct labeling, where you place labels right next to the data they describe.\nThere are two main tools for this in ggplot2:\n\nMethod 1: The “ggplot Way” with geom_label(): This approach uses the same aes() aesthetic mapping you’re already familiar with. You can map variables from your data to the label, x, and y aesthetics. It’s best when the position of your label depends on the data itself (e.g., placing a label at the mean of a group).\n\nIn the example below, we calculate the mean position for each penguin species on the fly and use that to place the labels.\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n    geom_point(alpha = 0.5) +\n    # Add labels using a summarized data frame\n    geom_label(data = penguins |&gt;\n                 group_by(species) |&gt;\n                 summarise_at(c(\"bill_depth_mm\", \"bill_length_mm\"), mean, na.rm = TRUE),\n               aes(label = species), fontface = \"bold\", size = 4, alpha=.6) +\n    # Remove the redundant legend\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 16: A scatter plot of penguin bill dimensions that uses direct labeling. The geom_label() layer calculates the mean position for each species and places the label directly on the plot, making it easier to identify the groups without a legend.\n\n\n\n\n\n\nMethod 2: The “Manual Way” with annotate().\n\nThe annotate() function is for adding “one-off” plot elements. It does not use aesthetic mappings. Instead, you give it the exact coordinates and attributes for the thing you want to add.\nThis gives you precise control over label placement, but it comes at a price: it’s not linked to your data and won’t update automatically. It’s best for adding a single title, an arrow, or manually placing a few labels where the position is fixed. I often choose this at the very last step of making an explanatory plot when there is a specific space I can see is best for such labels.\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n    geom_point(alpha = 0.5) +\n    # Add labels using a summarized data frame\n    annotate(geom = \"label\", label = c(\"Gentoo\", \"Chinstrap\", \"Adelie\"), \n             x = c(14, 18.5,20), y = c(55,55,34), \n             color = c(\"blue\",\"forestgreen\",\"red\"), \n             fontface = \"bold\", size = 5, alpha=.6)+\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 17: This plot demonstrates direct labeling using the annotate() function. This method provides precise control by requiring the user to manually specify the exact coordinates, text, and color for each label, independent of the data mapping.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Making cleaR plots"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/plots_foR_medium.html",
    "href": "book_sections/betteR_plots/plots_foR_medium.html",
    "title": "[• 10. Plots for the medium]{#plots_foR_medium) .quarto-section-identifier}",
    "section": "",
    "text": "Motivating Scenario:\nYou’ve just completed a big analysis and created a clear, honest ggplot figure that perfectly shows your main result. Now you realize you have to give a talk, write a paper, and present a poster. You’re thinking about just using the same plot for each case, but realize that, forexample, no one will be able to read your axes labels if you present your plots in a talk. So now you want to customize your plot for the medium of presentation.\n\nLearning Goals: By the end of this subchapter, you should be able to:\n\nAdopt a strategic mindset for data visualization by recognizing that the “best” plot depends on its context and tailoring your design to the specific demands of the medium (e.g., talk, poster, paper).\nPrepare publication-quality figures for scientific papers by:\n\nCombining multiple plots into a single, cohesive multi-panel figure using patchwork.\nApplying clean, professional themes like theme_bw() or theme_classic().\nFormatting labels with mathematical symbols or italics using expression() and/or ggtext.\n\nDesign effective plots for live presentations like talks and posters by:\n\nDramatically increasing text size for readability from a distance using theme().\nUsing attention-grabbing visuals (like icons or isotype plots) to stand out in a crowded poster hall.\n\nEnhance plots for digital mediums by:\n\nMaking plots interactive with packages like plotly and highcharter.\nDesigning clear, compelling infographic components that tell a single, powerful story.\n\nKnow when to be pragmatic by deciding between a pure-R solution and using graphics software for final, one-off annotations and design touches.\n\n\n\nTailoring Plots for Your Medium\nThere are many ways to tell a story. You could share it informally with a friend, write it in a book, produce it as a play, or perform it live on stage (like on The Moth podcast). Even if the core story is the same, you would tailor your approach for each medium.\nThe same is true when presenting data visually. We’ve already discussed that a good plot is designed for its specific audience and purpose. You might present your findings in a scientific talk, on a poster, in a manuscript for a peer-reviewed journal, a digital document, or perhaps “clickbait” or an infographic for a broader audience. Each of these mediums has different demands.\nHere, we’ll go over how to customize your plots in R for a few of these common formats.\n\nMake your life easier by knowing when you can skip doing it in R.\nR is great because it is reproducible and dependable. What’s more, once you can do something in R, it’s pretty straightforward to apply that skill to a new dataset. This comes in handy when you’ll be doing the same task a bunch. But figuring out how to do a very specific, one-off tweak in R can be a huge time sink, and sometimes it makes more sense to make final touches in powerpoint, illustrator, or photoshop than to figure it our in R. Before you spend hours on a minor customization, ask yourself a few questions. Here is the guidance I use when making this decision:\n\nFrequency: Will I do this sort of thing more than a handful of times? If so, I try to figure it out in R. If it’s a one-off task, I often don’t.\n\nTime Investment: How long will this take to figure out in R versus doing it “by hand” in e.g. PowerPoint? If a manual edit in PowerPoint takes five minutes, that’s often smarter than spending two hours wrestling with code for a minor annotation.\nMotivation: Why do I want to do this in R? Is it for convenience, utility, and/or growth, or am I motivated by pride? Don’t do it for pride!\nMedium of Presentation: What is my final output? There is an expectation that figures for published papers are highly reproducible, so I use R exclusively for such figures. But for posters and talks, I am more likely to do a final touch-up “by hand.”\n\n\n\n\nPlots for a scientific talk\n\nUnfolding the narrative\nWe previously saw that talks offer a great opportunity to walk your audience through a plot as you build it up one step at a time. We also saw that as you do so you can provide the audience with plausible alternative outcomes and how we would interpret them biologically. You can achieve this by working through your ggplot code slowly (e.g. initially exclude geom_point() etc) and using annotate() or even Powerpoint to add alternative outcomes.\n\n\nMaking things BIG (literally)\n\n\nLoading and formatting hybrid zone data\nlibrary(dplyr)\nlibrary(stringr)\nhz_pheno_link &lt;- \"https://raw.githubusercontent.com/ybrandvain/datasets/refs/heads/master/clarkia_hz_phenotypes.csv\"\n\nhz_phenos &lt;- read_csv(hz_pheno_link) |&gt;\n  filter(replicate == \"N\", !is.na(avg_petal_area))           |&gt;\n  select(site, ssp =subspecies, prot = avg_protandry, herk = avg_herkogamy, area = avg_petal_area,lat, lon) |&gt;\n  mutate(site_ssp = paste(site, ssp),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\\\\?\",replacement = \" uncertain\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" X\",replacement = \" xantiana\"),\n         site_ssp = str_replace(string = site_ssp , pattern = \" P\",replacement = \" parviflora\"))\n\n\ninitial_plot &lt;- hz_phenos |&gt; \n  filter(ssp != \"X?\")      |&gt;\n  mutate(ssp = case_when(ssp == \"P\"~ \"parviflora\",\n                         ssp == \"X\"~ \"xantiana\"))|&gt;\n  ggplot(aes(x = site, y = area, color = site)) +\n  geom_jitter(width = 0.1, height = 0.0, size = 3, alpha = 0.7) +\n  stat_summary(fun.data = \"mean_cl_normal\", \n               geom = \"errorbar\", \n               width = 0.25, \n               position = position_nudge(x = .35, y=0))+\n  labs(x = \"Site\", \n       y = \"Petal area (mm^2)\",\n       color = \"subspecies\")+\n  facet_wrap(~ssp, nrow = 1)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPlots in scientific talks require large text! We achieve this largely through the theme() function where we use element_text() to customize a specific feature of our plot.\nBelow is an interactive R session to get you started on this!\n\nFirst, run the code as is to see the initial_plot with its default text sizes.\nNext, copy and paste the code from the margin on the right into the session to see how theme() can change the text size.\nChallenge: You will notice the y-axis tick mark labels are still small. Add a line of code inside theme() to make the axis.text.y larger as well.\nFinally, feel free to add any other elaborations / explorations.\n\n\n\ninitial_plot +\n   theme_linedraw()+\n  theme(axis.text.x =  element_text(size = 18), \n        axis.title.x = element_text(size = 25),\n        axis.title.y = element_text(size = 25),\n        strip.text = element_text(size = 25),\n        legend.position = \"none\")\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nRemember ggthemeassist can really help with these tasks, and is a great way to learn!\n\nggplot2 has several built-in themes (like theme_linedraw() I added above) built into ggplot2. Read more here. Additional packages like ggthemes or hrbrthemes have even more options.\nAs we did above, we can customize these themes - but be sure to do this after running the theme. For example, the code below makes a large x-axis title with theme_linedraw.\n\ninitial_plot + \n  theme_linedraw() +\n  theme(axis.title.x = element_text(size = 25))\n\nBy contrast, this next bit of code makes a standard x-axis title size with theme_linedraw.\n\ninitial_plot +\n  theme(axis.title.x = element_text(size = 25) ) + \n  theme_linedraw()\n\nRevisit the code above to see this for yourself.\n\n\n\n\nPlots for a poster presentation\nLike a plot for a talk, a plot for a poster must be readable from a distance. In fact, for a poster, this is even more critical.\n\nText should be huge\n\nUnlike a talk, a scientific poster does not get you a “captive audience.” Most poster presentations take place in large, crowded halls. So, I recommend having something about your poster that makes passersby want to look more closely. To achieve this, I often tend to break some of the standard plotting rules. For example, some carefully chosen “distractions” or “chartjunk” can get you attention. If you can do this while keeping your plot honest and interpretable, you’re in great shape.\n\nBelow I show how you can add images as data points with the ggimage package.\nIn the previous edition of this book, I showed how you could make bargraphs with pictures (so called “isotype plots”) with the ggtextures package. Read my previous text here.\n\n\n\nUsing ggimage\nlibrary(ggimage)\n\nplot_data &lt;- hz_phenos |&gt; \n    filter(ssp != \"X?\")   \n\nmean_area &lt;- plot_data |&gt;\n    group_by(site,ssp)|&gt;\n    summarise(area= mean(area))\n\n\nggplot(plot_data,aes(x = ssp, y = area)) +\n    geom_image( \n      data  = mean_area,\n      image = \"https://github.com/ybrandvain/datasets/blob/master/clarkia_petal.png?raw=true\",\n      size  = .1) +\n    geom_jitter( aes(color = ssp), size = 2, alpha = .2, width = .4, height = 0)+\n    facet_wrap(~site, ncol=4)+\n    scale_y_continuous(limits = c(0,2.5))+\n    stat_summary(aes(color = ssp),\n                 fun.data = \"mean_cl_normal\", \n                 geom = \"errorbar\", \n                 width = 0.25, \n                 position = position_nudge(x = .5, y=0))+\n  labs(x = \"Subspecies\", \n       y = \"Petal area (mm^2)\",\n       color = \"subspecies\")+\n  theme_test()+\n  theme(axis.text.x =  element_text(size = 18), \n        axis.title.x = element_text(size = 25),\n        axis.title.y = element_text(size = 25),\n        strip.text = element_text(size = 25),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nDigital documents\nFor online documents. Digital formats open up opportunities to engage readers with interactive graphs and animations, making data visualization more dynamic and accessible. Here are some powerful tools to consider:\n\nThe Shiny package allows you to build interactive web applications that let users update graphs, tables, and other visual outputs in real-time, facilitating exploratory data analysis and the creation of interactive dashboards.\ngganimate enables the creation of animations, bringing your data to life and capturing your audience’s attention more effectively. I used this package to make all of the gifs throughout this book.\nOther packages, such as plotly, ggiraph, rbokeh, and highcharter, offer additional capabilities for creating interactive and visually engaging graphs. These tools make it easy for users to explore your data in a more hands-on way. For example, Figure Figure 1 shows an interactive plot that focuses on a specific penguin species when you hover over its points.\n\nFor more information, check out the Interactive Graphs chapter in Modern Data Visualization with R (Kabacoff, 2024). For deeper exploration, see the book Interactive Web-Based Data Visualization with R, plotly, and shiny (Sievert, 2020).\n\nlibrary(highcharter)\n\nhchart(penguins, \"scatter\", hcaes(x = flipper_length_mm, \n                                  y = bill_length_mm, \n                                  group = species, \n                                  color = species))\n\n\n\n\n\n\n\nFigure 1: Example interactive plot using highcharter\n\n\n\n\n\n\nFor Scientific Papers: Polishing for Publication\nScientific papers have specific conventions, and your plots should meet them to look professional and be clearly understood. As discussed in the previous section (read the bit on Brown M&Ms) scientists often judge a paper based on features of figures. So be sure to invest time in cleaning up the theme, precisely formatting labels with italics or mathematical symbols, and combining several plots into a single, multi-panel figure.\n\n\nCombining Plots into a Multi-Panel Figure\nOften, a single figure in a paper needs to tell a complex story by showing multiple views of the data. The patchwork package is the best tool for this job. It lets you combine separate ggplot objects into a single, publication-ready figure with panel labels (A, B, C, etc.).\nIn Figure 2 we create three different plots from our Clarkia hybrid zone data and then combine them into one figure. Doing so, I highlight some tricks of patchwork:\n\nThe + sign combines plots right to left (but sometimes you need to give it further directions).\n\nThe / sign combines plots up and down (but sometimes you need to give it further directions).\n\nplot_layout(guides = \"collect\") combines like legends to minimize redundancy and prevent wasted space.\n\nplot_annotation(tag_levels = 'A') adds letters to plots.\n\nRead the documentation for more information.\n\n\nMaking mutli-panel plots with patchwork\n# Create plot 1: A density plot of petal area\np1 &lt;- ggplot(hz_phenos, aes(x = herk, fill = ssp)) +\n  geom_density(alpha = 0.7) +\n  theme_bw() +\n  labs(subtitle = \"Distribution of Herkogamy\",  x =  \"Herkogamy\")\n\n# Create plot 2: A density plot of protandry\np2 &lt;- ggplot(hz_phenos, aes(x = prot, fill = ssp)) +\n  geom_density(alpha = 0.7) +\n  theme_bw() +\n  labs(subtitle = \"Distribution of Protandry\", x = \"Protandry\")\n\n# Create plot 3: A scatter plot of two traits\np3 &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7, size = 3, show.legend = FALSE) +\n  theme_bw() +\n  labs(subtitle = \"Trait Correlation\", x = \"Protandry\", y = \"Herkogamy\")\n\n\n\n# Combine them with patchwork\n(p1 + p2)/p3 +\n  plot_layout(guides = \"collect\") + # Collect legends into one\n  plot_annotation(tag_levels = 'A')   # Add \"A\", \"B\", and \"C\" panel labels\n\n\n\n\n\n\n\n\nFigure 2: A multi-panel figure created with the patchwork package to provide a comprehensive view of two floral traits in Clarkia subspecies. (A) A density plot showing the distribution of Herkogamy, revealing that subspecies ‘P’ (red) has much lower values than ‘X’ (green). (B) A similar density plot for Protandry. (C) A scatter plot revealing the positive correlation between the two traits. This combined view effectively shows both the individual trait distributions and their relationship in a single figure.\n\n\n\n\n\n\n\n\nAdding Mathematical and Styled Text\nScientific papers require precision in labels. This can mean adding mathematical symbols for units (like mm²) or using italics for species names, as is standard in biology. These goals can be reached in more than one way. Two standard approaches are:\n\nUsing the ggtext package which allows you to write in html. OR\n\nUsing the expression() function, which is more “classic R” but a bit of a pain.\n\n\n\nhtml help I don’t know how to write greek letter or whatever in html. Luckily you can consult these resources (for html math symbols and html for Greek letters ) or ask a chatbot to help!\nUnfold the code below to see the ggtext approach I used to make Figure 3. Note however, that you will likely run into code doing it a different way elsewhere.\n\n\nItalics and math\n# You may need to install ggtext: install.packages(\"ggtext\")\nlibrary(ggtext)\n\n# First, create proper species names for the labels\nhz_phenos_formatted &lt;- hz_phenos |&gt;\n  mutate(ssp_name = case_when(\n    ssp == \"P\" ~ \"*C. x. parviflora*\", # Use Markdown for italics\n    ssp == \"X\" ~ \"*C. x. xantiana*\",\n    ssp == \"X?\" ~ \"Uncertain\")) |&gt;\n  filter(ssp_name != \"Uncertain\")\n\nggplot(hz_phenos_formatted, aes(x = area, fill = ssp_name)) +\n  geom_histogram(bins = 15, color = \"white\", alpha = 0.8) +\n  facet_wrap(~ssp_name) +\n  theme_linedraw() +\n  labs(y = \"Frequency\",\n       x = \"Petal Area  (in mm&lt;sup&gt;2&lt;/sup&gt;)\",\n       fill = \"Subspecies\") +\n  theme(strip.text = element_markdown(size =18), # Use element_markdown() to render italics\n        legend.text = element_markdown(),# Use element_markdown() to render italics\n        axis.title = element_markdown(size = 15),\n        legend.position = \"bottom\")  \n\n\n\n\n\n\n\n\nFigure 3: Faceted histograms comparing the distribution of petal area for two subspecies. This plot italicizes subspecies names and uses a super-script to show that a value is squared.\n\n\n\n\n\n\n\n\nUsing Publication-Ready Themes\nThe default ggplot2 theme with its grey background is great for data exploration, but it’s not the standard for formal publications. Part of this goes back to the “brown M&M thing” - readers trust authors who make conscious decisions rather than defaulting to pre-set options. So, a standard step in making “publication-ready” plot is to apply a cleaner, simpler theme. As shown in Figure 4, functions like theme_bw(), theme_classic(), or theme_minimal() can help! (see above).\n\n\nUsing publication style themes\nlibrary(ggplot2)\nlibrary(patchwork) \n\n# A basic plot with the default theme\np_default &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Default ggplot2 Theme\")+\n  theme(legend.position = \"bottom\")\n\n# The same plot with theme_classic()\np_classic &lt;- ggplot(hz_phenos, aes(x = prot, y = herk, color = ssp)) +\n  geom_point(alpha = 0.7) +\n  theme_classic() + # Apply the new theme\n  labs(title = \"theme_classic()\")+\n  theme(legend.position = \"bottom\")\n\n# Display side-by-side for comparison\np_default + p_classic\n\n\n\n\n\n\n\n\nFigure 4: A comparison of ggplot2 themes for a scatter plot of Clarkia traits. (Left) The default theme_grey() has a grey background and grid lines, excellent for data exploration. (Right) Applying theme_classic() removes the background and grid, creating a cleaner look often preferred for scientific publications.\n\n\n\n\n\n\nBe sure to use a consistent theme throughout the manuscript.\n\n\n\n\n\nFrom Chart to Infographic: Telling a Visual Story\nSo far, we have focused on making excellent charts for scientific communication. But what if you need to explain your findings to the public, on a website, or in a report for a general audience? That’s when you move from a chart to an infographic.\nWhile a scientific plot must show the full, transparent distribution of data, an infographic often simplifies the view to make a single message more powerful and clear. An infographic is much like poster presentation but with fewer details – it should grab attention and make a clear point and assume the audience trusts us. This means shifting your priorities from showing comprehensive data to communicating one single, powerful message. While we don’t lie with infographics, our goal is to make the point clear, and not worry as much about transparency.\nLet’s take our Clarkia hybrid zone data and create one component for an infographic. Our story is simple: one subspecies has a larger petal area than the other. Notice how the code below differs from our previous plots. We summarize the data before plotting, remove all axes, and use a bold title to state the main finding directly. Figure 5 is designed to be a self-contained story.\n\n\nMaking an infographic\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales) # For number() formatting\nlibrary(ggtext)\n# 1. Summarize the data to get the key takeaway\nsummary_data &lt;- hz_phenos |&gt;\n  filter(ssp %in% c(\"P\", \"X\")) |&gt;\n  group_by(ssp,site) |&gt;\n  summarise(mean_area = mean(area, na.rm = TRUE)) |&gt;\n  mutate(ssp_name = if_else(ssp == \"P\", \"parviflora\", \"xantiana\"))\n\n# 2. Build the infographic-style plot\nggplot(summary_data, aes(x = ssp_name, y = mean_area, fill = ssp_name, size = mean_area)) +\n    geom_col(width = 0.6, show.legend = FALSE) +\n    # Add direct labels with formatted numbers\n    geom_text(\n        y=0,\n        aes(label = number(mean_area, accuracy = 0.01)),\n        vjust = -0.2, # Nudge text just above the bar\n      #  size = 7,\n        fontface = \"bold\",\n      show.legend = FALSE\n    ) +\n  scale_size_continuous(range = c(2, 15))+\n    # Use a deliberate, simple color palette\n  scale_fill_manual(values = c(\"parviflora\" = \"gray60\", \"xantiana\" = \"#56B4E9\"))+\n  scale_x_discrete(labels =  c(\n  parviflora= \"&lt;br&gt;parviflora&lt;br&gt;&lt;br&gt;&lt;img src='https://www.calflora.org/app/up/entry/57/17193.jpg'\n    width='100' /&gt;\",\n  xantiana = \"&lt;br&gt;xantiana&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;img src='https://www.calflora.org/app/up/io/132/io39763-0.jpg'\n    width='100' /&gt;&lt;br&gt;\")) +\n    # Add a strong title and subtitle that tell the story\n    theme_void() +\n    labs(\n        title = \"Clarkia xantiana Has Larger Petals\",\n        subtitle = \"Average petal area for two Clarkia subspecies across all sites.\",\n        x = NULL, y = \"petal area\" # We don't need axis titles\n    ) +\n    # Start with a completely blank theme and add back only what we need\n    theme(\n        plot.title = element_text(size = 22, face = \"bold\", hjust = 0.5, margin = margin(b = 10)),\n        plot.subtitle = element_text(size = 18, hjust = 0.5, margin = margin(b = 20)),\n        axis.text.x = element_markdown(size = 18, face = \"bold\"),\n        axis.title.y = element_text(size = 22, face = \"bold\",angle = 90),\n        strip.text =  element_text(size = 18, angle = 270),\n        # Add a buffer around the plot\n        plot.margin = margin(15, 15, 15, 15),\n        panel.border = element_rect(color = \"pink\", fill = NA, size = 1))+\n    facet_grid(site~., labeller = \"label_both\")\n\n\n\n\n\n\n\n\nFigure 5: An example of an infographic component created from the Clarkia dataset. The plot uses horizontal bars and direct labels to clearly show that xantiana has a larger average petal area than parviflora at each of the four field sites. Including photographs of the flowers makes the comparison more tangible and engaging for a general audience.\n\n\n\n\n\n\n\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC Press.\n\n\nSievert, C. (2020). Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Plots for the medium"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html",
    "title": "• 10. Better ggplots summary",
    "section": "",
    "text": "Chapter summary\nLinks to: Summary. Chatbot tutor. Questions. Glossary. R functions. R packages. More resources.\nPolishing a ggplot plot is not about running a single command, but is an iterative process of refinement, moving from a default chart to a polished, explanatory figure.\nWe do this by layering components. We start with a basic plot and add functions to clarify labels (with the labs() function and the ggtext package) and control aesthetics like color and shape (with scale_*_manual()). We then adjust non-data elements with the theme() function, arrange categories logically with the forcats package, and combine plots into larger narratives with patchwork.\nThroughout this process, we also make sure to consider the presentation format, tailoring our choices for each specific medium. A working understaing of ggplot is essential, but luckily you don’t need to have all this memorized—knowing how to use books, friends, chatbots, GUIs like ggThemeAssist, and other resources can help!",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_chapter-summary",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_chapter-summary",
    "title": "• 10. Better ggplots summary",
    "section": "",
    "text": "Chatbot tutor\nPlease interact with this custom chatbot (link here) I have made to help you with this chapter. I suggest interacting with at least ten back-and-forths to ramp up and then stopping when you feel like you got what you needed from it.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_practice-questions",
    "href": "book_sections/betteR_plots/betteR_plots_summary.html#better_plots_summary_practice-questions",
    "title": "• 10. Better ggplots summary",
    "section": "Practice Questions",
    "text": "Practice Questions\nThe following questions will walk you through the iterative process of refining a plot, from a messy default to a polished, clear visualization.\nTo start, let’s create a basic plot from the palmerpenguins dataset. It shows the distribution of flipper lengths for each species. As you can see, it has several problems we need to fix!\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\n# The base plot we will improve upon\nbase_plot &lt;- ggplot(penguins, aes(x = species, y = flipper_length_mm)) +\n  geom_point()\n\nbase_plot\n\n\n\n\n\n\n\nFigure 1: Our starting point: a messy plot with overlapping points.\n\n\n\n\n\n\nQ1) The plot above suffers from severe overplotting, making it hard to see the distribution of points. Which geom_* function is specifically designed to fix this by adding a small amount of random noise to the points’ positions? geom_jitter()geom_point(position = ‘dodge’)geom_smooth()geom_bin2d()\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQ2) Let’s fix the overplotting. In the R chunk below, replace geom_point() with the correct function from the previous question. To keep the data honest, make sure all y-values stay the same, and that x values are clearly associated with a category.\nAfter running the corrected code, what is the best description of the resulting plot?\n\n The points form a single, undifferentiated cloud. The plot now shows three horizontal bars instead of points. The points for each species are now spread out horizontally in distinct vertical columns. The plot is unchanged from the original.\n\n\n\nHint\n\nIn the geom_*_ Set the height argument to 0 and providing a small width (e.g., 0.2)\n\n\nQ3) Great! Now look at the x-axis. ggplot2 defaults to alphabetical order (Adelie, Chinstrap, Gentoo). Use fct_reorder() from the forcats package to reorder the species factor from largest to smallest median flipper length. Now which species now appears first (leftmost) on the x-axis? AdelieChinstrapGentooThe order remains alphabetical\n\n\nHint 1\n\nAdd a mutate() call before ggplot()\n\n\n\nHint 2\n\nAdd a .na_rm =TRUE to ignore NA values and .desc = TRUE to go from greatest to smallest.\n\n\n\nCode\n\n\nlibrary(forcats)\n\npenguins |&gt;\n  mutate(species = fct_reorder(species, flipper_length_mm, median, .na_rm =TRUE, .desc = TRUE))|&gt; \n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_jitter(width = 0.2, height = 0)\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQ4) Our plot is now well-organized, but the labels are not publication-ready. Add a labs() layer to the good_start plot above to achieve the following:\n\nSet the title to “Penguin Flipper Lengths”\n\nSet the x axis label to “Species”\n\nSet the y axis label to “Flipper Length (mm)”\n\nThe labs() function is used to change titles and axis labels. If you also wanted to change the title of the color legend, which argument would you add inside labs()? legend.title = ‘My Title’color = ‘My Title’fill = ‘My Title’legend = ‘My Title’\n\nQ5) Now, imagine you need to put this plot on a slide for a presentation. The text is far too small. Add a theme() layer to the code below to make the axis titles (axis.title) size 20.\nInside theme(), the element_text() function has many arguments besides size. Which argument would you use to change the font from normal to bold? style = ‘bold’font = ‘bold’face = ‘bold’bold = TRUE\n\nQ6) Now you want the species names (e.g. Gentoo, Chinstrap, etc) and flipper lengths (e.g. 170, 210, etc.) to be large (size = 16). What would you add to the theme to do this?\n\n axis.text(size = 16) axis.text = element_text(size = 16) axis.names = element_text(size = 16) axis.ticks = element_text(size = 16)\n\n\nQ7) Now you want the species names (e.g. Gentoo, Chinstrap, etc) but not flipper lengths (e.g. 170, 210, etc.) to be italicized. What would you add to the theme to do this? (NOTE we did this a different ay in the chapter)\n\n element_text(size = 16, face = 'italic') axis.text.x = element_text(size = 16, face = 'italis'), axis.text.y = element_text(size = 16)\n\n\nQ8) The plot above shows the raw data well. Now, let’s add a summary statistic. In the webr chunk below, add a stat_summary() layer to display the mean value for each species as a large, black point (size = 5, color = \"black\"). Hint: You’ll need to specify fun = \"mean\" and geom = \"point\" inside stat_summary().\nAfter successfully adding the summary layer, what new visual element appears on your plot?\n\n A single large black point for each species, located at its mean value. A black horizontal line showing the mean for each species. All the jittered points for each species turn black. A black bar appears behind the points for each species.\n\n\nQ9) Faceting is a powerful way to create “small multiples.” In the chunk above, add a facet_wrap() layer to the scatter plot to create separate panels for each island.\nAfter adding the facet layer correctly, which Islands have Chinstrap penguins? (select all correct) BiscoeDreamTorgersennone of them\n\n\n\n📊 Glossary of Terms\n\n\n🎨 1. Core Visualization Concepts\n\nAesthetic Mapping: The process of connecting variables in the data to visual properties (aesthetics) of the plot, such as x/y position, color, shape, or size.\nGeom (Geometric Object): The visual shape used to represent data, such as a point, a bar, or a line.\nLayering: The process of building a plot by starting with a base and sequentially adding new visual elements on top of each other.\nCognitive Load: The mental effort required to interpret a plot. A well-designed figure reduces cognitive load by being clear and intuitive (e.g., using direct labels instead of a legend), allowing the reader to focus their brainpower on the data’s story, not on trying to figure out the plot itself.\n\n\n\n\n📊 2. Representing and Arranging Data\n\nJittering: Adding a small amount of random noise to the position of data points to prevent them from overlapping perfectly, making it easier to see the distribution of dense data.\nData Summary: A statistical value (like a mean or confidence interval) calculated from raw data and added to a plot to help guide the reader’s eye to a key pattern.\nCategorical Ordering: The deliberate arrangement of categorical data on an axis in a logical way (e.g., by size, by geographic location) to make trends more obvious, rather than using the default alphabetical order.\nRedundant Coding: The practice of mapping a single variable to multiple aesthetics (e.g., mapping a category to both color and shape). This improves clarity and accessibility.\n\n\n\n\n📝 3. Annotation and Polishing\n\nDirect Labeling: The practice of placing text labels directly on or next to data elements, rather than in a separate legend, to make a plot easier to read.\nMathematical Notation: The use of specially formatted text in labels to represent mathematical symbols, such as superscripts (for units like mm²), subscripts, or Greek letters.\nPlot Theme: The collection of all non-data elements of a plot that control its overall look and feel, such as the background color, grid lines, and font styles.\nColor Palette: A set of colors chosen to represent data in a plot. Palettes can be chosen for clarity (qualitative), to show a gradient (sequential), or for aesthetic style.\n\n\n\n\n🖼️ 4. Advanced Figure Types\n\nSmall Multiples (Faceting): A series of small plots that use the same scales and axes, with each plot showing a different subset of the data. This technique is used to make comparisons across groups.\nMulti-Panel Figure: A single figure that combines several individual plots into a larger, organized layout, often used in scientific papers to tell a complex story in a compact space.\nIsotype Plot: A type of chart that uses a repeated icon or image to represent quantity, often used in posters and infographics to be more engaging than a standard bar chart.\nInfographic: A visual representation of information that blends data visualization, graphic design, and text to tell a compelling narrative, typically for a general audience.\nInteractive Plot: A plot, typically for a digital medium, that allows the user to engage with the data by hovering, clicking, zooming, or filtering to explore the data themselves.\n\n\n\n\n✨ 5. Principles & Philosophy\n\nThe “Brown M&M” Principle: A reference to the band Van Halen, who used a “no brown M&Ms” clause in their contract as a quick test for attention to detail. In data visualization, it refers to a small flaw in a plot (like a typo or misalignment) that signals a potential lack of care in the more critical underlying analysis, eroding audience trust.\n\n\n\n\n\n\nKey R Functions\n*All functions are in the ggplot2 package unless otherwise stated.\n\n\ngeom_image() ([ggimage]): Adds images to a plot, often used to represent data points or summaries.\n\n\n📝 1. Labels & Annotations\n\nlabs(): The primary function for setting the plot’s title, subtitle, caption, and the labels for each axis and legend.\ngeom_label() / geom_text(): Adds text-based labels to a plot, mapping data variables to the label aesthetic. geom_label() adds a background box to the text.\nannotate(): Adds a single, “one-off” annotation (like a piece of text or a rectangle) to a plot at specific, manually-defined coordinates.\nelement_markdown() In the ggtext package: Used inside theme() to render plot text (like axis labels or facet titles) that contains Markdown or HTML for styling.\n\n\n\n\n🧮 2. Summaries & Ordering\n\nstat_summary(): Calculates summary statistics (like means or confidence intervals) on the fly and adds them to the plot as a new layer (e.g., as bars or errorbars). This in gplot2, but required the Hmisc package.\nfct_reorder() In the forcats package: Reorders the levels of a categorical variable (a factor) based on a summary of another variable (e.g., order sites by their mean petal area).\nfct_relevel() In the forcats package: Reorders the levels of a factor “by hand” into a specific, manually-defined order.\n\n\n\n\n🎨 3. Controlling Aesthetics (Colors, Shapes, etc.)\n\nscale_color_manual() / scale_fill_manual(): Manually sets the specific colors or fill colors for each level of a categorical variable.\nscale_color_brewer() / scale_fill_brewer(): Applies pre-made, high-quality color palettes from the RColorBrewer package.\nscale_color_viridis_d() / scale_color_viridis_c(): Applies perceptually uniform and colorblind-friendly palettes from viridis. Use _d for discrete data and _c for continuous data.\n\n\n\n\n🖼️ 4. Arranging & Combining Plots\n\nfacet_wrap(): Creates “small multiples” by splitting a plot into a series of panels based on the levels of a categorical variable.\nplot_annotation() In the forcats package: Adds overall titles and panel tags (e.g., A, B, C) to a combined figure.\nplot_layout() In the forcats package: Controls the layout of a combined figure, such as collecting all legends into a single area.\n\n\n\n\n⚙️ 5. Theming & Final Touches\n\ntheme(): The master function for modifying all non-data elements of the plot, such as backgrounds, grid lines, and text styles.\nelement_text(): Used inside theme() to specify the properties of text elements, like size, color, and face (e.g., “bold”).\ntheme_classic() / theme_bw(): Applies a complete, pre-made theme to a plot with a single command, often for a cleaner, publication-ready look.\n\n\n\n\n\n\nR Packages Introduced\n\n\n📦 Core Tidyverse & Plotting.\n\nggplot2: The core package we use for all plotting, based on the Grammar of Graphics.\ndplyr: Used for data manipulation and wrangling, like filter() and mutate().\nforcats: The essential tool for working with categorical variables (factors), especially for reordering them in a logical way.\n\n\n\n\n🖼️ Arranging, Annotating & Polishing Plots.\n\npatchwork: A tool for combining separate ggplot objects into a single, multi-panel figure.\nggtext: A powerful package that allows you to use rich text formatting (like Markdown and HTML) in plot labels for effects like italics and custom colors.\nHmisc: A general-purpose package that contains many useful functions, including mean_cl_normal for calculating confidence intervals in stat_summary().\nscales: Provides tools for controlling the formatting of numbers and labels on plot axes and legends.\n\n\n\n\n✨ AFlair (Images, Animations & Themes)\n\nggimage: Used to add images as data points or layers in your plots.\nggtextures: Allows you to create isotype plots (bar graphs made of images).\ngganimate: Brings your static ggplots to life by creating animations.\n[ggthemes]https://github.com/jrnold/ggthemes): Provides a collection of additional plot themes, including styles from publications like The Economist and The Wall Street Journal.\n\n\n\n\n🖱️ Interactivity\n\nplotly: A powerful package for creating interactive web-based graphics. Its ggplotly() function can make almost any ggplot interactive with one line of code.\nhighcharter: Another popular and powerful package for creating a wide variety of interactive charts.\nShiny: R’s framework for building full, interactive web applications and dashboards directly from your R code.\n\n\n\n\n🎨 Themed Color Palettes\n\nMetBrewer: Provides beautiful and accessible color palettes inspired by artworks from the Metropolitan Museum of Art.\nwesanderson: A fun package that provides color palettes inspired by the films of director Wes Anderson.\n\n\n\n\n🛠️ Helper Tools\n\nggThemeAssist: An RStudio add-in that provides a graphical user interface (GUI) for editing theme() elements, helping you learn how to customize your plot’s appearance.\n\n\n\n\n\nAdditional Resources\n\nInteractive Cookbooks & Galleries:\n\nThe R Graph Gallery: An extensive, searchable gallery of almost every chart type imaginable, created with ggplot2 and other R tools. Each example comes with the full, reproducible R code.\nFrom Data to Viz: A fantastic tool that helps you choose the right type of plot for your data. It provides a decision tree that leads to ggplot2 code for each chart.\nThe ggplot2 Extensions Gallery: The official showcase of packages that extend the power of ggplot2 with new geoms, themes, and capabilities. A goodt place to discover new visualization tools in the R ecosystem.\nA ggplot2 Tutorial for Beautiful Plotting in R Many fantastic examples. This is great! Have a look!\n\nOnline Books:\n\nR Graphics Cookbook, 2nd Edition: A great “how-to” manual by Winston Chang. It is a collection of practical, problem-oriented “recipes” for solving common plotting tasks in ggplot2.\nData Visualization: A Practical Introduction: Kieran Healy’s blends data visualization theory with practical ggplot2 code.\nInteractive web-based data visualization with R, plotly, and shiny: A guide by Carson Sievert for turning your ggplot2 plots into interactive graphics and building web applications. This is a bit dated, but still useful.\n\nVideos & Community:\n\nThe #TidyTuesday Project: A weekly data project from the R for Data Science community. It is the best place to see hundreds of creative and inspiring examples of what’s possible with ggplot2 and to practice your own skills.\nData visualization with R’s tidyverse and allied packages A collection of videos by Pat Schloss includes maps, community microbiolgy and more.",
    "crumbs": [
      "10. Better Figures in R",
      "• 10. Better ggplots summary"
    ]
  },
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "# Rest of Book",
    "section": "",
    "text": "Preliminary material\nPreface: Clarkia and its data // Types of variables",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "toc.html#section-i-introduction-to-r",
    "href": "toc.html#section-i-introduction-to-r",
    "title": "# Rest of Book",
    "section": "Section I: Introduction to R",
    "text": "Section I: Introduction to R\n1. Getting Started with R: Functions and vectors // Loading packages and data // Data types // RStudio orientation // Summary of Chapter 1.\n\n2. Working with data in R: Adding variables // Selecting variables // Summarizing variables // Choosing rows // Summary of Working with Data in R.\n\n3. Intro to ggplot: One continuous variable // Saving ggplots // Continuous and categorical // Two categorical variables // Two continuous variables // Many explanatory variables // Summary of Intro to ggplot.\n\n4. Reproducible science: Collecting data // Reproducible analyses // Summary of Reproducible Science.",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "toc.html#second-section-summarizing-data",
    "href": "toc.html#second-section-summarizing-data",
    "title": "# Rest of Book",
    "section": "Second Section: Summarizing Data",
    "text": "Second Section: Summarizing Data\n5. Univariate Summaries: Summarizing shape // Changing shape // Summarizing center // Summarizing variability // Summary of Summaries.\n6. Associations: Categorical vs. continuous // Two categorical variables // Two continuous variables // Summary of Associations.\n7. Linear Models: Mean as a model // Categorical predictors // Regression // Two predictors // Summary of Desciptive Linear Modelling.",
    "crumbs": [
      "Rest of Book"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Asmuth, J., Morson, E. M., & Rips, L. J. (2018). Children’s\nunderstanding of the natural numbers’ structure. Cognitive\nScience, 42(6), 1945–1973. https://doi.org/https://doi.org/10.1111/cogs.12615\n\n\nBabbage, C. (1864). Passages from the life of a philosopher.\nLongman; Co.\n\n\nBakker, J. D. (2024). Applied multivariate statistics in\nR. Pressbooks.\n\n\nBeall, C. M. (2006). Andean, Tibetan, and Ethiopian\npatterns of adaptation to high-altitude hypoxia. Integrative\nand Comparative Biology, 46(1), 18–24. https://doi.org/10.1093/icb/icj004\n\n\nBehrouzi, P., & Wit, E. (2017). Detecting epistatic selection with\npartially observed genotype data using copula graphical models.\nJournal of the Royal Statistical Society: Series C (Applied\nStatistics), 68. https://doi.org/10.1111/rssc.12287\n\n\nBergstrom, C. T., & West, J. D. (2020). Calling bullshit: The\nart of skepticism in a data-driven world. Random House.\n\n\nBjörklund, M. (2019). Be careful with your principal components.\nEvolution, 73(10), 2151–2158. https://doi.org/https://doi.org/10.1111/evo.13835\n\n\nBroman, K. W., & Woo, K. H. (2018). Data organization in\nspreadsheets. The American Statistician, 72(1), 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989\n\n\nBryan, J. J. (2020). STAT 545: Data wrangling, exploration, and\nanalysis with r. Bookdown. https://stat545.com\n\n\nChang, W. (2020). R graphics cookbook: Practical recipes for\nvisualizing data. https://r-graphics.org/\n\n\nChari, L., Tara AND Pachter. (2023). The specious art of single-cell\ngenomics. PLOS Computational Biology, 19(8), 1–20. https://doi.org/10.1371/journal.pcbi.1011288\n\n\nD’Hont, A., Denoeud, F., Aury, J.-M., Baurens, F.-C., Carreel, F.,\nGarsmeur, O., Noel, B., Bocs, S., Droc, G., Rouard, M., Da Silva, C.,\nJabbari, K., Cardi, C., Poulain, J., Souquet, M., Labadie, K., Jourda,\nC., Lengellé, J., Rodier-Goud, M., … Wincker, P. (2012). The banana\n(musa acuminata) genome and the evolution of monocotyledonous plants.\nNature, 488(7410), 213–217. https://doi.org/10.1038/nature11241\n\n\nGould, P. (1981). Letting the data speak for themselves. Annals of\nthe Association of American Geographers, 71(2), 166–176.\nhttps://doi.org/https://doi.org/10.1111/j.1467-8306.1981.tb01346.x\n\n\nGrolemund, G. (2014). Hands-on programming with r: Write your own\nfunctions and simulations. \" O’Reilly Media, Inc.\".\n\n\nGrolemund, G., & Wickham, H. (2018). R for data science.\n\n\nHealy, J., & McInnes, L. (2024). Uniform manifold approximation and\nprojection. Nature Reviews Methods Primers, 4(1), 82.\nhttps://doi.org/10.1038/s43586-024-00363-x\n\n\nHealy, K. (2018). Data visualization: A practical introduction.\nPrinceton University Press.\n\n\nHiggins, P. D. R. (2024). Reproducible medical research with r.\nBookdown. https://bookdown.org/pdr_higgins/rmrwr/\n\n\nIsmay, C., & Kim, A. Y. (2019). Statistical inference via data\nscience: A ModernDive into r and the tidyverse. CRC Press.\n\n\nKabacoff, R. (2024). Modern data visualization with r. CRC\nPress.\n\n\nLever, J., Krzywinski, M., & Altman, N. (2017). Principal component\nanalysis. Nature Methods, 14(7), 641–642. https://doi.org/10.1038/nmeth.4346\n\n\nLewis, C. (2024). Data management in large-scale education\nresearch. CRC Press.\n\n\nMarx, V. (2024). Seeing data as t-SNE and UMAP do. Nature\nMethods, 21(6), 930–933. https://doi.org/10.1038/s41592-024-02301-x\n\n\nNovembre, J., Johnson, T., Bryc, K., Kutalik, Z., Boyko, A. R., Auton,\nA., Indap, A., King, K. S., Bergmann, S., Nelson, M. R., Stephens, M.,\n& Bustamante, C. D. (2008). Genes mirror geography within europe.\nNature, 456(7218), 98–101. https://doi.org/10.1038/nature07331\n\n\nNovembre, J., & Stephens, M. (2008). Interpreting principal\ncomponent analyses of spatial population genetic variation. Nature\nGenetics, 40(5), 646–649. https://doi.org/10.1038/ng.139\n\n\nSaccenti, E. (2024). A gentle introduction to principal component\nanalysis using tea-pots, dinosaurs, and pizza. Teaching\nStatistics, 46(1), 38–52. https://doi.org/https://doi.org/10.1111/test.12363\n\n\nSandve, A. A. T., Geir Kjetil AND Nekrutenko. (2013). Ten simple rules\nfor reproducible computational research. PLOS Computational\nBiology, 9(10), 1–4. https://doi.org/10.1371/journal.pcbi.1003285\n\n\nSianta, S. A., Moeller, D. A., & Brandvain, Y. (2024). The extent of\nintrogression between incipient &lt;i&gt;clarkia&lt;/i&gt; species is\ndetermined by temporal environmental variation and mating system.\nProceedings of the National Academy of Sciences,\n121(12), e2316008121. https://doi.org/10.1073/pnas.2316008121\n\n\nSievert, C. (2020). Interactive web-based data visualization with r,\nplotly, and shiny. Chapman; Hall/CRC.\n\n\nSuzuki, Y., Endo, M., Cañas, C., Ayora, S., Alonso, J. C., Sugiyama, H.,\n& Takeyasu, K. (2014). Direct analysis of holliday junction\nresolving enzyme in a DNA origami nanostructure. Nucleic Acids\nResearch, 42(11), 7421–7428. https://doi.org/10.1093/nar/gku320\n\n\nTufte, E. R. (1983). The visual display of quantitative\ninformation (p. 197). pub-gp.\n\n\nTufte, E. R. (1990). Envisioning information. Graphics Press.\n\n\nWattenberg, M., Viégas, F., & Johnson, I. (2016). How to use t-SNE\neffectively. Distill. https://doi.org/10.23915/distill.00002\n\n\nWhitlock, M. C., & Schluter, D. (2020). The analysis of\nbiological data (Third). Macmillan.\n\n\nWickham, H. (2014b). Tidy data. Journal of Statistical\nSoftware, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWickham, H. (2014a). Tidy data. Journal of Statistical Software,\nArticles, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data\nanalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilke, C. O. (2019). Fundamentals of data visualization: A primer on\nmaking informative and compelling figures. O’Reilly Media.\n\n\nYi, X., & Latch, E. K. (2022). Nonrandom missing data can bias\nprincipal component analysis inference of population genetic structure.\nMol Ecol Resour, 22(2), 602–611. https://doi.org/10.1111/1755-0998.13498",
    "crumbs": [
      "References"
    ]
  }
]